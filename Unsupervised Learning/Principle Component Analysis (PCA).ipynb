{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc28d93-1881-440f-8e4f-0453015ce797",
   "metadata": {},
   "source": [
    "# Principle Component Analysis (PCA)\n",
    "---\n",
    "\n",
    "1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. \n",
    "2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column's standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597da931-594f-4cfa-b7c3-069dc1486aa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "### Statistics\n",
    "\n",
    "#### Expected value\n",
    "\n",
    "Given a random variable $X$, its **expected value is its mean value**.\n",
    "$$ \\mathbb{E}[X] = \\mu $$\n",
    "where $\\mu$ is the mean value of the random variable $X$.\n",
    "\n",
    "1. Linearity of Expectation\n",
    "\n",
    "    $$ \\mathbb{E}[X + Y] = \\mathbb{E}[X] + E [Y] $$\n",
    "    $$ \\mathbb{E}[aX + b] = \\mathbb{E}[a] \\mathbb{E}[X] +\\mathbb{E}[b] = a \\mathbb{E}[X] +b $$\n",
    "    \n",
    "1. If $X$ is a discrete random variable with a finite number of values $x_{1}, x_{2}, \\dots, x_{k}$ occurring with probabilities $p_{1}, p_{2}, \\dots, p_{k}$, respectively, the expected value  of $X$ is defined as\n",
    "\n",
    "    $$ \\mathbb{E}[X] = \\sum_{i=1}^{k} x_{i}p_{i} $$\n",
    "\n",
    "1. If the possibilities are the same ($p_{1} = p_{2} = \\dots = p_{n} = \\frac{1}{n}$), then \n",
    "\n",
    "    $$ \\mathbb{E}[X] = \\frac{1}{n} \\sum_{i=1}^{k} x_{i} $$\n",
    "\n",
    "#### Variance\n",
    "\n",
    "The **variance** of a random variable $X$ is the expected value of the squared deviation from the mean of $X$:\n",
    "$$ \\operatorname{var}(X) = \\mathbb{E}[(X - \\mu)^{2}] $$\n",
    "which can also be expressed as:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{var}(X) & = \\mathbb{E}[(X - \\mu)^{2}] \\\\\n",
    "& = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}] \\\\\n",
    "& = \\mathbb{E}[X^{2} - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^{2}] \\\\\n",
    "& = \\mathbb{E}[X^{2}] - 2\\mathbb{E}[X]\\mathbb{E}[X] + \\mathbb{E}[X]^{2} \\\\\n",
    "& = \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "The **covariance** of two random variables $X$ and $Y$ measures the strength of the correlation between them:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{cov}(X, Y) & = \\mathbb{E}[(X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y])] \\\\\n",
    "& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- If $\\operatorname{cov}(X, Y) > 0$, then $Y$ tends to increase as $X$ increases. \n",
    "- If $\\operatorname{cov}(X, Y) < 0$, then $Y$ tends to decrease as $X$ increases.\n",
    "- If $\\operatorname{cov}(X, Y) = 0$, then $X$ and $Y$ are uncorrelated.\n",
    "\n",
    "1. If we sample $n$ observations from $X$ and $Y$ to get vectors $\\mathbf{x}$ and $\\mathbf{y}$, then we can actually compute $\\operatorname{cov}(X, Y)$ based on the values in $\\mathbf{x}$ and $\\mathbf{y}$, since the expected value of a random variable is just the mean of the its observations:\n",
    "    $$ \\operatorname{cov}(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{x}_{i} - \\bar{\\mathbf{x}}) (\\mathbf{y}_{i} - \\bar{\\mathbf{y}}) $$\n",
    "    where $\\bar{\\mathbf{x}}$ and $\\bar{\\mathbf{y}}$ are the mean of $\\mathbf{x}$ and $\\mathbf{y}$, respectively. \n",
    "1. In the special case of $Y = X$, the covariance reduces to variance.\n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    \\operatorname{cov}(X, X) & = \\mathbb{E}[(X - \\mathbb{E}[X]) (X - \\mathbb{E}[X])] \\\\\n",
    "    & = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}] \\\\\n",
    "    & = \\operatorname{var}(X) \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "#### Covariance matrix (variance-covariance matrix)\n",
    "\n",
    "Given $n$ random variables $X_{1}, X_{2}, \\dots, X_{n}$, the **covariance matrix** is a square matrix of the size $n \\times n$ giving the covariance between every pair of random variables, where \n",
    "$$ \\mathbf{V}_{i, j} = \\operatorname{cov}(X_{i}, X_{j}) $$\n",
    "1. $\\mathbf{V}$ is always symmetric, since $\\operatorname{cov}(X_{i}, X_{j}) = \\operatorname{cov}(X_{j}, X_{i})$.\n",
    "1. The diagonal elements of $\\mathbf{V}$ are the variances of the random variables ($\\mathbf{V}_{i, i} = \\operatorname{cov}(X_{i}, X_{i}) = \\operatorname{var}(X_{i})$).\n",
    "1. Given a matrix $\\mathbf{X}$ that has $n$ observations (rows) and $d$ variables (columns),  let \n",
    "\n",
    "    $$ \\mathbf{A} = \\mathbf{X}^{T}\\mathbf{X} $$\n",
    "    \n",
    "    Then, each element of the matrix $\\mathbf{A}$ is the dot product of each pair of the columns in $\\mathbf{X}$. \n",
    "    \n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    \\mathbf{A}_{i, j} & = \\mathbf{X}_{*, i} \\cdot \\mathbf{X}_{*, j} \\\\\n",
    "    & = \\sum_{k=1}^{n} \\mathbf{X}_{k, i}\\mathbf{X}_{k, j} \\\\\n",
    "    \\end{align}\n",
    "    $$ \n",
    "    \n",
    "    Now let's look at the covariance between each pair of the variables (columns) in $\\mathbf{X}$, \n",
    "    \n",
    "    $$ \\operatorname{cov}(\\mathbf{X}_{*, i}, \\mathbf{X}_{*, j}) = \\frac{1}{n} \\sum_{k=1}^{n} (\\mathbf{X}_{k, i} - \\bar{\\mathbf{X}}_{k, i}) (\\mathbf{X}_{k, j} - \\bar{\\mathbf{X}}_{k, j}) $$\n",
    "    \n",
    "    Assuming that the each variable (column) of $\\mathbf{X}$ is zero-centered (the means of the columns in $\\mathbf{X}$ are 0):\n",
    "    \n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    \\operatorname{cov}(\\mathbf{X}_{*, i}, \\mathbf{X}_{*, j}) & = \\frac{1}{n} \\sum_{k=1}^{n} (\\mathbf{X}_{k, i} - \\bar{\\mathbf{X}}_{k, i}) (\\mathbf{X}_{k, j} - \\bar{\\mathbf{X}}_{k, j}) \\\\\n",
    "    & = \\frac{1}{n} \\sum_{k=1}^{n} \\mathbf{X}_{k, i} \\mathbf{X}_{k, j} \\\\\n",
    "    & = \\frac{1}{n} \\mathbf{A}_{i, j}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "    Thus, the covariance matrix $\\mathbf{V}$ between each pair of the variables in zero-centered matrix $\\mathbf{X}$ is \n",
    "    \n",
    "    $$ \\mathbf{V} = \\frac{1}{n} \\mathbf{X}^{T}\\mathbf{X} $$ \n",
    "\n",
    "### Linear algebra\n",
    "\n",
    "#### Orthogonal\n",
    "Two vectors $\\mathbf{a}$ and $\\mathbf{b}$ are **orthogonal** if their inner product is 0:\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = 0 $$\n",
    "\n",
    "#### Vector projection\n",
    "The **vector projection** of a vector $\\mathbf{a}$ on a vector $\\mathbf{b}$ is the orthogonal projection of $\\mathbf{a}$ onto $\\mathbf{b}$:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{proj}_{\\mathbf{b}}\\mathbf{a} & = (\\lvert \\mathbf{a} \\rvert \\cos(\\theta)) \\hat{\\mathbf{b}} \\\\\n",
    "& = (\\mathbf{a} \\cdot \\hat{\\mathbf{b}}) \\hat{\\mathbf{b}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\lvert \\mathbf{a} \\rvert$ is the length of $\\mathbf{a}$, $\\theta$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$, and $\\hat{\\mathbf{b}}$ is the unit vector that has the same direction with $\\mathbf{b}$.\n",
    "\n",
    "#### Orthonormal basis\n",
    "\n",
    "The vectors $\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{d}$ form an **orthonormal basis** for the space $V$ with $d$ dimensions, if the vectors $\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{d}$ are unit vectors and orthogonal to each other.\n",
    "\n",
    "Given that the vectors $\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\dots, \\mathbf{v}_{d}$ form an orthonormal basis for the space $V$, the projection of a vector $\\mathbf{w}$ onto $V$ is the sum of the projections of $\\mathbf{w}$ onto the individual basis vectors:\n",
    "$$ \\tilde{\\mathbf{w}} = \\sum_{i=1}^{d} (\\mathbf{w} \\cdot \\mathbf{v}_{i}) \\mathbf{v}_{i} $$\n",
    "\n",
    "#### Eigenvectors, Eigenvalues \n",
    "\n",
    "Given a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\lambda$ is said to be an **eigenvalue** of $\\mathbf{A}$ if there exists a **eigenvector** $\\mathbf{z} \\in \\mathbb{R}^n \\neq 0$, such that:\n",
    "$$ \\mathbf{A}\\mathbf{z} = \\lambda\\mathbf{z} $$\n",
    "    \n",
    "#### Eigendecomposition (spectral decomposition) \n",
    "\n",
    "Let $\\mathbf{M}$ be a real symmetric $d \\times d$ matrix with eigenvalues $\\lambda_{1}, \\lambda_{2}, \\dots , \\lambda_{d}$ and corresponding orthonormal eigenvectors $\\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d}$. Then:\n",
    "$$ \\mathbf{M} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T $$ \n",
    "where $\\mathbf{\\Lambda}$ is a diagonal matrix with $\\lambda_{1}, \\lambda_{2}, \\dots , \\lambda_{d}$ in diagonal and 0 elsewhere and $\\mathbf{Q}$ matrix has $\\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d}$ vectors as columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e3faa-4b1e-439c-9901-f501c75bd647",
   "metadata": {},
   "source": [
    "## PCA algorithm \n",
    "---\n",
    "\n",
    "> **Function**: PCA.  \n",
    "> **Input**: A matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and a integer value indicating the objective dimension $a$.  \n",
    "> **Output**: a transformed matrix $\\hat{\\mathbf{X}} \\in \\mathbb{R}^{n \\times a}$.  \n",
    "> 1. Standardize the input. For $j$ in $[1, 2, \\dots, d]$: $\\mathbf{X}_{*, j} = \\frac{\\mathbf{X}_{*, j} - \\operatorname{mean}(\\mathbf{X}_{*, j})}{\\operatorname{std}(\\mathbf{X}_{*, j})}$.\n",
    "> 1. Calculate the covariance matrix between columns. $\\mathbf{V} = \\frac{1}{n} \\mathbf{X}^{T}\\mathbf{X}$.\n",
    "> 1. Use eigendecomposition to decompose $\\mathbf{V}$ to get a list of eigenvalues $\\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{d}$ and corresponding eigenvectors $\\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d}$.\n",
    "> 1. Sort eigenvalues in the decreasing order and select the eigenvectors with $a$ largest eigenvalues. View the $a$ eigenvectors as $a$ columns of the matrix $\\mathbf{E} \\in \\mathbb{R}^{d \\times a}$.\n",
    "> 1. Get the projected matrix $\\hat{\\mathbf{X}}$ in $a$ dimensions. $\\hat{\\mathbf{X}} = \\mathbf{X}\\mathbf{E}$\n",
    "> 1. Return $\\hat{\\mathbf{X}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a16fc-9b0b-4a20-b0e7-3d522c1a4acc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PCA objective derivation\n",
    "***\n",
    "\n",
    "#### Projection of a single instance (vector) to the subspace\n",
    "\n",
    "An instance $\\mathbf{x} \\in \\mathbb{R}^{d}$ can be projected to any given orthonormal basis $\\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{d}$ of dimension $d$ without any error:\n",
    "$$ \\mathbf{x} = \\tilde{\\mathbf{x}} = \\sum_{i=1}^{d} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} $$\n",
    "\n",
    "However, there is a inevitable reconstruction error if $\\mathbf{x}$ is projected to only $m$ ($m < d$) dimensions $\\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{m}$:\n",
    "$$ \\mathbf{x} \\neq \\tilde{\\mathbf{x}} = \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} $$\n",
    "\n",
    "We can measure the error by the squared distance:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{err} & = \\lVert \\mathbf{x} - \\tilde{\\mathbf{x}} \\rVert^{2} \\\\\n",
    "& = \\big\\lVert \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\big\\rVert^{2} \\\\\n",
    "& = \\left( \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\right) \\cdot \\left( \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\right) \\\\\n",
    "& = \\mathbf{x} \\cdot \\mathbf{x} - \\mathbf{x} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} - \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\cdot \\mathbf{x} + \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\\\ \n",
    "& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\left( \\mathbf{x} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\right) + \\sum_{i=1}^{m} ((\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i}) ((\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i}) & \\text{[$\\because \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{j} = 0$ if $i \\neq j$]} \\\\\n",
    "& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) + \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) (\\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i}) \\\\\n",
    "& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} + \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} & \\text{[$\\because \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1$]} \\\\\n",
    "& = \\mathbf{x} \\cdot \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### PCA as minimizing projection error\n",
    "\n",
    "Given a dataset of $n$ instances and $d$ variables, we can use mean squared error to measure the reconstruction error of all instances in the dataset projected to $m$ dimensions:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{MSE} & = \\frac{1}{n} \\sum_{j=1}^{n} \\lVert \\mathbf{x}_{j} - \\tilde{\\mathbf{x}}_{j}\\rVert^{2} \\\\\n",
    "& = \\frac{1}{n} \\sum_{j=1}^{n} \\left( \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\right) \\\\\n",
    "& = \\frac{1}{n} \\sum_{j=1}^{n} \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j} - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The goal of PCA is to get a particular orthonormal basis of only $m$ dimensions such that the mean squared error of projecting all instances on to it is minimized:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad &  - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "1. The constraint is added so that the $\\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{m}$ are all unit vectors.\n",
    "1. The first term $\\frac{1}{n} \\sum_{j=1}^{n} \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j}$ is omitted in the objective because it is a fixed value once the dataset is provided.\n",
    "\n",
    "#### PCA as maximizing projection variance\n",
    "The objective of minimizing projection error defined above is the same as maximizing its negation. \n",
    "$$ \n",
    "\\begin{align}\n",
    "\\min \\quad & - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\max \\quad & \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The variance of the projection of all instances to the dimension $\\hat{\\mathbf{w}}_{i}$ is:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{var}(X) & = \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n",
    "\\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) & = \\mathbb{E}[(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2}] - \\mathbb{E}[\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}]^{2} \\\\\n",
    "\\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) & = \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 - \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The sum of the variances of the projections to all $m$ dimensions is:\n",
    "$$ \\sum_{i=1}^{m} \\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) = \\sum_{i=1}^{m} \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 - \\sum_{i=1}^{m} \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 $$\n",
    "\n",
    "Since the dataset is preprocessed to be zero-centered (each variable has mean 0), the last term of the equation above becomes 0:\n",
    "$$ \\sum_{i=1}^{m} \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 = \\sum_{i=1}^{m} \\left( \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\right) \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 = \\sum_{i=1}^{m} (0 \\cdot \\hat{\\mathbf{w}}_{i})^{2} = 0 $$\n",
    "\n",
    "Thus, we can see that minimizing projection error is the same as maximizing the sum of the projection variance:\n",
    "$$ \\sum_{i=1}^{m} \\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) = \\sum_{i=1}^{m} \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 = \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c487-7a17-45a0-8c84-5caa6450c208",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solving PCA objective\n",
    "***\n",
    "\n",
    "Given the minimization problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "we can first rewrite the objective in the matrix form:\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "= & \\sum_{i=1}^{m} \\frac{1}{n} \\sum_{j=1}^{n} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n",
    "= & \\sum_{i=1}^{m} \\frac{1}{n} (\\mathbf{X}\\hat{\\mathbf{w}}_{i})^{T} (\\mathbf{X}\\hat{\\mathbf{w}}_{i}) \\\\\n",
    "= & \\sum_{i=1}^{m} \\frac{1}{n} \\hat{\\mathbf{w}}_{i}^{T}\\mathbf{X}^{T} \\mathbf{X}\\hat{\\mathbf{w}}_{i} \\\\\n",
    "= & \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\frac{\\mathbf{X}^{T}\\mathbf{X}}{n} \\hat{\\mathbf{w}}_{i} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since we have already zero-centered the dataset, we can replace $\\frac{\\mathbf{X}^{T}\\mathbf{X}}{n}$ with the covariance matrix $\\mathbf{V}$. Thus, the maximization problem in the matrix form is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} \\\\\n",
    "\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The Lagrangian of the optimization problem is:\n",
    "$$ L(\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{m}, \\lambda_{1} \\dots, \\lambda_{m}) = - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} + \\sum_{i=1}^{m} \\lambda_{i}(\\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} - 1) $$\n",
    "\n",
    "Solving $L$ by\n",
    "1. Setting the derivative of $L$ w.r.t $\\hat{\\mathbf{w}}_{i}$ to be 0:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{\\partial L}{\\partial \\hat{\\mathbf{w}}}_{i} & = 0 \\\\\n",
    "    -\\sum_{i=1}^{m} 2\\mathbf{V}\\hat{\\mathbf{w}}_{i} + \\sum_{i=1}^{m} 2\\lambda_{i}\\hat{\\mathbf{w}}_{i} & = 0 \\\\\n",
    "    \\sum_{i=1}^{m} 2\\mathbf{V}\\hat{\\mathbf{w}}_{i} & = \\sum_{i=1}^{m} 2\\lambda_{i}\\hat{\\mathbf{w}}_{i} \\\\\n",
    "    \\mathbf{V}\\hat{\\mathbf{w}}_{i} & = \\lambda_{i}\\hat{\\mathbf{w}}_{i}, \\quad i = 1, \\dots m \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    The results show that the results we want are the eigenvectors $\\hat{\\mathbf{w}}_{i}$ and eigenvalues $\\lambda_{i}$ of $\\mathbf{V}$. \n",
    "1. Setting the derivative of $L$ w.r.t $\\lambda_{i}$ to be 0:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{\\partial L}{\\partial \\lambda_{i}} & = 0 \\\\\n",
    "    \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} - 1 & = 0 \\\\ \n",
    "    \\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} & = 1, \\quad i = 1, \\dots m \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "    The constraints show that the eigenvectors must also be unit vectors.\n",
    "1. Plug in the results back to the objective: \n",
    "    $$ -\\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\lambda_{i} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\lambda_{i} \\hat{\\mathbf{w}}_{i}^{T} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\lambda_{i} $$\n",
    "    The last equation shows that we need to select the $m$ largest eigenvalues to minimize the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299851dd-913d-4370-a312-186ba220255a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reference \n",
    "***\n",
    "\n",
    "1. https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "1. https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\n",
    "1. https://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
