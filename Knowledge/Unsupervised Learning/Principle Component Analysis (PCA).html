<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Joeyonng - Principle Component Analysis (PCA)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Joeyonng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../Knowledge" rel="" target="">
 <span class="menu-text">Knowledge</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Notes" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng-backyard/" rel="" target="">
 <span class="menu-text">Backyard</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Principle Component Analysis (PCA)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Learning Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/1_Statistical_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/2_Bayesian_Classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Classifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/3_Effective_Class_Size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Effective Class Size</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Empirical Risk Minimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/5_Uniform_Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uniform Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/6_PAC_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PAC Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/7_Rademacher_Complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rademacher Complexity</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary">Preliminary</a>
  <ul class="collapse">
  <li><a href="#statistics" id="toc-statistics" class="nav-link" data-scroll-target="#statistics">Statistics</a></li>
  <li><a href="#linear-algebra" id="toc-linear-algebra" class="nav-link" data-scroll-target="#linear-algebra">Linear algebra</a></li>
  </ul></li>
  <li><a href="#pca-algorithm" id="toc-pca-algorithm" class="nav-link" data-scroll-target="#pca-algorithm">## PCA algorithm</a></li>
  <li><a href="#pca-objective-derivation" id="toc-pca-objective-derivation" class="nav-link" data-scroll-target="#pca-objective-derivation">PCA objective derivation</a></li>
  <li><a href="#solving-pca-objective" id="toc-solving-pca-objective" class="nav-link" data-scroll-target="#solving-pca-objective">## Solving PCA objective</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Principle Component Analysis (PCA)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><em>Updated 01-13-2023 (First commited 02-27-2022)</em></p>
<hr>
<ol type="1">
<li><p>Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions.</p></li>
<li><p>To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.</p></li>
</ol>
<section id="preliminary" class="level2">
<h2 class="anchored" data-anchor-id="preliminary">Preliminary</h2>
<hr>
<section id="statistics" class="level3">
<h3 class="anchored" data-anchor-id="statistics">Statistics</h3>
<section id="expected-value-expectation" class="level4">
<h4 class="anchored" data-anchor-id="expected-value-expectation">Expected value (Expectation)</h4>
<p>Given a random variable <span class="math inline">X</span>, its <strong>expected value (expectation) is its mean value</strong>.</p>
<p><span class="math display"> \mathbb{E}[X] = \mu </span></p>
<p>where <span class="math inline">\mu</span> is the mean value of the random variable <span class="math inline">X</span>.</p>
<ol type="1">
<li><p>Linearity of Expectation</p>
<p><span class="math display"> \mathbb{E}[X + Y] = \mathbb{E}[X] + E [Y] </span> <span class="math display"> \mathbb{E}[aX + b] = \mathbb{E}[a] \mathbb{E}[X] +\mathbb{E}[b] = a \mathbb{E}[X] +b </span></p></li>
<li><p>If <span class="math inline">X</span> is a discrete random variable with a finite number of values <span class="math inline">x_{1}, x_{2}, \dots, x_{k}</span> occurring with probabilities <span class="math inline">p_{1}, p_{2}, \dots, p_{k}</span>, respectively, the expected value of <span class="math inline">X</span> is defined as</p>
<p><span class="math display"> \mathbb{E}[X] = \sum_{i=1}^{k} x_{i}p_{i} </span></p>
<p>If the possibilities are the same (<span class="math inline">p_{1} = p_{2} = \dots = p_{n} = \frac{1}{n}</span>), then</p>
<p><span class="math display"> \mathbb{E}[X] = \frac{1}{n} \sum_{i=1}^{k} x_{i} </span></p></li>
<li><p>In general, given a discrete random variable <span class="math inline">X</span> with probability mass function (PMF) <span class="math inline">P_{X}(x)</span>, the expected value of a function <span class="math inline">f(x)</span> that takes <span class="math inline">X</span> as the inputs is</p>
<p><span class="math display"> \mathbb{E}_{X}[f(X)] = \sum_{x \in X} f(x)P_{X}(x) </span></p></li>
</ol>
</section>
<section id="variance" class="level4">
<h4 class="anchored" data-anchor-id="variance">Variance</h4>
<p>The <strong>variance</strong> of a random variable <span class="math inline">X</span> is the expected value of the squared deviation from the mean of <span class="math inline">X</span>:</p>
<p><span class="math display"> \operatorname{var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^{2}] </span></p>
<p>which can also be expressed as:</p>
<p><span class="math display">
\begin{align}
\operatorname{var}(X) &amp; = \mathbb{E}[(X - \mathbb{E}[X])^{2}] \\
&amp; = \mathbb{E}[X^{2} - 2X\mathbb{E}[X] + \mathbb{E}[X]^{2}] \\
&amp; = \mathbb{E}[X^{2}] - 2\mathbb{E}[X]\mathbb{E}[X] + \mathbb{E}[X]^{2} &amp; \text{[Linearity of Expectation]}\\
&amp; = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2} \\
\end{align}
</span></p>
</section>
<section id="covariance" class="level4">
<h4 class="anchored" data-anchor-id="covariance">Covariance</h4>
<p>The <strong>covariance</strong> of two random variables <span class="math inline">X</span> and <span class="math inline">Y</span> measures the strength of the correlation between them:</p>
<p><span class="math display">
\begin{align}
\operatorname{cov}(X, Y) &amp; = \mathbb{E}[(X - \mathbb{E}[X]) (Y - \mathbb{E}[Y])] \\
&amp; = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] \\
\end{align}
</span></p>
<ol type="1">
<li><p>Covariance tells the relation between how <span class="math inline">Y</span> changes from <span class="math inline">\mathbf{E}(Y)</span> when <span class="math inline">X</span> changes from <span class="math inline">\mathbf{E}(X)</span>.</p>
<ul>
<li><p>If <span class="math inline">\operatorname{cov}(X, Y) &gt; 0</span>, then <span class="math inline">Y</span> tends to increase as <span class="math inline">X</span> increases (positive correlation).</p></li>
<li><p>If <span class="math inline">\operatorname{cov}(X, Y) &lt; 0</span>, then <span class="math inline">Y</span> tends to decrease as <span class="math inline">X</span> increases (negative correlation).</p></li>
<li><p>If <span class="math inline">\operatorname{cov}(X, Y) = 0</span>, then <span class="math inline">X</span> and <span class="math inline">Y</span> are uncorrelated.</p></li>
</ul></li>
<li><p>If we sample <span class="math inline">n</span> observations from <span class="math inline">X</span> and <span class="math inline">Y</span> to get vectors <span class="math inline">\mathbf{x}</span> and <span class="math inline">\mathbf{y}</span>, then we can actually compute <span class="math inline">\operatorname{cov}(X, Y)</span> based on the values in <span class="math inline">\mathbf{x}</span> and <span class="math inline">\mathbf{y}</span>, since the expected value of a random variable is just the mean of the its observations:</p>
<p><span class="math display"> \operatorname{cov}(\mathbf{x}, \mathbf{y}) = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{x}_{i} - \bar{\mathbf{x}}) (\mathbf{y}_{i} - \bar{\mathbf{y}}) </span></p>
<p>where <span class="math inline">\bar{\mathbf{x}}</span> and <span class="math inline">\bar{\mathbf{y}}</span> are the mean of <span class="math inline">\mathbf{x}</span> and <span class="math inline">\mathbf{y}</span>, respectively.</p></li>
<li><p>In the special case of <span class="math inline">Y = X</span>, the covariance reduces to variance.</p>
<p><span class="math display">
\begin{align}
\operatorname{cov}(X, X) &amp; = \mathbb{E}[(X - \mathbb{E}[X]) (X - \mathbb{E}[X])] \\
&amp; = \mathbb{E}[(X - \mathbb{E}[X])^{2}] \\
&amp; = \operatorname{var}(X) \\
\end{align}
</span></p></li>
</ol>
</section>
<section id="covariance-matrix-variance-covariance-matrix" class="level4">
<h4 class="anchored" data-anchor-id="covariance-matrix-variance-covariance-matrix">Covariance matrix (variance-covariance matrix)</h4>
<p>Given <span class="math inline">n</span> random variables <span class="math inline">X_{1}, X_{2}, \dots, X_{n}</span>, the <strong>covariance matrix</strong> <span class="math inline">\mathbf{V}</span> is a square matrix of the size <span class="math inline">n \times n</span> giving the covariance between every pair of random variables, where</p>
<p><span class="math display"> \mathbf{V}_{i, j} = \operatorname{cov}(X_{i}, X_{j}) </span></p>
<ol type="1">
<li><p><span class="math inline">\mathbf{V}</span> is always symmetric, since <span class="math inline">\operatorname{cov}(X_{i}, X_{j}) = \operatorname{cov}(X_{j}, X_{i})</span>.</p></li>
<li><p>The diagonal elements of <span class="math inline">\mathbf{V}</span> are the variances of the random variables (<span class="math inline">\mathbf{V}_{i, i} = \operatorname{cov}(X_{i}, X_{i}) = \operatorname{var}(X_{i})</span>).</p></li>
<li><p>Given a matrix <span class="math inline">\mathbf{X}</span> that has <span class="math inline">n</span> observations (rows) and <span class="math inline">d</span> variables (columns), let</p>
<p><span class="math display"> \mathbf{A} = \mathbf{X}^{T}\mathbf{X} </span></p>
<p>Then, each element of the matrix <span class="math inline">\mathbf{A}</span> is the dot product of each pair of the columns in <span class="math inline">\mathbf{X}</span>.</p>
<p><span class="math display">
\begin{align}
\mathbf{A}_{i, j} &amp; = \mathbf{X}_{*, i} \cdot \mathbf{X}_{*, j} \\
&amp; = \sum_{k=1}^{n} \mathbf{X}_{k, i}\mathbf{X}_{k, j} \\
\end{align}
</span></p>
<p>Now let’s look at the covariance between each pair of the variables (columns) in <span class="math inline">\mathbf{X}</span>,</p>
<p><span class="math display"> \operatorname{cov}(\mathbf{X}_{*, i}, \mathbf{X}_{*, j}) = \frac{1}{n} \sum_{k=1}^{n} (\mathbf{X}_{k, i} - \bar{\mathbf{X}}_{k, i}) (\mathbf{X}_{k, j} - \bar{\mathbf{X}}_{k, j}) </span></p>
<p>Assuming that the each variable (column) of <span class="math inline">\mathbf{X}</span> is zero-centered (the means of the columns in <span class="math inline">\mathbf{X}</span> are 0):</p>
<p><span class="math display">
\begin{align}
\operatorname{cov}(\mathbf{X}_{*, i}, \mathbf{X}_{*, j}) &amp; = \frac{1}{n} \sum_{k=1}^{n} (\mathbf{X}_{k, i} - \bar{\mathbf{X}}_{k, i}) (\mathbf{X}_{k, j} - \bar{\mathbf{X}}_{k, j}) \\
&amp; = \frac{1}{n} \sum_{k=1}^{n} \mathbf{X}_{k, i} \mathbf{X}_{k, j} \\
&amp; = \frac{1}{n} \mathbf{A}_{i, j}
\end{align}
</span></p>
<p>Thus, the covariance matrix <span class="math inline">\mathbf{V}</span> between each pair of the variables in zero-centered matrix <span class="math inline">\mathbf{X}</span> is</p>
<p><span class="math display"> \mathbf{V} = \frac{1}{n} \mathbf{X}^{T}\mathbf{X} </span></p></li>
</ol>
</section>
</section>
<section id="linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra">Linear algebra</h3>
<section id="orthogonal" class="level4">
<h4 class="anchored" data-anchor-id="orthogonal">Orthogonal</h4>
<p>Two vectors <span class="math inline">\mathbf{a}</span> and <span class="math inline">\mathbf{b}</span> are <strong>orthogonal</strong> if their inner product is 0:</p>
<p><span class="math display"> \mathbf{a} \cdot \mathbf{b} = 0 </span></p>
</section>
<section id="vector-projection" class="level4">
<h4 class="anchored" data-anchor-id="vector-projection">Vector projection</h4>
<p>The <strong>vector projection</strong> of a vector <span class="math inline">\mathbf{a}</span> on a vector <span class="math inline">\mathbf{b}</span> is the orthogonal projection of <span class="math inline">\mathbf{a}</span> onto <span class="math inline">\mathbf{b}</span>:</p>
<p><span class="math display">
\begin{align}
\operatorname{proj}_{\mathbf{b}}\mathbf{a} &amp; = (\lvert \mathbf{a} \rvert \cos(\theta)) \hat{\mathbf{b}} \\
&amp; = (\mathbf{a} \cdot \hat{\mathbf{b}}) \hat{\mathbf{b}} \\
\end{align}
</span></p>
<p>where <span class="math inline">\lvert \mathbf{a} \rvert</span> is the length of <span class="math inline">\mathbf{a}</span>, <span class="math inline">\theta</span> is the angle between <span class="math inline">\mathbf{a}</span> and <span class="math inline">\mathbf{b}</span>, and <span class="math inline">\hat{\mathbf{b}}</span> is the unit vector that has the same direction with <span class="math inline">\mathbf{b}</span>.</p>
</section>
<section id="orthonormal-basis-orthonormal-matrix" class="level4">
<h4 class="anchored" data-anchor-id="orthonormal-basis-orthonormal-matrix">Orthonormal basis, orthonormal matrix</h4>
<p>The vectors <span class="math inline">\hat{\mathbf{v}}_{1}, \hat{\mathbf{v}}_{2}, \dots, \hat{\mathbf{v}}_{d} \in \mathbb{R}^{d}</span> form an <strong>orthonormal basis</strong> for the space <span class="math inline">V</span> with <span class="math inline">d</span> dimensions if the vectors <span class="math inline">\hat{\mathbf{v}}_{1}, \hat{\mathbf{v}}_{2}, \dots, \hat{\mathbf{v}}_{d}</span> are unit vectors and orthogonal to each other.</p>
<p>Given that the vectors <span class="math inline">\hat{\mathbf{v}}_{1}, \hat{\mathbf{v}}_{2}, \dots, \hat{\mathbf{v}}_{d}</span> form an orthonormal basis for the space <span class="math inline">V</span>, the projection of a vector <span class="math inline">\mathbf{w}</span> onto <span class="math inline">V</span> is the sum of the projections of <span class="math inline">\mathbf{w}</span> onto the individual basis vectors:</p>
<p><span class="math display"> \tilde{\mathbf{w}} = \sum_{i=1}^{d} (\mathbf{w} \cdot \hat{\mathbf{v}}_{i}) \hat{\mathbf{v}}_{i} </span></p>
</section>
<section id="eigenvectors-eigenvalues" class="level4">
<h4 class="anchored" data-anchor-id="eigenvectors-eigenvalues">Eigenvectors, Eigenvalues</h4>
<p>Given a matrix <span class="math inline">\mathbf{A} \in \mathbb{R}^{n \times n}</span>, <span class="math inline">\lambda</span> is said to be an <strong>eigenvalue</strong> of <span class="math inline">\mathbf{A}</span> if there exists a <strong>eigenvector</strong> <span class="math inline">\mathbf{z} \in \mathbb{R}^n \neq 0</span>, such that:</p>
<p><span class="math display"> \mathbf{A}\mathbf{z} = \lambda\mathbf{z} </span></p>
<ul>
<li>It can interpreted as: the application of <span class="math inline">\mathbf{A}</span> to <span class="math inline">\mathbf{z}</span> is the same as changing the length of <span class="math inline">\mathbf{z}</span> by a factor of <span class="math inline">\lambda</span> without changing <span class="math inline">\mathbf{z}</span>’s direction</li>
</ul>
</section>
<section id="eigendecomposition-spectral-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="eigendecomposition-spectral-decomposition">Eigendecomposition (spectral decomposition)</h4>
<p>Let <span class="math inline">\mathbf{M}</span> be a real symmetric <span class="math inline">d \times d</span> matrix with eigenvalues <span class="math inline">\lambda_{1}, \lambda_{2}, \dots , \lambda_{d}</span> and corresponding orthonormal eigenvectors <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{1}, \dots \mathbf{u}_{d}</span>. Then:</p>
<p><span class="math display"> \mathbf{M} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T </span></p>
<p>where <span class="math inline">\mathbf{\Lambda}</span> is a diagonal matrix with <span class="math inline">\lambda_{1}, \lambda_{2}, \dots , \lambda_{d}</span> in diagonal and 0 elsewhere and <span class="math inline">\mathbf{Q}</span> matrix has <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{1}, \dots \mathbf{u}_{d}</span> vectors as columns.</p>
</section>
</section>
</section>
<section id="pca-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="pca-algorithm">## PCA algorithm</h2>
<p><strong>Function</strong>: PCA.<br>
<strong>Input</strong>: A matrix <span class="math inline">\mathbf{X} \in \mathbb{R}^{n \times d}</span> and an integer value indicating the objective dimension <span class="math inline">m</span>.<br>
<strong>Output</strong>: a transformed matrix in low dimension <span class="math inline">\hat{\mathbf{X}} \in \mathbb{R}^{n \times m}</span>.<br>
1. Standardize the input. For <span class="math inline">j</span> in <span class="math inline">[1, 2, \dots, d]</span></p>
<pre><code>$$ \mathbf{X}_{*, j} = \frac{\mathbf{X}_{*, j} - \operatorname{mean}(\mathbf{X}_{*, j})}{\operatorname{std}(\mathbf{X}_{*, j})} $$.</code></pre>
<ol type="1">
<li><p>Calculate the covariance matrix between columns.</p>
<p><span class="math display"> \mathbf{V} = \frac{1}{n} \mathbf{X}^{T}\mathbf{X} </span>.</p></li>
<li><p>Use eigendecomposition to decompose <span class="math inline">\mathbf{V}</span> to get a list of eigenvalues <span class="math inline">\lambda_{1}, \lambda_{2}, \dots, \lambda_{d}</span> and corresponding eigenvectors <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{1}, \dots \mathbf{u}_{d}</span>.</p></li>
<li><p>Sort eigenvalues in the decreasing order and select the eigenvectors with <span class="math inline">m</span> largest eigenvalues. View the <span class="math inline">m</span> eigenvectors as <span class="math inline">m</span> columns of the matrix <span class="math inline">\mathbf{E} \in \mathbb{R}^{d \times m}</span>.</p></li>
<li><p>Get the transformed matrix <span class="math inline">\hat{\mathbf{X}}</span> in <span class="math inline">m</span> dimensions.</p>
<p><span class="math display"> \hat{\mathbf{X}} = \mathbf{X}\mathbf{E} </span>.</p></li>
<li><p>Return <span class="math inline">\hat{\mathbf{X}}</span>.</p></li>
</ol>
</section>
<section id="pca-objective-derivation" class="level2">
<h2 class="anchored" data-anchor-id="pca-objective-derivation">PCA objective derivation</h2>
<hr>
<section id="projection-of-a-single-instance-vector-to-the-subspace" class="level4">
<h4 class="anchored" data-anchor-id="projection-of-a-single-instance-vector-to-the-subspace">Projection of a single instance (vector) to the subspace</h4>
<p>An instance <span class="math inline">\mathbf{x} \in \mathbb{R}^{d}</span> can be projected to any given orthonormal basis <span class="math inline">\hat{\mathbf{w}}_{1}, \hat{\mathbf{w}}_{2}, \dots, \hat{\mathbf{w}}_{d} \in \mathbb{R}^{d}</span> of dimension <span class="math inline">d</span> without any error:</p>
<p><span class="math display"> \mathbf{x} = \tilde{\mathbf{x}} = \sum_{i=1}^{d} (\mathbf{x} \cdot \hat{\mathbf{w}}_{i}) \hat{\mathbf{w}}_{i} </span></p>
<p>However, there is a inevitable reconstruction error if <span class="math inline">\mathbf{x}</span> is projected to only <span class="math inline">m</span> (<span class="math inline">m &lt; d</span>) dimensions <span class="math inline">\hat{\mathbf{w}}_{1}, \hat{\mathbf{w}}_{2}, \dots, \hat{\mathbf{w}}_{m}</span>:</p>
<p><span class="math display"> \mathbf{x} \neq \tilde{\mathbf{x}} = \sum_{i=1}^{m} (\mathbf{x} \cdot \hat{\mathbf{w}}_{i}) \hat{\mathbf{w}}_{i} </span></p>
<p>We can measure the error by the squared distance:</p>
<p><span class="math display">
\begin{align}
\operatorname{err} &amp; = \lVert \mathbf{x} - \tilde{\mathbf{x}} \rVert^{2} \\
&amp; = \big\lVert \mathbf{x} - \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) \hat{\mathbf{w}}_{i} \big\rVert^{2} \\
&amp; = \left( \mathbf{x} - \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) \hat{\mathbf{w}}_{i} \right) \cdot \left( \mathbf{x} - \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) \hat{\mathbf{w}}_{i} \right) \\
&amp; = \mathbf{x} \cdot \mathbf{x} - \mathbf{x} \cdot \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i} - \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i} \cdot \mathbf{x} + \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i} \cdot \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i} \\
&amp; = \mathbf{x} \cdot \mathbf{x} - 2 \left( \mathbf{x} \cdot \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i} \right) + \sum_{i=1}^{m} ((\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i}) ((\mathbf{x} \cdot \hat{\mathbf{w}}_{i})\hat{\mathbf{w}}_{i}) &amp; [\hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{j} = 0 \text{ if } i \neq j] \\
&amp; = \mathbf{x} \cdot \mathbf{x} - 2 \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})(\mathbf{x} \cdot \hat{\mathbf{w}}_{i}) + \sum_{i=1}^{m} (\mathbf{x} \cdot \hat{\mathbf{w}}_{i}) (\mathbf{x} \cdot \hat{\mathbf{w}}_{i}) (\hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{i}) \\
&amp; = \mathbf{x} \cdot \mathbf{x} - 2 \sum_{i=1}^{m}(\mathbf{x} \cdot \hat{\mathbf{w}}_{i})^{2} + \sum_{i=1}^{m} (\mathbf{x} \cdot \hat{\mathbf{w}}_{i})^{2} &amp; [\hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{i} = 1] \\
&amp; = \mathbf{x} \cdot \mathbf{x} - \sum_{i=1}^{m} (\mathbf{x} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\end{align}
</span></p>
</section>
<section id="pca-as-minimizing-projection-error" class="level4">
<h4 class="anchored" data-anchor-id="pca-as-minimizing-projection-error">PCA as minimizing projection error</h4>
<p>Given a dataset of <span class="math inline">n</span> instances and <span class="math inline">d</span> variables, we can use mean squared error to measure the reconstruction error of all instances in the dataset projected to <span class="math inline">m</span> dimensions:</p>
<p><span class="math display">
\begin{align}
\operatorname{MSE} &amp; = \frac{1}{n} \sum_{j=1}^{n} \lVert \mathbf{x}_{j} - \tilde{\mathbf{x}}_{j}\rVert^{2} \\
&amp; = \frac{1}{n} \sum_{j=1}^{n} \left( \mathbf{x}_{j} \cdot \mathbf{x}_{j} - \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \right) \\
&amp; = \frac{1}{n} \sum_{j=1}^{n} \mathbf{x}_{j} \cdot \mathbf{x}_{j} - \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\end{align}
</span></p>
<p>The goal of PCA is to get a particular orthonormal basis of only <span class="math inline">m</span> dimensions such that the mean squared error of projecting all instances on to it is minimized:</p>
<p><span class="math display">
\begin{align}
\min \quad &amp;  - \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\text{s.t. } \quad &amp; \hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{i} = 1, \quad i = 1, \dots m \\
\end{align}
</span></p>
<ol type="1">
<li>The constraint is added so that the <span class="math inline">\hat{\mathbf{w}}_{1}, \hat{\mathbf{w}}_{2}, \dots, \hat{\mathbf{w}}_{m}</span> are all unit vectors.</li>
<li>The first term <span class="math inline">\frac{1}{n} \sum_{j=1}^{n} \mathbf{x}_{j} \cdot \mathbf{x}_{j}</span> is omitted in the objective because it is a fixed value once the dataset is provided.</li>
</ol>
</section>
<section id="pca-as-maximizing-projection-variance" class="level4">
<h4 class="anchored" data-anchor-id="pca-as-maximizing-projection-variance">PCA as maximizing projection variance</h4>
<p>The objective of minimizing projection error defined above is the same as maximizing its negation.</p>
<p><span class="math display">
\begin{align}
\min \quad &amp; - \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\max \quad &amp; \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\end{align}
</span></p>
<p>The variance of the projection of all instances to the dimension <span class="math inline">\hat{\mathbf{w}}_{i}</span> is:</p>
<p><span class="math display">
\begin{align}
\operatorname{var}(X) &amp; = \mathbb{E}[X^{2}] - \mathbb{E}[X]^{2} \\
\operatorname{var}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) &amp; = \mathbb{E}[(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2}] - \mathbb{E}[\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}]^{2} \\
\operatorname{var}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) &amp; = \frac{1}{n}\sum_{j=1}^{n}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^2 - \left( \frac{1}{n}\sum_{j=1}^{n}\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i} \right)^2 \\
\end{align}
</span></p>
<p>The sum of the variances of the projections to all <span class="math inline">m</span> dimensions is:</p>
<p><span class="math display"> \sum_{i=1}^{m} \operatorname{var}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) = \sum_{i=1}^{m} \frac{1}{n}\sum_{j=1}^{n}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^2 - \sum_{i=1}^{m} \left( \frac{1}{n}\sum_{j=1}^{n}\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i} \right)^2 </span></p>
<p>Since the dataset is preprocessed to be zero-centered (each variable has mean 0), the last term of the equation above becomes 0:</p>
<p><span class="math display"> \sum_{i=1}^{m} \left( \frac{1}{n}\sum_{j=1}^{n}\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i} \right)^2 = \sum_{i=1}^{m} \left( \left( \frac{1}{n}\sum_{j=1}^{n}\mathbf{x}_{j} \right) \cdot \hat{\mathbf{w}}_{i} \right)^2 = \sum_{i=1}^{m} (0 \cdot \hat{\mathbf{w}}_{i})^{2} = 0 </span></p>
<p>Thus, we can see that minimizing projection error is the same as maximizing the sum of the projection variance:</p>
<p><span class="math display"> \sum_{i=1}^{m} \operatorname{var}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i}) = \sum_{i=1}^{m} \frac{1}{n}\sum_{j=1}^{n}(\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^2 = \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} </span></p>
</section>
</section>
<section id="solving-pca-objective" class="level2">
<h2 class="anchored" data-anchor-id="solving-pca-objective">## Solving PCA objective</h2>
<p>Given the minimization problem:</p>
<p><span class="math display">
\begin{align}
\min \quad &amp; - \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
\text{s.t. } \quad &amp; \hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{i} = 1, \quad i = 1, \dots m \\
\end{align}
</span> we can first rewrite the objective in the matrix form:</p>
<p><span class="math display">
\begin{align}
&amp; \frac{1}{n} \sum_{j=1}^{n} \sum_{i=1}^{m} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
= &amp; \sum_{i=1}^{m} \frac{1}{n} \sum_{j=1}^{n} (\mathbf{x}_{j} \cdot \hat{\mathbf{w}}_{i})^{2} \\
= &amp; \sum_{i=1}^{m} \frac{1}{n} (\mathbf{X}\hat{\mathbf{w}}_{i})^{T} (\mathbf{X}\hat{\mathbf{w}}_{i}) \\
= &amp; \sum_{i=1}^{m} \frac{1}{n} \hat{\mathbf{w}}_{i}^{T}\mathbf{X}^{T} \mathbf{X}\hat{\mathbf{w}}_{i} \\
= &amp; \sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T} \frac{\mathbf{X}^{T}\mathbf{X}}{n} \hat{\mathbf{w}}_{i} \\
\end{align}
</span></p>
<p>Since we have already zero-centered the dataset, we can replace <span class="math inline">\frac{\mathbf{X}^{T}\mathbf{X}}{n}</span> with the covariance matrix <span class="math inline">\mathbf{V}</span>. Thus, the minimization problem in the matrix form is:</p>
<p><span class="math display">
\begin{align}
\min \quad &amp; - \sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T} \mathbf{V} \hat{\mathbf{w}}_{i} \\
\text{s.t. } \quad &amp; \hat{\mathbf{w}}_{i} \cdot \hat{\mathbf{w}}_{i} = 1, \quad i = 1, \dots m \\
\end{align}
</span></p>
<p>The Lagrangian of the optimization problem is:</p>
<p><span class="math display"> L(\mathbf{w}_{1}, \dots, \mathbf{w}_{m}, \lambda_{1} \dots, \lambda_{m}) = - \sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T} \mathbf{V} \hat{\mathbf{w}}_{i} + \sum_{i=1}^{m} \lambda_{i}(\hat{\mathbf{w}}_{i}^{T}\hat{\mathbf{w}}_{i} - 1) </span></p>
<p>Solving <span class="math inline">L</span> by 1. Setting the derivative of <span class="math inline">L</span> w.r.t <span class="math inline">\hat{\mathbf{w}}_{i}</span> to be 0:</p>
<pre><code>$$
\begin{align}
\frac{\partial L}{\partial \hat{\mathbf{w}}}_{i} &amp; = 0 \\
-\sum_{i=1}^{m} 2\mathbf{V}\hat{\mathbf{w}}_{i} + \sum_{i=1}^{m} 2\lambda_{i}\hat{\mathbf{w}}_{i} &amp; = 0 \\
\sum_{i=1}^{m} 2\mathbf{V}\hat{\mathbf{w}}_{i} &amp; = \sum_{i=1}^{m} 2\lambda_{i}\hat{\mathbf{w}}_{i} \\
\mathbf{V}\hat{\mathbf{w}}_{i} &amp; = \lambda_{i}\hat{\mathbf{w}}_{i}, \quad i = 1, \dots m \\
\end{align}
$$

The results show that the results we want are the eigenvectors $\hat{\mathbf{w}}_{i}$ and eigenvalues $\lambda_{i}$ of $\mathbf{V}$. </code></pre>
<ol type="1">
<li><p>Setting the derivative of <span class="math inline">L</span> w.r.t <span class="math inline">\lambda_{i}</span> to be 0:</p>
<p><span class="math display">
\begin{align}
\frac{\partial L}{\partial \lambda_{i}} &amp; = 0 \\
\sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T}\hat{\mathbf{w}}_{i} - 1 &amp; = 0 \\
\hat{\mathbf{w}}_{i}^{T}\hat{\mathbf{w}}_{i} &amp; = 1, \quad i = 1, \dots m \\
\end{align}
</span></p>
<p>The constraints show that the eigenvectors must also be unit vectors.</p></li>
<li><p>Plug in the results back to the objective:</p>
<p><span class="math display"> -\sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T} \mathbf{V} \hat{\mathbf{w}}_{i} = - \sum_{i=1}^{m} \hat{\mathbf{w}}_{i}^{T} \lambda_{i} \hat{\mathbf{w}}_{i} = - \sum_{i=1}^{m} \lambda_{i} \hat{\mathbf{w}}_{i}^{T} \hat{\mathbf{w}}_{i} = - \sum_{i=1}^{m} \lambda_{i} </span></p>
<p>The last equation shows that we need to select the <span class="math inline">m</span> largest eigenvalues to minimize the objective.</p></li>
</ol>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<hr>
<ol type="1">
<li>https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c</li>
<li>https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf</li>
<li>https://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>