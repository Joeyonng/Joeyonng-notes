{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd8900e-ba4f-4638-9d5c-ac9e10aacd42",
   "metadata": {},
   "source": [
    "# Quantization Survey\n",
    "\n",
    "*07-28-2021*\n",
    "\n",
    "This page contains my reading notes on \n",
    "\n",
    "- [**A Survey of Quantization Methods for Efficient Neural Network Inference**](https://www.semanticscholar.org/paper/A-Survey-of-Quantization-Methods-for-Efficient-Gholami-Kim/04e283adccf66742130bde4a4dedcda8f549dd7e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe5671-f71f-4ec0-8957-87b4dec6fd0e",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "Given a full precision number $x$, which is either a weight or an activation in the network, we want to only use $2^{k}$ number of distinct values $\\hat{x}$ to replace $x$ in the inference time.\n",
    "1. $k$ here is called the bit-width.\n",
    "1. The goal is to reduce the inference time and memory usage because less number of distinct values uses less memory and benefit from integer arithmetic hardware.\n",
    "1. Unlike the application of quantization method used in signal processing, whose primary goal is to minimze the difference between the quantized values and the full-precision values, the quantization in neural network aims to minimize the accuracy drop, which can be achieved even if the average difference is huge. \n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967359f-90d0-412e-a3b4-eedc71ca3e3b",
   "metadata": {},
   "source": [
    "## Uniform Quantization\n",
    "1. Uniform quantization means that the values after the quantization are equally spaced:\n",
    "$$\\hat{x}_{n} - \\hat{x}_{n - 1} = \\hat{x}_{n - 1} - \\hat{x}_{n - 2}$$\n",
    "1. A widely used method of uniform quantization is as follows:\n",
    "    1. The quantization operator maps the real values to a set of consecutive integers in the range of $[-2^{k-1}, 2^{k-1} - 1]$:\n",
    "    $$ Q(x) = \\mathrm{round}(\\frac{x - b}{s}) $$\n",
    "    Here $s$ is the scaling factor, $b$ is the bias and $\\mathrm{round}()$ is to round the float to nearest integer.\n",
    "    1. Both $s$ and $b$ can be directly calculated if we have selected a range of $x$ to be $[\\alpha, \\beta]$:\n",
    "    $$ s = \\frac{\\beta - \\alpha}{2^{k} - 1} $$\n",
    "    $$ b = \\frac{\\beta - \\alpha}{2} $$\n",
    "    The scaling factor essentially divide the range $(\\alpha, \\beta)$ into $2^{k}$ numbers of same size partitions. The bias shifts the selected range to be zero centered. \n",
    "    1. Finally, the quantized value $\\hat{x}$ that should be used in the inference can be mapped from $Q(x)$:\n",
    "    $$ \\hat{x} = sQ(x) + b $$\n",
    "1. Symmetric and asymmetric quantization\n",
    "    1. If the selected range $[\\alpha, \\beta]$ is symmetric around 0 i.e. $\\alpha = -\\beta$, then the quantization is called symmertric. Otherwise, it is called asymmertric. \n",
    "    1. Symmetric quantization doesn't require b $(b=0)$ since the selected range is already zero centered. However, it can cause unused/over-used quantized value if the $x$ is not symmertric.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
