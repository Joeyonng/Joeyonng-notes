<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Joeyonng - Decision Tree (DT)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Joeyonng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Knowledge</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Notes" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng-backyard/" rel="" target="">
 <span class="menu-text">Backyard</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Decision Tree (DT)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Learning Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/1_Statistical_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/2_Bayesian_Classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Classifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/3_Effective_Class_Size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Effective Class Size</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Empirical Risk Minimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/5_Uniform_Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uniform Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/6_PAC_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PAC Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/7_Rademacher_Complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rademacher Complexity</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary">Preliminary</a>
  <ul class="collapse">
  <li><a href="#statistics" id="toc-statistics" class="nav-link" data-scroll-target="#statistics">Statistics</a></li>
  </ul></li>
  <li><a href="#tree-basics" id="toc-tree-basics" class="nav-link" data-scroll-target="#tree-basics">## Tree basics</a></li>
  <li><a href="#impurity-function" id="toc-impurity-function" class="nav-link" data-scroll-target="#impurity-function">Impurity function</a>
  <ul class="collapse">
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a></li>
  <li><a href="#regression" id="toc-regression" class="nav-link" data-scroll-target="#regression">Regression</a></li>
  </ul></li>
  <li><a href="#splitting-criteria" id="toc-splitting-criteria" class="nav-link" data-scroll-target="#splitting-criteria">## Splitting criteria</a></li>
  <li><a href="#stopping-condition" id="toc-stopping-condition" class="nav-link" data-scroll-target="#stopping-condition">Stopping condition</a></li>
  <li><a href="#label-assignment" id="toc-label-assignment" class="nav-link" data-scroll-target="#label-assignment">## Label assignment</a></li>
  <li><a href="#pruning" id="toc-pruning" class="nav-link" data-scroll-target="#pruning">Pruning</a></li>
  <li><a href="#cart-tree-building" id="toc-cart-tree-building" class="nav-link" data-scroll-target="#cart-tree-building">## CART Tree building</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Decision Tree (DT)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<hr>
<ol type="1">
<li>Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance.</li>
<li>To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances.</li>
<li>Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.</li>
</ol>
<section id="preliminary" class="level2">
<h2 class="anchored" data-anchor-id="preliminary">Preliminary</h2>
<hr>
<section id="statistics" class="level3">
<h3 class="anchored" data-anchor-id="statistics">Statistics</h3>
<section id="kullback-leibler-divergence-kl-divergence" class="level4">
<h4 class="anchored" data-anchor-id="kullback-leibler-divergence-kl-divergence">Kullback-Leibler Divergence (KL Divergence)</h4>
<p>KL Divergence is a method to measure <strong>difference between two probability distributions</strong> over the same random variable <span class="math inline">X</span>. Given a discrete random variable <span class="math inline">X</span> and two probability distributions <span class="math inline">P(X)</span> and <span class="math inline">Q(X)</span>, KL Divergence is defined as:</p>
<p><span class="math display">
\begin{align}
D_{KL}(P \Vert Q) &amp; = \mathbb{E}_{X \sim P}[\log P(X) - \log Q(X)] \\
&amp; = \sum_{x \in X} P(x) (\log P(x) - \log Q(x)) &amp; [\text{definition of expected value of } \log P(x) - \log Q(x)] \\
&amp; = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}  &amp; [\log a - \log b = \log \frac{a}{b}] \\
\end{align}
</span></p>
<ul>
<li><p>KL Divergence is not symmetric and thus cannot be used as an distance metric</p>
<p><span class="math display"> D_{KL}(P \Vert Q) \neq D_{KL}(Q \Vert P) </span></p></li>
</ul>
</section>
</section>
</section>
<section id="tree-basics" class="level2">
<h2 class="anchored" data-anchor-id="tree-basics">## Tree basics</h2>
<p>Decision tree is composed of <strong>nodes</strong> and <strong>edges</strong>. - Each node corresponds to a subset of the original dataset. - The root node is the original training dataset provided to train the decision tree. - The path from the root node to a node specifies how the subset of the dataset is partitioned from the original training dataset.</p>
<p>We follow the notations listed below: - A node: <span class="math inline">t</span> - A decision tree is a set of nodes: <span class="math inline">T</span> - The original training set: <span class="math inline">\mathbf{D}</span> - The subset of the dataset that corresponds to node <span class="math inline">t</span>: <span class="math inline">\mathbf{D}_{t}</span></p>
</section>
<section id="impurity-function" class="level2">
<h2 class="anchored" data-anchor-id="impurity-function">Impurity function</h2>
<hr>
<p>The <strong>impurity function</strong> <span class="math inline">F(\cdot)</span> measures the impureness of a set of labels. - <span class="math inline">F(\cdot)</span> achieves maximum only when the labels are in uniform distribution. - <span class="math inline">F(\cdot)</span> achieves minimum only when the labels provided are the same.</p>
<p>We use <span class="math inline">F(\mathbf{D})</span> in the following context to compute the impureness of the labels in the dataset <span class="math inline">\mathbf{D}</span> using the impurity function <span class="math inline">F</span>.</p>
<section id="classification" class="level3">
<h3 class="anchored" data-anchor-id="classification">Classification</h3>
<p>Given a dataset <span class="math inline">\mathbf{D}</span> with <span class="math inline">C</span> unique labels, <span class="math inline">P(c)</span> is the probability of label <span class="math inline">c</span> in the dataset, which is computed by dividing the number of instances with label <span class="math inline">c</span> by the total number instances in <span class="math inline">\mathbf{D}</span>.</p>
<ul>
<li>When there is only one class in <span class="math inline">\mathbf{D}</span>, the dataset is pure and thus impurity functions should return 0.</li>
<li>On the contrary, if all possible labels are in <span class="math inline">\mathbf{D}</span> and also have the same number of instances, <span class="math inline">\mathbf{D}</span> achieves the maximum impureness.</li>
</ul>
<section id="gini-impurity-index-gini-impurity" class="level4">
<h4 class="anchored" data-anchor-id="gini-impurity-index-gini-impurity">Gini Impurity Index (Gini Impurity)</h4>
<p>Gini impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.</p>
<p><span class="math display">
\begin{align}
F(\mathbf{D}) &amp; = \sum_{c \in C} P(c)(1 - P(c)) \\
&amp; = \sum_{c \in C} P(c) - P(c)^{2} \\
&amp; = 1 - \sum_{c \in C} P(c)^2 \\
\end{align}
</span></p>
</section>
<section id="shannon-entropy-entropy" class="level4">
<h4 class="anchored" data-anchor-id="shannon-entropy-entropy">Shannon Entropy (Entropy)</h4>
<p><span class="math display"> F(\mathbf{D}) = \sum_{c \in C} P(c) \log P(c) </span></p>
<p>Entropy can be thought as the difference measured by KL Divergence between the probability distribution of the unique labels represented in the current dataset <span class="math inline">\mathbf{D}</span> and the distribution of the most impure dataset. - The larger the entropy, the more far away from a uniform distribution is the distribution of the labels represented by <span class="math inline">\mathbf{D}</span>.</p>
<p><span class="math display">
\begin{align}
D_{KL}(P \Vert Q) &amp; = \sum_{x \in X} P(x)(\log P(x) - \log Q(x)) &amp; [\text{KL Divergence definition}] \\
&amp; = \sum_{c \in C} P(c)(\log P(c) - \log Q(c)) &amp; [\text{substitute } x \text{ with label } c] \\
&amp; = \sum_{c \in C} P(c)(\log P(c) - \log \frac{1}{C}) &amp; [Q(c) = \frac{1}{C} \text{ is uniform distribution}] \\
&amp; = \sum_{c \in C} P(c)(\log P(c) - (\log 1 - \log C)) \\
&amp; = \sum_{c \in C} P(c)(\log P(c) + \log C) \\
&amp; = \sum_{c \in C} P(c)\log P(c) + \log C \sum_{c \in C} P(c) \\
&amp; = \sum_{c \in C} P(c) \log P(c) + \log C &amp; [\sum_{c \in C} P(c) = 1] \\
&amp; = \sum_{c \in C} P(c) \log P(c) &amp; [\log C \text{ is a constant and can be dropped}] \\
\end{align}
</span></p>
</section>
</section>
<section id="regression" class="level3">
<h3 class="anchored" data-anchor-id="regression">Regression</h3>
<p>Given a dataset <span class="math inline">\mathbf{D}</span> with continuous labels <span class="math inline">\{y_{1}, y_{2}, \dots, y_{n}\}</span>, impurity functions can be defined a similar way. - If the labels in <span class="math inline">\mathbf{D}</span> are very similar (low variance), the impurity functions should return a value closed to 0. - If the labels in <span class="math inline">\mathbf{D}</span> are very different from each other (high variance), impurity functions should return a very large value.</p>
<section id="mean-squared-error" class="level4">
<h4 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h4>
<p>A dataset’s impurity can be simply measured by the mean squared error.</p>
<p><span class="math display"> F(\mathbf{D}) = \frac{1}{N} \sum_{i}^{n} (y_{i} - \bar{y})^{2} </span></p>
<p>where <span class="math inline">\bar{y}</span> is the mean value of the labels in dataset <span class="math inline">\mathbf{D}</span>.</p>
</section>
</section>
</section>
<section id="splitting-criteria" class="level2">
<h2 class="anchored" data-anchor-id="splitting-criteria">## Splitting criteria</h2>
<p>A <strong>split</strong> is a way that divides a feature space into different groups and is used in the tree building process to split a node to children nodes. - Binary split (2-way split): split a feature space into 2 groups. A node will have 2 sub-nodes. - k-way split: split a feature space into k groups. A node will have k sub-nodes.</p>
<p>The most important question of building a decision tree is how to choose the best split to split a node (dataset) into different child nodes (sub-datasets). - A <strong>splitting criteria</strong> is a function that measures the impurity difference between the dataset before splitting and the datasets after splitting. - The best split <span class="math inline">s</span> for the node <span class="math inline">t</span> should be the one that has the maximum splitting criteria <span class="math inline">\Delta F(\mathbf{D}_{t}, s)</span>.</p>
<p>Given a set of datasets <span class="math inline">D = \{ \mathbf{D}_{1}, \mathbf{D}_{2}, \dots, \mathbf{D}_{k} \}</span> created by applying split <span class="math inline">s</span> to the dataset <span class="math inline">\mathbf{D}_{t}</span>, the splitting criteria is defined as:</p>
<p><span class="math display"> \Delta F(\mathbf{D}_{t}, s) = F(\mathbf{D}_{t}) - \sum_{\mathbf{D}_{t} \in D} \frac{\lvert \mathbf{D} \rvert}{\lvert \mathbf{D}_{t} \rvert} F(\mathbf{D}) </span></p>
<p>If <span class="math inline">F(\cdot)</span> is the entropy function, <span class="math inline">\Delta F(\mathbf{D}, s)</span> is called <strong>Information Gain</strong>.</p>
</section>
<section id="stopping-condition" class="level2">
<h2 class="anchored" data-anchor-id="stopping-condition">Stopping condition</h2>
<hr>
<p>Each split produces new nodes that recursively become the starting points for new splits. - A node stops splitting when certain <strong>stopping conditions</strong> are satisfied. and such nodes are <strong>leaf nodes</strong>. - The leaf node doesn’t have children but has a label according to the dataset it corresponds to.</p>
<p>The basic stopping condition is that the dataset that the leaf node corresponds to has impureness of 0 (single training instance or all training instance have the same label), in which case the splitting stops because there is no need to reduce the impureness.</p>
<p>However, always splitting into pureness usually induces overfitting. Thus, there are other stopping conditions that can achieve <strong>early stopping</strong> to avoid overfitting. - Dataset size is below a threshold. - Splitting criteria improvement is below a threshold. - Tree depth is above a threshold. - Number of nodes is above a threshold.</p>
</section>
<section id="label-assignment" class="level2">
<h2 class="anchored" data-anchor-id="label-assignment">## Label assignment</h2>
<p>For each node, we can assign a label to the node according to the labels of the dataset it corresponds to. - Classification: majority label. - Regression: mean label.</p>
<p>Every node can have a label assigned, but only leaf nodes use labels.</p>
<section id="misclassification-cost" class="level4">
<h4 class="anchored" data-anchor-id="misclassification-cost">Misclassification cost</h4>
<p>Assuming the assigned label of the node <span class="math inline">t</span> is <span class="math inline">y_{t}</span>, the <strong>misclassification cost</strong> of a node <span class="math inline">t</span> is - Classification:</p>
<pre><code>$$ r(t) = 1 - P(y_{t}) $$</code></pre>
<ul>
<li><p>Regression:</p>
<p><span class="math display"> r(t) = \frac{1}{N(t)} \sum_{y \in t} (y - y_{t}) </span></p>
<p>where <span class="math inline">y \in t</span> means all labels in the dataset that node <span class="math inline">t</span> corresponds to.</p></li>
</ul>
<p>Then weighted misclassification cost of the node <span class="math inline">t</span> is defined as the product of misclassification cost and the probability of picking an instance that is in the node <span class="math inline">t</span>.</p>
<p><span class="math display"> R(t) = P(t) r(t) = \frac{\lvert \mathbf{D}_{t} \rvert}{\lvert \mathbf{D} \rvert} r(t) </span></p>
<p>Then the misclassification cost of the a tree <span class="math inline">T</span> is</p>
<p><span class="math display"> R(T) = \sum_{t \in \hat{T}} R(t) </span></p>
<p>where <span class="math inline">\hat{T}</span> is the set of the leaf nodes in tree <span class="math inline">T</span>.</p>
<p>The weighted misclassification cost of a node <span class="math inline">t</span> is always higher than the sum of the weighted misclassification costs of the children nodes <span class="math inline">T_{c} = \{t_{1}, t_{2}, \dots, t_{k}\}</span> that <span class="math inline">t</span> splits to.</p>
<p><span class="math display"> R(t) \geq \sum_{t_{i} \in T_{c}} R(t_{i}) </span></p>
<p>Thus, if we split one of the leaf nodes of <span class="math inline">T</span> to get a new and larger tree <span class="math inline">T'</span>, then</p>
<p><span class="math display"> R(T) \geq R(T') </span></p>
<p>This shows that the misclassification cost of a tree will always decrease or stay the same if we continue to split its leaf nodes.</p>
</section>
</section>
<section id="pruning" class="level2">
<h2 class="anchored" data-anchor-id="pruning">Pruning</h2>
<hr>
<p>Another way to avoid overfitting is through <strong>pruning</strong>, which is the process to make some internal nodes leaf nodes and remove their children from a sufficiently large tree <span class="math inline">T_{max}</span> that is rooted at <span class="math inline">t_{max}</span>.</p>
<section id="minimal-cost-complexity-pruning" class="level4">
<h4 class="anchored" data-anchor-id="minimal-cost-complexity-pruning">Minimal cost-complexity pruning</h4>
<p>Previously we show that <span class="math inline">R(T)</span> is not a good measure of the performance of a tree because it always favors a larger tree. Thus we introduce another metric called <strong>cost-complexity</strong> that also considers the size of the tree. - If we consider each node has a complexity of <span class="math inline">\alpha</span>, the cost-complexity of a node <span class="math inline">t</span> is</p>
<pre><code>$$ R_{\alpha}(t) = R(t) + \alpha  $$
    </code></pre>
<ul>
<li><p>Thus, the cost-complexity of a tree <span class="math inline">T</span> is</p>
<p><span class="math display"> R_{\alpha}(T) = \sum_{t \in \hat{T}} R_{\alpha}(t) = \sum_{t \in \hat{T}} (R(t) + \alpha) = R(T) + \alpha \lvert \hat{T} \rvert </span></p>
<p>where <span class="math inline">\hat{T}</span> is the set of leaf nodes of <span class="math inline">T</span>.</p></li>
</ul>
<p>Cost-complexity can be seen as adding a regularization term that penalize the complexity of the tree to the misclassification cost. - <span class="math inline">\alpha</span> is the regularization parameter that balances the training accuracy and tree complexity.</p>
<ul>
<li>Given <span class="math inline">\alpha</span>, the goal of the pruning of <span class="math inline">T_{max}</span> is to get a pruned tree <span class="math inline">\hat{T}_{max}</span> (a subtree of <span class="math inline">T_{max}</span> that also rooted at <span class="math inline">t_{max}</span>) that minimizes <span class="math inline">R_{\alpha}(\hat{T}_{max})</span>.</li>
</ul>
</section>
<section id="weakest-link-cutting" class="level4">
<h4 class="anchored" data-anchor-id="weakest-link-cutting">Weakest-link cutting</h4>
<p><strong>Weakest-link cutting</strong> is an efficient way of doing the minimal cost-complexity pruning.</p>
<p>If the tree <span class="math inline">T</span> is pruned by deleting subtree <span class="math inline">T_{t}</span> rooted at the node <span class="math inline">t</span> (replaced with <span class="math inline">t</span>), the cost-complexity difference between pruned tree <span class="math inline">\hat{T}</span> and unpruned tree <span class="math inline">T</span> is</p>
<p><span class="math display"> R_{\alpha}(\hat{T}) - R_{\alpha}(T) = R_{\alpha}(t) - R_{\alpha}(T_{t}) </span></p>
<ul>
<li><p>If <span class="math inline">\alpha = 0</span>, <span class="math inline">R_{\alpha}(t) - R_{\alpha}(T_{t}) = R(t) - R(T_{t}) \geq 0</span>.</p></li>
<li><p>As <span class="math inline">\alpha</span> becomes larger, <span class="math inline">R_{\alpha}(t) - R_{\alpha}(T_{t})</span> is getting smaller and will eventually becomes <span class="math inline">&lt; 0</span>, since <span class="math inline">\alpha</span> is increasing slower than <span class="math inline">\alpha \lvert \hat{T} \rvert</span>.<br>
</p></li>
<li><p>Given a sufficiently large <span class="math inline">\alpha</span>, <span class="math inline">R_{\alpha}(t) - R_{\alpha}(T_{t}) &lt; 0</span>, which means that the cost-complexity of the node <span class="math inline">t</span> is better than its subtree <span class="math inline">T_{t}</span>, and thus <span class="math inline">T_{t}</span> should be pruned.</p></li>
</ul>
<p>Given a tree <span class="math inline">T</span>, the <strong>weakest link</strong> <span class="math inline">\bar{t}</span> is the <em>internal node</em> in <span class="math inline">T</span> that achieves <span class="math inline">R_{\alpha}(\bar{t}) - R_{\alpha}(T_{\bar{t}}) = 0</span> with the smallest <span class="math inline">\alpha</span> value.</p>
<ul>
<li><p>The <span class="math inline">\alpha</span> value that achieves <span class="math inline">R_{\alpha}(t) - R_{\alpha}(T_{t}) = 0</span> can be directly calculated.</p>
<p><span class="math display">
  \begin{align}
  R_{\alpha}(t) - R_{\alpha}(T_{t}) &amp; = 0 \\
  R(t) + \alpha - (R(T) + \alpha \lvert \hat{T} \rvert) &amp; = 0 \\
  R(t) - R(T) + \alpha (1 + \lvert \hat{T} \rvert) &amp; = 0 \\
  \alpha &amp; = \frac{R(t) - R(T_{t})}{\lvert T_{t} \rvert - 1} \\
  \end{align}
  </span></p></li>
<li><p>The weakest link is defined as</p>
<p><span class="math display"> \bar{t} = \arg \min_{t \in T \setminus \hat{T}} \frac{R(t) - R(T_{t})}{\lvert T_{t} \rvert - 1} </span></p>
<p>where <span class="math inline">T \setminus \hat{T}</span> means the set of the internal nodes of <span class="math inline">T</span>.</p></li>
<li><p>If there are more than 1 internal node that achieves <span class="math inline">R_{\alpha}(\bar{t}) - R_{\alpha}(T_{\bar{t}}) = 0</span> with same minimum <span class="math inline">\alpha</span> value, they are all called the weakest links.</p></li>
</ul>
<p>Weakest-link cutting finds the optimal subtree <span class="math inline">\hat{T}</span> of <span class="math inline">T_{max}</span> that minimizes <span class="math inline">R_{\alpha}(\hat{T})</span> with a predefined threshold <span class="math inline">\alpha_{max}</span> in a iterative way.</p>
<ul>
<li><p>We start the pruning process by first removing from <span class="math inline">T_{max}</span> the subtrees <span class="math inline">T_{t}</span> rooted at nodes <span class="math inline">t</span> that have already achieved <span class="math inline">R_{\alpha}(t) - R_{\alpha}(T_{t}) = 0</span>. We denote the resulting tree <span class="math inline">T_{0}</span>.</p></li>
<li><p>In each iteration <span class="math inline">i</span>, the weakest link(s) <span class="math inline">\bar{t}</span> of tree <span class="math inline">T_{i - 1}</span> is identified by calculating</p>
<p><span class="math display"> \bar{t} = \arg \min_{t \in T_{i - 1} \setminus \hat{T}_{i - 1}} \frac{R(t) - R(T_{t})}{\lvert T_{t} \rvert - 1} </span></p>
<p>In the meantime, we can also calculate the <span class="math inline">\alpha_{i}</span> that identifies the weakest link(s).</p>
<p><span class="math display"> \alpha_{i} = \min_{t \in T_{i - 1} \setminus \hat{T}_{i - 1}} \frac{R(t) - R(T_{t})}{\lvert T_{t} \rvert - 1} </span></p></li>
<li><p>We replace <span class="math inline">T_{\bar{t}}</span> (the subtree rooted at <span class="math inline">\bar{t}</span>) by <span class="math inline">\bar{t}</span> and denote the resulting tree <span class="math inline">T_{i}</span>.</p></li>
<li><p>Continue the iteration until the minimum <span class="math inline">\alpha</span> required to achieve <span class="math inline">R_{\alpha}(\bar{t}) - R_{\alpha}(T_{\bar{t}}) = 0</span> is above a predefined threshold <span class="math inline">\alpha_{max}</span>.</p></li>
</ul>
</section>
</section>
<section id="cart-tree-building" class="level2">
<h2 class="anchored" data-anchor-id="cart-tree-building">## CART Tree building</h2>
<section id="identify-all-possible-splits" class="level4">
<h4 class="anchored" data-anchor-id="identify-all-possible-splits">Identify all possible splits</h4>
<p>CART considers binary split of a single feature for each node (each node only splits a one feature and only has 2 children). - For a categorical feature that has <span class="math inline">k</span> distinct values, CART considers all possible ways to split the <em>k</em> distinct values into 2 groups. - The maximum ways of splitting is <span class="math inline">2^{k - 1} - 1</span>. - e.g.&nbsp;If the categorical feature has 4 distinct values: <span class="math inline">\{1, 2, 3, 4\}</span>, then all possible splits are</p>
<pre><code>    |index|1|2|3|4|5|6|7|
    |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
    |left child|{1}|{2}|{3}|{4}|{1, 2}|{1, 3}|{1, 4}|
    |right child|{2, 3, 4}|{1, 3, 4}|1, 2, 4}|{1, 2, 3}|{3, 4}|{2, 4}|{2, 3}|</code></pre>
<ul>
<li>For a numerical feature that has <span class="math inline">k</span> distinct values appeared in the dataset, CART considers all the intervals between 2 consecutive values as the splits.
<ul>
<li><p>The maximum ways of splitting is <span class="math inline">k - 1</span>.</p></li>
<li><p>e.g.&nbsp;If the numerical feature has 6 distinct values: <span class="math inline">\{-5.0, 1.0, 3.0, 5.0, 7.0, 11.0\}</span>, then all possible splits are</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">index</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">left child</td>
<td style="text-align: center;"><span class="math display"> \leq -2.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 2.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 4.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 6.0 </span></td>
<td style="text-align: center;"><span class="math display"> \leq 9.0 </span></td>
</tr>
<tr class="even">
<td style="text-align: center;">right child</td>
<td style="text-align: center;"><span class="math display"> &gt; -2.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 2.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 4.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 6.0 </span></td>
<td style="text-align: center;"><span class="math display"> &gt; 9.0 </span></td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul>
<p>At a given node, CART considers all possible splits of all features and chooses the one that has the maximum splitting criteria.</p>
</section>
<section id="recursive-tree-building" class="level4">
<h4 class="anchored" data-anchor-id="recursive-tree-building">Recursive tree building</h4>
<ol type="1">
<li>Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split.</li>
<li>Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split.</li>
<li>Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset.</li>
<li>The splitting stops when no further splitting can be made (the dataset contains only one class).</li>
</ol>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<hr>
<ol type="1">
<li>https://victorzhou.com/blog/intro-to-random-forests/</li>
<li>https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf</li>
<li>https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html</li>
<li>http://www.odbms.org/wp-content/uploads/2014/07/DecisionTrees.pdf</li>
<li>https://scientistcafe.com/ids/splitting-criteria.html</li>
<li>https://online.stat.psu.edu/stat508/book/export/html/647</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>