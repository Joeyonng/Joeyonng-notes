[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Litao Qiao 乔立涛",
    "section": "",
    "text": "Hello! I’m Litao Qiao, also known online as Joeyonng. I’m currently a PhD student in ECE department of University of California, San Diego (UCSD) since 2020. UCSD is also where I earned my bachelor’s in 2018.\nMy primary research interest is Explainable machine learning and my expertise extends to general machine learning theory, combinatorial logic, and deep learning.\nThis website serves as a platform for sharing my learned knowledge and research notes. Additionally, I enjoy developing web applications and you can find some of my side projects in my Github."
  },
  {
    "objectID": "index.html#selected-papers",
    "href": "index.html#selected-papers",
    "title": "Litao Qiao 乔立涛",
    "section": "Selected Papers",
    "text": "Selected Papers\nQiao, Litao, Weijia Wang, Sanjoy Dasgupta and Bill Lin. “Rethinking Logic Minimization for Tabular Machine Learning.” IEEE Transactions on Artificial Intelligence 4 (2023): 1129-1140.\nQiao, Litao, Weijia Wang and Bill Lin. “Alternative Formulations of Decision Rule Learning from Neural Networks.” Mach. Learn. Knowl. Extr. 5 (2023): 937-956.\nWang, Weijia, Litao Qiao and Bill Lin. “Disjunctive Threshold Networks for Tabular Data Classification.” IEEE Open Journal of the Computer Society 4 (2023): 185-194.\nQiao, Litao, Weijia Wang and Bill Lin. “Learning Accurate and Interpretable Decision Rule Sets from Neural Networks.” AAAI Conference on Artificial Intelligence (2021)."
  },
  {
    "objectID": "Notes/Birkhoff+.html",
    "href": "Notes/Birkhoff+.html",
    "title": "Birkhoff+",
    "section": "",
    "text": "07-12-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/Birkhoff+.html#problem-definition",
    "href": "Notes/Birkhoff+.html#problem-definition",
    "title": "Birkhoff+",
    "section": "Problem definition:",
    "text": "Problem definition:\nFor a given n \\times n doubly stochastic matrix X^{\\star} and an \\epsilon \\geq 0, our goal is to find a small collection of permutation matrices P_{1}, P_{2}, \\ldots, P_{k}, and weights \\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k} with \\sum_{i=1}^{k}\\theta_{i} \\leq 1 such that \\lVert X^{\\star} - \\sum_{i=1}^{k}\\theta_{i}P_{i} \\rVert_{F} \\leq \\epsilon."
  },
  {
    "objectID": "Notes/Birkhoff+.html#definitions",
    "href": "Notes/Birkhoff+.html#definitions",
    "title": "Birkhoff+",
    "section": "Definitions",
    "text": "Definitions\n\nWhen applied to vectors or matrices, +, -, \\cdot are all element-wise operations and we write x &lt; (\\leq, &gt;, \\geq ) y when all elements of x are &lt; (\\leq, &gt;, \\geq) of all elements of y.\nDoubly stochastic matrix: a doubly stochastic matrix is a square matrix of non-negative real numbers, each of whose rows and columns sums to 1.\nFrobenius norm: \\lVert X \\rVert_{F} = \\sqrt{\\sum_{i, j}\\vert X(i, j) \\vert^2}, which is the same as the l2 norm of a vector: \\lVert x \\rVert_{2} = \\sqrt{\\sum_{i}\\vert x(i) \\vert^2}\nConvex hull \\mathrm{conv}(A): The convex hull of set A (\\mathrm{conv}(A)) is the smallest convex set that contains set A.\nBirkhoff polytope \\mathcal{B}: the convex set that contains all doubly stochastic matrices of size n \\times n.\nPermutation matrices \\mathcal{P}: the extreme points of the Birkhoff polytope.\nLinear program \\mathrm{LP}(c, \\mathcal{X}): minimize c^{T}x, subject to x \\in \\mathcal{X}. We assume that the solution returned is always an extreme point (x \\in \\mathcal{P} if \\mathcal{X} = \\mathcal{B}).\nBirkhoff step: \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) = \\min_{a,b}\\{(X^{\\star}(a, b) - X_{k-1}(a,b) - 1)P_{k}(a,b) + 1\\}. It computes the step size (weight) by taking the minimum non-zero element of the difference matrix (masked by the permutation matrix P_{k}) between X^{\\star} and X."
  },
  {
    "objectID": "Notes/Birkhoff+.html#algorithms-all-matrices-x-are-represented-using-vectors-x-by-stacking-the-matrix-columns",
    "href": "Notes/Birkhoff+.html#algorithms-all-matrices-x-are-represented-using-vectors-x-by-stacking-the-matrix-columns",
    "title": "Birkhoff+",
    "section": "Algorithms (All matrices (X) are represented using vectors (x) by stacking the matrix columns)",
    "text": "Algorithms (All matrices (X) are represented using vectors (x) by stacking the matrix columns)\n\nGeneral Birkhoff algorithm (Algorithm 1, 2 and 3)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1}\\theta_{i}) \\mathbin{/} n^2 // (Why this value)\n  p_{k} \\gets p \\in \\mathcal{I}_{k}(\\alpha)   // Get next permutation matrix\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)   // Get next weight based on the new permutation matrix\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nn is the size of X^{\\star} and d=n^2 is the number of elements in X^{\\star}.\n\\mathcal{I}_{k}(\\alpha) =\\{P \\in \\mathcal{P} \\mid X_{k-1}(a,b) + \\alpha P(a,b) \\leq X^{\\star}(a,b) \\}.\n\nIf P_{k} \\in \\mathcal{I}_{k}(\\alpha), then (X^{\\star} - X_{k-1}) \\cdot P_{k} \\geq \\alpha, (each element of the difference matrix between X^{\\star} and X_{k-1} masked by P_{k} is \\geq \\alpha).\nSince \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) is taking the minimum of non-zero element of (X^{\\star} - X_{k-1}) \\cdot P_{k} (definition 7), \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) \\geq \\alpha\n\nDue to the definitions of \\mathcal{I}_{k}(\\alpha) and \\mathrm{STEP}, x_{k} \\leq x^{\\star} all the time.\n\nBirkhoff algorithm can be seen as constructing a path from the origin (x_{0} = 0) to x^{\\star} while always remaining in a dotted box, where x_{0} and x^{\\star} are two diagonal vertices of the box (x_{k} \\leq x^{\\star}).\n\nGeneral Birkhoff algorithm doesn’t specify which specific permutation to select from \\mathcal{I}_{k}(\\alpha).\n\n\nClassic Birkhoff algorithm (Algorithm 4)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  p_k \\gets \\mathrm{LP}(-\\lceil x^{\\star} - x_{k-1} \\rceil, \\mathcal{B}).\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)  \n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nThe only slight difference is line 3, which is essentially doing the same thing with general Birkhoff algorithm.\n\nSince x^{\\star} - x_{k-1} \\in [0, 1], its ceiling (\\lceil x^{\\star} - x_{k-1} \\rceil) makes all nonzero elements to be 1, which makes \\mathrm{LP} treat all nonzero elements equally.\n\n\n\n\n\nFrank-Wolfe (FW) algorithm (Algorithm 5)\n\n\nx_{0} \\in \\mathcal{P}, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  p_k \\gets \\mathrm{LP}(-(x^{\\star} - x_{k-1}), \\mathcal{B}) // Get the next extreme point\n  \\theta_{k} \\gets (x^{\\star} - x_{k-1})^{T}(p_{k} - x_{k-1}) \\mathbin{/} \\lVert p_{k} - x_{k-1} \\rVert_{2}^{2} // Calculate the step size\n  x_{k} \\gets x_{k-1} + \\theta_{k}(p_{k} - x_{k-1}) // Update x\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nThe assumption of the FW algorithm is that there always exists an extreme point in the Birkhoff polytope (permutation matrix p_{k}) that is a direction (p_{k} - x_{k-1}) in which it is possible to improve the objective function.\n\nSince each extreme point of the Birkhoff polytope is an permutation matrix and thus the each step size can be considered as the weight of the permutation matrix, FW algorithm can be used to solve Birkhoff problem.\n\nThe paper selects the objective function to be f(x) = \\frac{1}{2} \\lVert x - x^{\\star} \\rVert_{2}^{2}, which measures the square of the l_{2} norm of the difference between the current matrix x_{k} and the objective matrix x^{\\star}.\n\nThe solving of \\mathrm{LP} at line 3 is essentially trying to find an extreme point p_{k} in the Birkhoff polytope by solving {\\mathrm{argmin}_{u \\in \\mathcal{P}}}\\nabla f(x_{k-1})^{T}u, in which \\nabla f(x_{k-1}) = -(x^{\\star} - x_{k-1}).\nThe way to calculate \\theta_{k} at line 4 is derived by solving f(x_{k}) - f(x_{k-1}).\nNote at line 5 x_{k} = x_{k-1} + \\theta_{k}(p_{k} - x_{k-1}) = (1 - \\theta)x_{k-1} + \\theta_{k}p_{k}, so x_{k} is still a linear combination of permutation matrices.\n\nDifference with the Birkhoff algorithm:\n\nFW uses a random extreme point instead of 0 as the starting point, since it requires x_{k} \\in \\mathcal{B} at all time.\nFW does not require that x_{k} \\leq x^{\\star} and thus x^{\\star} can be approximated from any direction.\n\n\n\nFully Corrective Frank-Wolfe (Algorithm 6)\n\nFCFW provides the best approximation with the number of extreme points selected up to iteration k\n\n\nfrom IPython.display import Image\nImage(filename='1.png')"
  },
  {
    "objectID": "Notes/Birkhoff+.html#birkhoff-algorithm-algorithm-7",
    "href": "Notes/Birkhoff+.html#birkhoff-algorithm-algorithm-7",
    "title": "Birkhoff+",
    "section": "Birkhoff+ algorithm (Algorithm 7)",
    "text": "Birkhoff+ algorithm (Algorithm 7)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1} \\theta_i) \\mathbin{/} n^2\n  p_{k} \\gets \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha)))\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nBirkhoff+ combines Birkhoff algorithm and FW algorithm. This approach wants to use the path or permutations that FW would select while remaining in the box that characterizes the Birkhoff’s approach.\nThe only real difference between Birkhoff+ and Birkhoff is the way to compute p_{k} (line 4).\nf_{\\beta}(x) = f(x) - \\beta \\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j) + \\frac{\\epsilon}{d}), which consists f(x) (same as what is used in FW) and - \\beta \\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j) + \\frac{\\epsilon}{d}) (the barrier).\n\nThe barrier is a penalty term that restricts the x_{k} to always remain in the Birkhoff box.\n\nIf we ignore \\frac{\\epsilon}{d} and \\beta, -\\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j)) \\to \\infty if any x^{\\star}(j) \\approx x(j). That is, the penalty will be very high if any of the element of x is closed to x^{\\star}.\nSince x_{0} = 0 &lt; x^{\\star} at the starting point and the barrier will prevent all elements of x getting closed to x^{\\star}, the barrier can prevent x &gt; x^{\\star}.\n\n\\beta is a hyper-parameter that balances the objective function and the barrier. In the code, it is tuned smaller and smaller as the x_{k} is getting closed to x^{\\star}.\n\\frac{\\epsilon}{d} is used to avoid \\log \\to \\infty.\n\n\\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha))) = \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}) + b_{k}, \\mathcal{B}), where b_{k} = \\frac{\\epsilon}{d} \\cdot \\mathbb{1}(x^{\\star} - x_{k-1} \\leq \\alpha) is a penalty vector to force the solver to do not select the elements of vector (x^{\\star} - x_{k}) smaller than \\alpha.\n\nFor all elements of vector (x^{\\star} - x_{k}) smaller than \\alpha, b_{k} = \\frac{\\epsilon}{d}. Otherwise, b_{k} = 0.\n\n\n\nBirkhoff+ (max_rep) algorithm (Algorithm 8)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1} \\theta_i) \\mathbin{/} n^2\n  for i = 1,\\ldots, \\mathrm{max\\_rep} do\n    p_{i} \\gets \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha)))\n    \\theta_{i} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_{k})\n    if (\\theta_{i} &gt; \\alpha) \\alpha \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_{k})\n    else exit while loop\n    p_{k} \\gets p_{i}\n  end for\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nInstead of using a constant \\alpha, Birkhoff+ (max_rep) searches the largest \\alpha in each iteration by alternatively updating \\alpha and p_{k} until \\alpha cannot be updated larger or \\mathrm{max\\_rep} iterations is reached.\n\nLarger \\alpha \\to larger \\theta \\to less number of permutation matrices.\n\\alpha is set to be the largest step size (weight from \\mathrm{STEP} function) found so far.\n\n\n\nfrom IPython.display import Image\nImage(filename='2.png') \n\n\n\n\n\n\nSource code with comment\nhttps://github.com/vvalls/BirkhoffDecomposition.jl\n\nusing JuMP\nusing Clp\nusing Random\nusing SparseArrays\n\n\n# functionsBD.jl\n\nstruct polytope\n    A;\n    b;\n    l;\n    u;\n    model;\n    x;\nend\n\n@doc raw\"\"\"\nSolve a linear programing problem\n\"\"\"\nfunction LP(c, P)\n    @objective(P.model, Min, c'* P.x)\n    optimize!(P.model)\n    return value.(P.x)\nend\n\n@doc raw\"\"\"\nGet a random stochastic matrix\n\"\"\"\nfunction randomDoublyStochasticMatrix(n; num_perm=n^2)\n    M = zeros(n, n);\n    α = rand(num_perm)\n\n    α = α / sum(α);\n\n    for i = 1:num_perm\n        perm = randperm(n);\n        for j = 1:n\n            M[perm[j], j] += α[i];\n        end\n    end\n\n    return M;\nend\n\n\n@doc raw\"\"\"\nCreate Birkhoff polytope ``\\mathcal{B}`` (Section V-A), which contains all possible doublely stochastic matrices.\n\nSince the paper assumes that the solutions by solving linear programs over are Birkhoff polytope extreme points, \nthe solutions are permutation matrices (which are also doublely stochastic). \n\"\"\"\nfunction birkhoffPolytope(n)\n    # x is a doublely stochastic matrix that is represented by a vector (flattened).\n    # Use a constant matrix A(M') and a constant vector b to specify that x is doublely stochastic.\n    \n    M = zeros(n*n, 2*n);\n    # Specify the sum of each row of x equals to 1\n    for i = 1:n\n        M[(i-1)*n*n + (i-1)*n + 1 : (i-1)*n*n + (i-1)*n + n] = ones(n,1);\n    end\n    # Specify the sum of each col of x equals to 1\n    for i = 1:n\n        for j=1:n\n            M[n*n*n + (i-1)*n*n + (j-1)*n + i] = 1;\n        end\n    end\n\n    A = sparse(M');\n    b = ones(2*n);\n\n    model = Model(Clp.Optimizer)\n    set_silent(model)\n    @variable(model, 0 &lt;= x[1:n*n] &lt;= 1)\n    @constraint(model, A * x .== b)\n\n    return polytope(A, b, 0, 1, model, x)\n\nend\n\nbirkhoffPolytope\n\n\n\n# stepsizes.jl\n\n@doc raw\"\"\"\nGet the step size (weight) by taking the minimum non-zero entry of the difference matrix \n(masked by the permutation matrix y) between x_star and x.\n\"\"\"\nfunction getBirkhoffStepSize(x_star, x, y)\n    return minimum((x_star - x).*y - (y.-1));\nend\n\ngetBirkhoffStepSize\n\n\n\n# extremepoints.jl\n\n@doc raw\"\"\"\nBirkhoff+ (max_rep) Psudocode line 4-10\n\"\"\"\nfunction getEPBplus(x_star, x, B, max_rep, ε)\n    n = sqrt(size(x_star, 1))\n    d = size(x_star, 1);\n    i = 1;\n    y = 0;\n    α = 0;\n\n    while(i &lt;= max_rep)\n        # Calculate \\beta for this iteration.\n        # \\beta should become smaller and smaller.\n        z = Int16.(x_star - x .&gt; ε)\n        s = getBirkhoffStepSize(x_star, x, z)\n        beta = (s + ε/d)*0.5\n\n        # c is the gradient of the objective function with barrier.\n        # b is an iterm added to make I_k(\\alpha) to be B (Birkhoff polytope).\n        # See the paragraph in section VI.B after Corollary 2 for b.\n        c = -ones(d) + beta ./ (x_star - x .+ ε/d)\n        b = (n/ε).*Int16.(x_star - x .&lt;= α)\n        y_iter = LP(c + b, B);\n        \n        # If new solution (y_iter) makes objective function larger/worse (c'*y_iter &gt; c'*y_z),\n        # fall back to the solution from the last iteration (y_z/x).\n        y_z = x;\n        if(c'*y_iter &gt; c'*y_z)\n            y_iter = y_z\n        end\n\n        # \\alpha should be the largest step size found. \n        # Algorithm terminates when \\alpha doesn't increase\n        α_iter = getBirkhoffStepSize(x_star, x, y_iter);\n        if(α &lt; α_iter)\n            α = α_iter;\n            y = y_iter;\n        else\n            return y;\n        end\n        i = i + 1;\n    end\n\n    return y\nend\n\ngetEPBplus (generic function with 1 method)\n\n\n\n# birkdecomp.jl\n\n@doc raw\"\"\"\nBirkhoff+ (max_rep)\n\"\"\"\nfunction birkdecomp(X, ε=1e-12; max_rep=1)\n    n = size(X, 1);                                 # get size of Birkhoff polytope\n    x_star = reshape(X, n*n);                       # reshape doubly stochastic to vector\n    B = birkhoffPolytope(n);                        # Birkhoff polytope\n    ε = max(ε, 1e-15);                              # fix the maximum minimum precision\n    max_iter = (n-1)^2 + 1;\n\n    x = zeros(n*n);                                 # initial point\n\n    extreme_points = zeros(n*n, max_iter);          # extreme points (permutation) matrix\n    θ = zeros(max_iter);                            # weights vector\n    approx = Inf;                                   # approximation error\n    i = 1;                                          # iteration index\n\n    while(approx &gt; ε)\n        # Get next extreme point \n        y = getEPBplus(x_star, x, B, max_rep, ε)\n        # Get next weight (step size)\n        θi = getBirkhoffStepSize(x_star, x, y)\n        # Update x_k\n        x = x + θi*y;\n        # Store the new weight\n        θ[i] = θi;\n\n        # Update the Frobenius norm\n        approx = sqrt(sum((abs.(x_star-x)).^2));\n        \n        # Store the new extreme point matrix\n        extreme_points[:,i] = y;\n        i = i + 1;\n    end\n\n    return extreme_points[:, 1:i-1], θ[1:i-1]\n\nend\n\nbirkdecomp (generic function with 2 methods)\n\n\n\n# Generate a random doubly stochastic matrix (n is the dimension)\nn = 3;\nx = randomDoublyStochasticMatrix(n);\nP, w = birkdecomp(x);\n\ndisplay(x)\ndisplay(P);\ndisplay(w);\n\n3×3 Array{Float64,2}:\n 0.0607488  0.590595   0.348656\n 0.70177    0.0291194  0.269111\n 0.237482   0.380286   0.382233\n\n\n9×5 Array{Float64,2}:\n 0.0  0.0  0.0  1.0  0.0\n 1.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  1.0\n 1.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0\n 0.0  1.0  0.0  1.0  0.0\n 0.0  1.0  0.0  0.0  1.0\n 0.0  0.0  1.0  1.0  0.0\n 1.0  0.0  0.0  0.0  0.0\n\n\n5-element Array{Float64,1}:\n 0.38223288955133716\n 0.31953666864401353\n 0.20836217653438616\n 0.06074883235172196\n 0.029119432918541188\n\n\n\nx_new = zeros(n, n)\nfor i=1:length(w)\n    x_new = x_new + w[i] * reshape(P[:, i], n, n) \nend\ndisplay(x_new)\n\n3×3 Array{Float64,2}:\n 0.0607488  0.590595   0.348656\n 0.70177    0.0291194  0.269111\n 0.237482   0.380286   0.382233"
  },
  {
    "objectID": "Notes/MLIC IMLI.html",
    "href": "Notes/MLIC IMLI.html",
    "title": "MLIC IMLI",
    "section": "",
    "text": "04-11-2022\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/MLIC IMLI.html#satisfiability-problem-sat",
    "href": "Notes/MLIC IMLI.html#satisfiability-problem-sat",
    "title": "MLIC IMLI",
    "section": "Satisfiability problem (SAT)",
    "text": "Satisfiability problem (SAT)\nLet \\{C_{1}, \\dots, C_{m}\\} be a set of Boolean clauses on variables x_{1}, \\dots, x_{n} where each clause is a disjunction of literals, each literal being a Boolean variable or its negation. SAT is the problem of finding an assignment of the boolean variables that makes all clauses true\n\nMaximum satisfiability problem (MaxSAT)\nLet there be a nonnegative weight W(C) = w_{c} associated with each clause. MaxSAT is the problem of finding an assignment of the boolean variables that maximizes the total weight of the satisfied clauses.\n\n\nPartial MaxSAT problem\nLet \\{C_{1}, \\dots, C_{m}\\} consist of soft clauses and hard clauses. Let there be a nonnegative weight W(C) = w_{c} associated with each soft clause. Partial MaxSAT is the problem of finding an assignment to the boolean variables that makes all hard clauses true and maximizes the total weight of the satisfied soft clauses."
  },
  {
    "objectID": "Notes/MLIC IMLI.html#problem-formulation",
    "href": "Notes/MLIC IMLI.html#problem-formulation",
    "title": "MLIC IMLI",
    "section": "Problem formulation",
    "text": "Problem formulation\nInputs:\n\nBinary matrix \\mathbf{X} \\in \\mathbb{R}^{n \\times m} of n instances with m binary features.\nBinary vector \\mathbf{y} \\in \\mathbb{R}^{n} of n binary labels.\nInteger k indicating the number of clauses in the CNF rule\nRegularization parameter \\lambda.\n\nOutputs:\n\na CNF rule.\n\nVariables:\n\nk \\times m binary matrix \\mathbf{B} of the variables \\{b_{1, 1}, b_{1, 2}, \\dots, b_{1, m}, \\dots, b_{k, m}\\} that represents the result CNF rule.\n\n\\mathbf{B}_{l, j} = 1 means that the feature f_{j} appears in clause C_{i} of the CNF rule.\n\\mathbf{B}_{l} means the lth row of the matrix \\mathbf{B}, which is also the lth clause C_{l}.\n\nn binary noise variables \\{\\eta_{1}, \\dots, \\eta_{n}\\} that indicates the instances that can be treated as noise.\n\nIf \\eta_{i} = 1, the results CNF rule doesn’t have to classifies the \\mathbf{x}_{i} correctly.\n\n\nPartial MaxSAT formulation:\n\nCNF constraint\n Q = \\bigwedge_{i=1}^{n} N_{i} \\wedge \\bigwedge_{i=1, j=1}^{i=k, j=m} V_{i, j} \\wedge \\bigwedge_{i=1}^{n} D_{i}  \nWe want the training accuracy to be higher. Each non-noise will have a \\lambda weight. (soft clauses)\n N_{i} = \\neg \\eta_{i} \\quad W(N_{i}) = \\lambda \nWe want the CNF rule to be sparse. Each don’t care literal will have a 1 weight. (soft clauses)\n V_{l, j} = \\neg \\mathbf{B}_{l, j} \\quad W(V_{l, j}) = 1 \nWe want each non-noise instance to be correctly classified by the CNF rule. (hard clauses)\n D_{i} = \\left( \\neg \\eta_{q} \\rightarrow \\left( y_{i} \\leftrightarrow \\bigwedge_{l = 1}^{k} \\left( \\bigvee_{j=1}^{m} \\mathbf{X}_{i, j} \\wedge \\mathbf{B}_{l, j} \\right) \\right) \\right) \\quad W(D_{i}) = \\infty"
  },
  {
    "objectID": "Notes/Quantization Survey.html",
    "href": "Notes/Quantization Survey.html",
    "title": "Quantization Survey",
    "section": "",
    "text": "This page contains my reading notes on"
  },
  {
    "objectID": "Notes/Quantization Survey.html#problem",
    "href": "Notes/Quantization Survey.html#problem",
    "title": "Quantization Survey",
    "section": "Problem:",
    "text": "Problem:\nGiven a full precision number x, which is either a weight or an activation in the network, we want to only use 2^{k} number of distinct values \\hat{x} to replace x in the inference time. 1. k here is called the bit-width. 1. The goal is to reduce the inference time and memory usage because less number of distinct values uses less memory and benefit from integer arithmetic hardware. 1. Unlike the application of quantization method used in signal processing, whose primary goal is to minimze the difference between the quantized values and the full-precision values, the quantization in neural network aims to minimize the accuracy drop, which can be achieved even if the average difference is huge. 1."
  },
  {
    "objectID": "Notes/Quantization Survey.html#uniform-quantization",
    "href": "Notes/Quantization Survey.html#uniform-quantization",
    "title": "Quantization Survey",
    "section": "Uniform Quantization",
    "text": "Uniform Quantization\n\nUniform quantization means that the values after the quantization are equally spaced: \\hat{x}_{n} - \\hat{x}_{n - 1} = \\hat{x}_{n - 1} - \\hat{x}_{n - 2}\nA widely used method of uniform quantization is as follows:\n\nThe quantization operator maps the real values to a set of consecutive integers in the range of [-2^{k-1}, 2^{k-1} - 1]:  Q(x) = \\mathrm{round}(\\frac{x - b}{s})  Here s is the scaling factor, b is the bias and \\mathrm{round}() is to round the float to nearest integer.\nBoth s and b can be directly calculated if we have selected a range of x to be [\\alpha, \\beta]:  s = \\frac{\\beta - \\alpha}{2^{k} - 1}   b = \\frac{\\beta - \\alpha}{2}  The scaling factor essentially divide the range (\\alpha, \\beta) into 2^{k} numbers of same size partitions. The bias shifts the selected range to be zero centered.\nFinally, the quantized value \\hat{x} that should be used in the inference can be mapped from Q(x):  \\hat{x} = sQ(x) + b \n\nSymmetric and asymmetric quantization\n\nIf the selected range [\\alpha, \\beta] is symmetric around 0 i.e. \\alpha = -\\beta, then the quantization is called symmertric. Otherwise, it is called asymmertric.\nSymmetric quantization doesn’t require b (b=0) since the selected range is already zero centered. However, it can cause unused/over-used quantized value if the x is not symmertric."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html",
    "href": "Notes/SGD Warm Restarts.html",
    "title": "SGD Warm Restarts",
    "section": "",
    "text": "08-02-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#why-it-is-needed",
    "href": "Notes/SGD Warm Restarts.html#why-it-is-needed",
    "title": "SGD Warm Restarts",
    "section": "Why it is needed",
    "text": "Why it is needed\nThis paper proposes a very simple yet quite effective learning rate scheduling technique. It alternates between a cosine annealing (gradually decreasing with a cosine form) phase and warm restarts step (rapidly increase to a high value) of the learning rate.\n\nFor the SGD with momentum, which is a more traditional gradient optimization algorithm, the only hyper-parameter is the learning rate.  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\lambda is the learning rate, \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t, \\mu is the momentum parameter that is typically 0.9, and v_{t} is the correct accumulated gradient direction at time t.\nWe usually always want to decrease the learning rate as the training time increases because we can quickly approach the target quickly at first with large learning rate and then slow down to take small steps around the local minima so that we don’t overshoot.\n\nInstead of using traditional step-wise or linearly decreasing, SGDR uses a wave form that is closed to cosine function.\n\nAnother paper is the first to propose that it is beneficial to periodically decrease and increase the learning rate for neural network training. It explains the intuition and imperatively demonstrate its effectiveness. The intuition is that the model will usually be stuck in the saddle point instead of the local minima and using the high learning rate at the proper time will help model jump out of the saddle point and traverse through the saddle point quickly.\n\nInstead of gradually increasing the learning rate, SGDR “restarts” the learning rate by directly setting it to a high value at some epochs."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#cosine-annealing",
    "href": "Notes/SGD Warm Restarts.html#cosine-annealing",
    "title": "SGD Warm Restarts",
    "section": "Cosine annealing",
    "text": "Cosine annealing\nAt given epoch t, the learning rate l is calculated as follows:  l = l_{\\mathrm{min}} + \\frac{1}{2}(l_{\\mathrm{max}} - l_{\\mathrm{min}})(1 + \\cos(\\frac{T_{\\mathrm{cur}}}{T}\\pi))  where T_{\\mathrm{cur}} is how many epochs have been performed since the last restart, l_{\\mathrm{min}} is the min learning rate, l_{\\mathrm{max}} is the max learning rate, and T defines how many epochs is one period (how many epochs to restart)."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#warm-restart",
    "href": "Notes/SGD Warm Restarts.html#warm-restart",
    "title": "SGD Warm Restarts",
    "section": "Warm restart",
    "text": "Warm restart\nWhen T_{\\mathrm{cur}} = 0, l = l_{\\mathrm{max}} and when T_{\\mathrm{cur}} = T, l = l_{\\mathrm{min}}. Whenever T_{\\mathrm{cur}} = T, we set the l directly to l_{\\mathrm{max}}."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#notes",
    "href": "Notes/SGD Warm Restarts.html#notes",
    "title": "SGD Warm Restarts",
    "section": "Notes",
    "text": "Notes\n\nl_{\\mathrm{min}}, l_{\\mathrm{max}} and T are hyper-parameters. Typically l_{\\mathrm{min}} is set to 0 and l_{\\mathrm{max}} is set to the initial learning rate.\nThe figure below shows how the function looks like if we set l_{\\mathrm{max}} = 1, l_{\\mathrm{min}} = 0, and T = 1, that is, we gradually decrease the learning rate from 1 to 0 in the period of 1 epoch.\nT_{\\mathrm{cur}} can also be a fraction number that represents the number of batches in the epoch. If an epoch has 10 batches, T_{\\mathrm{cur}} is updated 0.1 after each batch.\nInstead of using the fixed period T, the authors suggested to define another hyper-parameter T_{\\mathrm{mult}} to make T increase after each restart.  T_{n+1} = T_{n} \\times T_{\\mathrm{mult}}  where n is the number of restarts.\nl_{\\mathrm{min}} and l_{\\mathrm{max}} can also be changed in each restart, but the authors suggest not to change it to reduce the number of hyper-parameters involved.\n\n\nfrom IPython.display import IFrame\nIFrame('https://www.desmos.com/calculator/9hrbpo2ajf?embed', width=500, height=500)"
  },
  {
    "objectID": "Notes/ML Q&A.html",
    "href": "Notes/ML Q&A.html",
    "title": "ML Q & A",
    "section": "",
    "text": "We always want the model to have low bias and low variance at the same time, but it is difficult.\nBias is the average difference between the predictions and the label.\nVariance refers to the sensitivity of our model to the fluctuations (noise) in the training set. OR variance describes how much a random variable of your model differs from its expected value.\nSince we only have training set that is a sample from the overall distribution, training our model to reduce bias will generally increase variance at the same time because we don’t know what is the noise.\nHigh bias and low variance lead to under-fitting, where your model is too simple to capture the regularities in your data. This will result in both low training accuracy and testing accuracy.\nLow bias and high variance lead to over-fitting, where your model is too complex that it captures all the patterns in your training data including noise. It will result in high training accuracy, but low testing accuracy.\n\n\n\n\n\nUnder-fitting is when your model is too simple to fit the training set correctly. This will result in low training accuracy and low testing accuracy, assuming that our training set is a reasonable sample of the overall distribution.\nTo combat under-fitting, the first we can do is to choose the model with more complexity. If decision tree under-performs on a dataset, we can try random forest. Second we can increase the parameters that our model have. Like increasing number of trees or the maximum depth of the decision tree. Last, for some models that uses iterative learning procedure such as neural network, we can increase the training time.\nOver-fitting is when your model is too complex such it can fit the training set very accurately. This means that it also fit the noise correctly and will result in high training accuracy and low testing accuracy.\nTo combat over-fitting, we first can reduce the number of parameters. Second, using ensemble of your current models is proven to reduce over-fitting. Third, without changing models, we can do data augmentation such as over-sampling more data to make the training better represent the actual distribution.\n\n\n\n\n\nSupervised learning task is usually provided with labeled dataset and goal is to predict the correct label for a given instance.\nUnsupervised learning only gives the instances themselves, without their labels. The goal is to learn patterns from those unlabeled data.\nSupervised learning include classification and regression. Classification is to predict a categorical label while regression is to predict a continuous value.\nUnsupervised learning include clustering and dimension reduction. Clustering is to group similar instances together and dimension reduction is to select the important features from all features.\n\n\n\n\n\nDimensionality refers to the number of features in your dataset.\nIt is harder for the models to search through a space as the number of features grows. The required number of training instances to achieve the same accuracy grows exponentially with the number of features. Since in practice the number of training instances are fixed, the performance of the models will typically decrease as the dimension increases.\nWe can use feature selection such as manual feature selection by human or feature extraction technique like PCA to reduce the dimensionality. The difference between selection and extraction is that selection selected subset of the original features and extraction extracts a set of new features from data.\n\n\n\n\n\nThe confusion matrix is used to evaluate the performance of an supervised classifier on a dataset. It has N rows and N columns where N is the number of classes in the dataset.\nEach row of the matrix gives the number of predictions from the classifier for a specific label and each column of the matrix gives the number of actual labels. If the classifier is for a binary classification dataset, the first row gives the number of true positives and the number of false positives and the second row will give false negatives and true negatives.\nOther performance evaluation methods such as accuracy or F1-score will be misleading on unbalanced dataset. For example, if we have a dataset that has 95 dogs and 5 cats and a classifier than always predict dog. We have 95% accuracy and 97.5% F1-score, which tells that our classifier is very good. Confusion matrix will give a whole picture of metrics that include true positive, true negative, false positive and false negative. If we add up true positive and false positive in the matrix, we can see that our classifier only predict positive labels.\n\nNotes: 1. For a binary classification problem, we can have several terms: 1. True positive: classifier gives positive class and the prediction is correct. 2. False positive: classifier gives positive class and the prediction is incorrect. 3. True negative: classifier gives negative class and the prediction is correct. 4. False negative: classifier gives negative class and the prediction is incorrect. 2. For a binary classification problem, we have several performance measures: 1. Sensitivity (true positive rate): number of positive predictions correctly labeled / number of instances with positive labels.  \\mathrm{\\frac{TP}{TP + FN}}  False positive rate: number of positive predictions incorrectly labeled / number of instances with negative labels.  \\mathrm{\\frac{FP}{FP + TN}}  2. Specificity (true negative rate): number of negative predictions correctly labeled / number of instances with negative labels.  \\mathrm{\\frac{TN}{TN + FP}}  3. Precision: number of positive instances correctly predicted / number of positive predictions.  \\mathrm{\\frac{TP}{TP + FP}}  4. Recall: same as sensitivity.  \\mathrm{\\frac{TP}{TP + FN}}  5. F1-score: harmonic mean of the precision and recall.  \\mathrm{2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}}\n\n\n\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc 1. An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The x axis is false positive rate and y axis is true positive rate. 2. An ROC curve plots TPR vs. FPR at different classification thresholds. For all models that first produce a score and then thresholded to give the classification, different thresholds mean different number of positive and negative predictions. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives and the curve will always go higher. 3. A perfect model will have a straight horizontal line at y = 1 while a perfectly wrong model will have a horizontal line at y = 0. A random guessing model will have a diagonal line, which means that the model has no class separation capacity. 4. AOC stands for area under the curve and measures the area under the ROC curve. It is used to measure a model’s performance across all possible classification thresholds.\n\n\n\n\nCalculate moving average of a set of data points is creating a series of averages of different subsets of the dataset. The dataset to be analyzed usually contains time series data (data that is indexed by timestamps). Exponential moving average (EMA) is one type of moving average algorithm.\nThe EMA value at timestamp t calculates the average of the data from the beginning to timestamp t, which is calculated by adding up the weighted value of the data at timestamp t and the weighted EMA value on the previous timestamp t-1.\nThe weighted part is called the smoothing factor and is a hyper-parameter that is between 0 and 1. Higher smoothing factor means the current data value is weighted more and the previous EMA value is weighted less in the calculation of the new EMA value.\nThis techniques is usually used by gradient optimizer to calculate new learning rate.\n\nNotes: 1. [EMA equation]: If the current timestamp is t, then the equation for EMA is:  y_{t} = \\alpha x_{t} + (1 - \\alpha)y_{t-1}  where y_{t} is the moving average at timestamp t, x_{t} is the data point at timestamp t, and \\alpha is the smoothing factor, which is the range [0, 1].\n\n\n\nTODO\n\n\n\n\n\n\n\nGradient descent is an optimization algorithm that can iteratively minimize the target function to find its local minimum. If the target function is convex, then gradient descent can also find its global minimum.\nFirst we calculate the derivatives of the target function with respect to the parameters of the model. This is the gradient.\nSecond we update the parameters to the opposite direction of the gradients to minimize the value of the target function. This is the descent.\nGradient descent will minimize the target function, while gradient ascent, which updates the parameters to the same direction of the gradients, will maximize the target function.\n\n\n\n\n\nTODO\n\n\n\n\nhttps://ruder.io/optimizing-gradient-descent/index.html#fn4 https://cs231n.github.io/neural-networks-3/#update 1. Normal gradient descent is batch gradient descent. One batch means one complete run of the training set. Thus, we need to evaluate all instances of the training set before one gradient update. Gradient descent is more slow, but guaranteed to converge to the local minimum. 2. Stochastic gradient descent means that one gradient update is performed for each instance evaluated. This approach converges faster and can be used on the fly as the new instance comes in, but can cause target function to fluctuate. 3. Mini-batch is the combination of the two above. Instead of whole batch or single instance, we take subset of training set as the mini-batch and evaluate them to get gradient for a single gradient update. This combines the benefits of two method above.\nNotes: 1. [Different gradient descent optimization algorithms] 1. SGD  \\theta_{t+1} = \\theta_{t} - \\lambda \\partial_{t}(\\theta)  where \\lambda is the learning rate and \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t. 2. SGD with Momentum: It will help the convergence speed of SGD because it reduces the oscillations of the SGD near the local minimas, which is done by building up the velocity in the correct direction that has consistent gradients  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\mu is the momentum parameter that is typically 0.9 and v_{t} is the correct accumulated gradient direction at time t. 3. Adagrad: It will make the learning rates of the weights that receive high gradients reduced, and the learning rates of the weights that receive small or infrequent updates increased. Thus we don’t have to manually tune the learning rates in the training progress. However, since there is no way to reduce the accumulated squared gradients in the denominator of the learning rate, the monotonically decreasing learning rate in the training process will eventually stop the training.  g_{t} = g_{t-1} + \\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{g_{t} + \\epsilon}} \\partial_{t}(\\theta)  where g_{t} is the accumulation of the squared gradient for \\theta until time t and \\epsilon is a small value used to prevent the division by 0. 4. RMSprop: RMSprop improves on Adagrad by replacing the accumulation of the past squared gradients with the exponential moving average of the past squared gradients. This can solve the issue of Adagrad that the learning rates are monotonically decreasing.  e_{t} = \\beta e_{t-1} + (1-\\beta)\\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{e_{t} + \\epsilon}} \\partial_{t}(\\theta) where e_{t} is the exponential moving average of the squared gradient for \\theta until time t and \\beta is like momentum that controls degree of weighting decay and is usually set to 0.9. 5. Adam: Adam improves on RMSprop by replacing the raw gradient with the exponential moving average of the past gradients in the update step. It thus combines the benefits of RMSprop and SGD with momentum.  v_{t} = \\beta_{1} v_{t-1} + (1-\\beta_{1})\\partial_{t}(\\theta)   e_{t} = \\beta_{2} e_{t-1} + (1-\\beta_{2})\\partial_{t}^{2}(\\theta)   \\hat{v}_{t} = \\frac{v_{t}}{1-\\beta_{1}^{t}}   \\hat{e}_{t} = \\frac{e_{t}}{1-\\beta_{2}^{t}}   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda \\hat{v}_{t}}{\\sqrt{\\hat{e}_{t} + \\epsilon}}  where \\hat{v}_{t} and \\hat{e}_{t} are the bias corrected versions of v_{t} and e_{t} and \\beta^{t} means the \\beta to the power of t.\n\n\n\n\nVanishing gradient happens to the parameters in the earlier layers of a deep neural network where the gradients are so small in the back-propagation process that the weights are not really changed.\nThe primary reason for this problem is the choice of activation functions such as sigmoid or hyperbolic tangent function, whose gradients are very small and are always much less than 1. If multiple layers with such activations are stacked together, the gradients to the earlier layers of the networks are multiplied lots of times with the loss gradient in the back-propagation process. Each layer reduce the original loss gradient by a fraction and in the end the gradient to the earlier layers are very small. Therefore, the large number of layers are also an important reason for vanishing gradient problem.\nOne effective solution to the problem is to use other activation functions such as ReLU.\n\n\n\n\n\nIt can help solve the vanishing gradient problem. The gradient of the ReLU for inputs larger than 0 is 1 and thus the gradient won’t be reduced in the back-propagation process.\nReLU is computationally more efficient because it only needs to cut the negative input to 0.\nHistorically speaking, ReLU is just good enough for neural network to be trained stably.\n\n\n\n\n\nL_1 regularization is also called Lasso regularization. It adds the sum of the absolute value of all weights in the neural network as a penalty term to the loss function.\nL_2 regularization is also called Ridge regularization. It adds the sum of the squared value of all weights in the neural network as a penalty term to the loss function.\nThey both are used to reduce over-fitting issue of the large neural network.\nThe key difference is the gradient of each penalty. The gradient of Lasso is a 1 or -1 depending on the sign of each weight, while the gradient of Ridge is 2 times the value of the parameter. The weights with Lasso can possibly shrink to exactly 0, while weights with Ridge can only shrink to a very small value instead of exact 0 because the gradients also decrease as the weights decrease. Thus Lasso can be used to train sparse neural network.\n\n\n\n\n\nDropout is to randomly drop neurons of the network in the training process to avoid over-fitting. A neuron is dropped means that the data and the gradient don’t go through that neuron in both forward or backward process.\nTypically dropout is applied per layer and we can set a probability p for each layer to indicate what percentage of neurons we want to drop in that layer. p is usually selected for 0.5 for hidden layer, but a less value for input layer like 0.1 because randomly dropping an entire column of input data is very risky.\nThe dropout neurons are changed every instance or mini-batch depending on what type of gradient descent we are using. We only apply dropout in the training process and we will use all neurons for testing to have consistent output.\n\n\n\n\n\nBatch normalization is to normalize the inputs to each layer for each mini-batch. Batch normalization is proved to help neural network training converges faster.\nBatch normalization first normalize the inputs to each layer by subtracting the mini-batch mean from each value and then divide it by the mini-batch standard deviation. This process will make each input value to be in the range between 0 to 1. Then we need to scale and shift the normalized value into a desirable range. The coefficients for scaling and shifting are also two parameters that are needed to be learned in the backward propagation.\nThe challenge that batch normalization is trying to solve is called internal covariate shift. Since each layer’s output is fed into next layer’s input, the change of the weights in the first layer due to backward process after a mini-batch will cause the change of its output distribution. The internal covariate shift slows down the training process because the learning in the next iteration needs to accommodate for this change.\n\nNotes: 1. [Batch Normalization Layer]: Batch normalization layer can be appended before each layer and outputs the same dimension as the inputs 1. Get the mean \\mu_{B} and standard deviation \\sigma_{B} of the inputs in a mini-batch:  \\mu_{B}=\\frac{1}{m}\\sum_{i=1}^{m}x_i   \\sigma_{B}=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_{B})^{2}}  2. Normalize the input:  \\hat{x}_i=\\frac{x_i-\\mu_{B}}{\\sigma_{B}}  3. Scale and shift back, where \\gamma is the scaling parameter and \\beta is the shifting parameter:  y_{i} = \\gamma\\hat{x}_{i}+\\beta \n\n\n\nhttps://www.deeplearning.ai/ai-notes/initialization/index.html 1. Xavier initialization is a specific way of initialize the weights and bias in the neural network such that the variance of the activations are relatively the same across all layers. 2. If we use Xavier initialization method, the bias will be initialized to 0 and the weights are randomly sampled from a normal distribution that has mean of 0 and variance of 1 over the the number of neurons in the last layer. 3. If we initialize the weights to have the same value, all neurons will have the same activations. Same activations mean that all neurons will have the same gradients and will evolve the same throughout the training. 4. If we initialize the weights to be too small or too large, the activations of each layer in the first several iterations will also be very small or large. Since gradients of weights for each layer are calculated based on the activations, large weights will result in gradient exploding and small weights result in gradient vanishing, both of which prevent the neural from efficiently learning.\n\n\n\n\nhttps://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning#clustering\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html 1. K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed. 2. We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change. 3. K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.\nNotes 1. [K-means algorithm]: 1. Initialize cluster centroids \\mu_{1}, ..., \\mu_{k} randomly. 2. Repeat until convergence: 1. Get the centroid for each instance x_{i}:  c_{i} = \\underset{\\mu_{i}}{\\operatorname{argmin}} \\lVert x_{i}-\\mu_{i} \\rVert^{2}  2. Update the centroid based on the instances:  \\mu_{j} = \\frac{\\sum_{i}^{m}1_{\\{c_i=j\\}}x_{i}}{\\sum_{i}^{m}1_{\\{c_i=j\\}}} \n\n\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c 1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. 2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.\nNotes: 1. [Eigenvectors, Eigenvalues]: Given a matrix A\\in\\mathbb{R}^{n\\times n}, \\lambda is said to be an eigenvalue of A if there exists a eigenvector z\\in\\mathbb{R}^n \\neq 0, such that:  Az = \\lambda z  2. [Eigendecomposition (spectral decomposition]: Let M be a real symmetric d \\times d matrix with eigenvalues \\lambda_{1}, ... , \\lambda_{d} and corresponding orthonormal eigenvectors u_{1}, ..., u_{d}. Then:  M = Q \\Lambda Q^T  where \\Lambda is a diagonal matrix with \\lambda_{1}, ... , \\lambda_{d} in diagonal and 0 elsewhere and Q matrix has u_{1}, ..., u{d} vectors as columns.\n\n\n\n\n\n\n\n[Bayes’ theorem]: the conditional possibility of event A given the event B is true P(A|B) can be computed as:  P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}  which in the Bayesian term is written as:  \\mathrm{Posterior} = \\frac{\\mathrm{Likelihood} \\cdot \\mathrm{Prior}}{\\mathrm{Evidence}}  If we think A as a label and B as a set of features:\n\nP(A|B) is the posterior probability of a label given a set of features.\nP(B|A) is the likelihood which is the probability of a set of features given a label.\nP(A) is the prior probability of a label.\nP(B) is the evidence probaility of a set of features.\n\n[Naive Bayes]: Naive Bayes is a classifier that selects the label \\hat{y} from all possible labels y \\in Y that has maximum conditional possibility given the instance \\mathbf{x}.  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} P(y|\\mathbf{x})  Applying Bayes’ theorem, we have:  P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\cdot P(y)}{P(\\mathbf{x})}  Since P(\\mathbf{x}) is a constant and is independent from P(y_{i}), we can simply drop it:  P(y|\\mathbf{x}) \\propto P(\\mathbf{x}|y) \\cdot P(y)  If we assume that each feature is independent from each other (naive conditional independence assumption), the possibility that the features values are all in \\mathbf{x} is the product of their possibilities:  P(\\mathbf{x}|y) = \\prod_{i}^{m}P(x_{i}|y)  Put them together, we have:  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} \\prod_{i}^{m}P(x_{i}|y) \\cdot P(y) \n\n\n\n\n\nhttps://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html 1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. 2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. 3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model’s output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction’s error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. 4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression. 5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels.\nNotes: 1. [Mean squared Error (MSE)]:\n \\mathrm{MSE} = \\frac{1}{n}\\sum_{i}^{n}(y_{i}-\\hat{y}_{i})^2  where y_{i} is the actual label and \\hat{y}_{i} is the prediction given by the classifier. 1. [Binary cross entropy (BCE)]: only works if the labels y_{i} are 0 or 1 and the values of predictions \\hat{y}_{i} are between 0 and 1.  \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y})))  which can be decomposed to two cases for each prediction and label pair:  -\\log(\\hat{y}_{i}) \\;\\mathrm{if}\\; y=1   -\\log(1 - \\hat{y}_{i}) \\;\\mathrm{if}\\; y=0  1. [Sigmoid (logistic) function and logic function]:  \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}   \\mathrm{logic}(x) = \\log(\\frac{x}{1 - x}) The inverse of sigmoid function is the logic function: \n    \\begin{alignat}{2}\n    x &= \\frac{1}{1 + e^{-y}} \\\\\n    \\frac{1}{x} &= 1 + e^{-y} \\\\\n    e^{-y} &= \\frac{1 - x}{x} \\\\\n    e^{y} &= \\frac{x}{1 - x} \\\\\n    y &= \\log(\\frac{x}{1 - x}) \\\\\n    \\end{alignat}\n     1. [How to solve the parameters for linear regression and logistic regression] 1. Linear regression can be solved mathematically by setting partial derivative of loss w.r.t each weight to 0: (bias is removed for simplification)  \\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{N} \\sum_{i}^{N} x_{i,k} \\bigg(\\sum_{j}^{D}w_{j}x_{i,j} - \\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} - x_{i, k}\\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}   = \\frac{2}{N} \\sum_{j}^{D} w_{j} \\bigg( \\sum_{i}^{N} x_{i,j}x_{i,k} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}  1. Gradient descent can be applied to solve both linear regression and logistic regression. Logistic regression doesn’t have a closed-form solution because of the non-linearity that the sigmoid function imposes.\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/extend-lm.html 1. Generalized linear models build on linear regression models to predict a non-Gaussian distribution. It keeps the weighted sum of the features of the linear regression, but connect the weighted sum and the expected mean of the output distribution through a possibly nonlinear function. 2. For example, the logistic regression is a type of the generalized linear models and it assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logic function. 3. Generalized additive models further relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature. It allows to model the potentially non-linear relations between the features and the output.\nNotes: 1. [Assumptions of linear regression]: 1. The input features are independent from each other (no interactions between the features). 2. The output distribution y given the input features X follows a Gaussian distribution. This follows the following theorem: &gt; Let X_1, ..., X_n be n mutually independent normal random variables, having means \\mu_1, ..., \\mu_n and variances \\sigma_1^2, ... \\sigma_n^2. If the random variable Y is a linear combinations of the X with w_1, ..., w_n coefficients: Y=\\sum_{i=1}^{n}w_iX_i, then Y is a Gaussian distribution with the mean \\mathrm{E}[Y] = \\sum_{i=1}^{n}b_i\\mu_i and variance \\mathrm{Var}[Y] = \\sum_{i=1}^{n}b_i^2\\sigma_i^2. 3. The true relationship between each feature X_i and y is linear. 2. [Components of GLM] 1. Random component: the probability distribution of the output variable Y. It’s expected value (mean value) is \\mathrm{E}(Y). 2. Systematic component: the weighted sum \\sum_{1}^{n}w_ix_i + w_0. 3. Link function: the relation between the random component and the systematic component g.  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}w_ix_i + w_0  3. [Equation of GAM]  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}f(x_i)  where f() can be arbitrarily defined function.\n\n\n\nhttps://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/ https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf 1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters. 1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane. 1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. 1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.\nNotes: 1. [SVM without slacks (hard margin SVM)]: Given a dataset with n instances x_{i} \\in R^{d} and n labels y_{i} \\in \\{-1, 1\\}, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights w \\in R^{d} and a bias b \\in R, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem: \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     1. Solving the above optimization problem will give us two parallel hyperplanes (w x + b = 1 and w x + b = -1) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between. 1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as  \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert w \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert w \\rVert} = \\frac{2}{\\lVert w \\rVert}  1. The constraints specify that the instances must be on the correct side of the two hyperplanes:  w x_{i} + b \\geq 1 \\quad \\mathrm{if} y_{i} = 1   w x_{i} + b \\leq -1 \\quad \\mathrm{if} y_{i} = -1  and y_{i}(w x_{i} + b) \\geq 1 summarizes the above two conditions. 1. [SVM with slacks (soft margin SVM)]: In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} + C \\sum_{i}^{n} \\xi_{i} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots n \\\\\n    \\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     where \\xi_{i} is the slack variable for the instance x_{i} and C is a hyperparameter that penalizes the misclassification of x_{i}. 1. If \\xi_{i} is nonzero for x_{i}, it means that x_{i} is on the misclassified side of w x_{i} + b = 1 (or w x_{i} + b = -1) and the distance is \\xi_{i}. 1. If C = 0, \\xi_{i} can be arbitrary large for each x_{i}. If C \\to \\inf, it is the same as hard margin SVM because any misclassification can induce infinite loss.\n\n[Solving hard margin SVM]\n\nRewrite the primal program for easier Lagrangian computation below: \n\\begin{alignat}{2}\n\\min \\quad & \\frac{1}{2} ww \\\\\n\\text{s.t. } \\quad & -(y_{i}(w x_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots n \\\\\n\\end{alignat}\n\nWe can derive the Lagrangian primal function from the primal program: \n\\begin{alignat}{2}\nL(w, b, \\alpha) & = f(w, b) + \\sum_{i}^{n} \\alpha h_{i}(w, b) \\\\\n& = \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n where \\alpha is a new variable called Lagrangian multiplier.\nThen we can write and solve Lagrangian dual function: \n\\begin{alignat}{2}\ng(\\alpha) & = \\min_{w, b} L(w, b, \\alpha) \\\\\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over w: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} & = 0 \\\\\nw - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} & = 0 \\\\\nw & = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over b: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial b} & = 0 \\\\\n\\sum_{i}^{n} \\alpha_{i}y_{i} & = 0 \\\\\n\\end{alignat}\n Plug in w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} back to g(\\alpha): \n\\begin{alignat}{2}\ng(\\alpha)\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right)\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 \\right) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\  \n\\end{alignat}\n Since we know that \\alpha_{i}y_{i} = 0, then b\\sum_{i}^{n} \\alpha_{i}y_{i} = 0, and thus the final Lagrange dual function is:  g(\\alpha) = \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Notice that \\alpha_{i}y_{i} = 0 is added as part of the constraint.\nSince strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:\n\nThe Lagrange dual problem only involves \\alpha_{i}, but primal problem has w and b, which are much more parameters.\nThe Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.\n\n\n[Solving soft margin SVM]\n\nSimilar as hard margin SVM, we can write Lagrangian dual function as: \n\\begin{alignat}{2}\ng(\\alpha, \\beta) & = \\min_{w, b} \\frac{1}{2} ww\n- \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n\\end{alignat}\n where a new Lagrange multiplier is introduced for the constraint \\xi_{i} \\geq 0.\nSimilar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the w, b, and \\xi_i: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} = 0 & \\Rightarrow w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i}^{n} \\alpha_{i}y_{i} = 0 \\\\\n\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n\\end{alignat}\n and plug the w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} and C = \\alpha_{i} + \\beta_{i} back in g(\\alpha, \\beta). \n\\begin{alignat}{2}\ng(\\alpha, \\beta)\n& = \\min_{w, b} \\frac{1}{2} ww + C\\sum_{i}^{n}\\xi_{i} - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right)\n     + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n} \\alpha_{i}\\xi_{i} + \\sum_{i}^{n} \\beta_{i}\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\sum_{i}^{n}\\alpha_{i}  - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\end{alignat}\n which has exactly the same form as Lagrangian dual function of hard margin SVM.\nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\beta_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Since we know C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}, the constraint \\beta_{i} \\geq 0 can be removed by merging into \\alpha_{i} \\geq 0: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n The only difference with Lagrange dual problem of hard margin SVM is the addition of C \\geq \\alpha_{i}.\n\n[Kernel trick]\n\nKernel trick\n\n[Duality and KKT conditions]\n\nThe Lagrangian dual problem:\n\nGiven a minimization primal problem: \n\\begin{alignat}{2}\n\\min_{x} \\quad & f(x) \\\\\n\\text{s.t. } \\quad & h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\quad & l_{j}(x) = 0, \\quad j = 1, \\dots, m \\\\\n\\end{alignat}\n\nThe Lagrangian is defined as:  L(x, u, v) = f(x) + \\sum_{i}^{n} u_{i}h_{i}(x) + \\sum_{j}^{m} v_{j}l_{j}(x)  where u_{i} and v_{j} are new variables called Lagrangian multipliers.\nThe Lagrange dual function is:  g(u, v) = \\min_{x} L(x, u, v) \nThe Lagrange dual problem is: \n\\begin{alignat}{2}\n\\max_{u, v} \\quad & g(u, v) \\\\\n\\text{s.t. } \\quad & u \\geq 0 \\\\\n\\end{alignat}\n\nThe properties of dual problem:\n\nThe dual problem is always convex even if the primal problem is not convex.\nFor any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).\n\n\nKarush-Kuhn-Tucker (KKT) conditions\n\nGiven the Lagrange dual problem stated above, the KKT conditions are:\n\nStationarity condition:  0 \\in \\partial \\left( f(x) + \\sum_{i=1}^{n} u_{i} h_{i}(x) + \\sum_{j=1}^{m} v_{j}l_{j}(x) \\right) \nComplementary Slackness:  u_{i}h_{i}(x) = 0, \\quad i = 1, \\dots, n \nPrimal feasibility:  h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n   l_{j}(x) = 0,  \\quad j = 1, \\dots, m \nDual feasibility:  u_{i} \\geq 0, \\quad i = 1, \\dots, n \n\nIf a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the x^{*} and u^{*}, v^{*} are primal and dual solutions if and only if x^{*} and u^{*}, v^{*} satisfy the KKT conditions.\n\n\n\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. 2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. 3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.\nNotes 1. [Gini Index and Information Entropy]: Both applies to a dataset (instances with labels) to measure its uncertainty. They both become 0 when there is only one class in the set.\nGini Index (Gini impurity) measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.  G(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)(1-\\textrm{P}(c)) = 1 - \\sum_{c=1}^{C} \\textrm{P}(c)^2  Information Entropy can be roughly thought as the dataset’s variance.  E(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)\\log_2\\textrm{P}(c)  In both cases, \\mathcal{D} is the dataset to be evaluated, C is the total number of classes in \\mathcal{D} and \\textrm{P}(c) is the probability of picking an instance with the class c (fraction of instances with class c in \\mathcal{D}). 2. [Gini Gain and Information Gain]: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset.  G(\\mathcal{D}, S) = M(\\mathcal{D}) - \\sum_{s\\in S}\\frac{\\lvert s \\rvert}{\\lvert D \\rvert} M(s) where \\mathcal{D} is the dataset before splitting, S are subsets of \\mathcal{D} created from all possible splitting of \\mathcal{d}, M is Gini Index (G) or Information Entropy (E), and \\lvert \\cdot \\rvert gives the number of items in a set. 3. [Decision tree training algorithm]: We consider binary classification decision tree. Given a dataset \\mathcal{D}, 1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. 2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. 3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset. 4. The splitting stops when no further splitting can be made (the dataset contains only one class).\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Random Forest contains many decision trees and combine all their outputs to give a final decision. 2. A particular goal in training a random forest is to make each tree in the forest different from each other. First is to use bootstrapping, which means that each decision tree is trained on different dataset that is randomly sampled with replacement from the original dataset. Then to further inject randomness, random subset of the features instead of all features are considered in each split of the decision tree. Then the final output of random forest is to take the majority vote or average each output. 3. The goal of randomize the decision trees and taking the aggregation result is to reduce the variance and thus prevent over-fitting of the single decision tree. By taking an average of the random predictions, some errors can cancel out. Using multiple trees in the prediction make random forest a black box and the explanation for a prediction is hard to be understood by the users.\nNotes: 1. [Bagging]: Bagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output.\n\n\n\nhttps://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html\nhttps://arxiv.org/pdf/1403.1452.pdf 1. Adaboost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (&gt;50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. 2. Adaboost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. 3. Adaboost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner.\nNotes: 1. [Adaboost algorithm] Here we show the adaboost algorithm for binary classification problems (y \\in \\{-1, 1\\}). 1. For the dataset with N instances, initialize the observation weights for each instance w_i=\\frac{1}{N}, i=1,2, ... ,N. 2. For m = 1 ... M, 1. Fit a classifier G_m(x) to the training instances with weights w_i. 2. Compute  E_m=\\frac{\\sum_{i=1}^{N} w_i \\mathcal{1}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^{N}w_i}  3. Compute  \\alpha_m = \\log(\\frac{1-E_m}{E_m})  4. Set  w_i \\gets w_i \\cdot e^{\\alpha_m y_i G_m(x_i)}  3. Final output of Adaboost:  G(x) = \\textrm{sign} (\\sum_{m=1}^M \\alpha_m G_m(x)) \n\n\n\n\nGradient boosting can be seen as the generalized version of boosting, i.e. Adaboost is one special case of gradient boosting. GB can be seen as\nGB is a generalized additive model of n weak learners.  G(x) = g_{1}(x) + \\dots + g_{n}(x)  where G(x) is the final gradient boosting model and g(x) is one type of weak learners.\nThe weak learner g(x) can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting.\n$g_{1}(x) g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets.\n\nGiven a loss function L(\\cdot), a training set X = \\{\\mathbf{x_{i}}\\}, \\mathbf{y} = \\{y_{i}\\}, a learning rate \\alpha, and a number of iterations M, the algorithm to train a GBRT is as follows: 1. Intiailize G(x) by fitting CART on D 1. For m = 1 \\dots M, 1. Evaluate the loss over the current G(x) 1. Calculate the gradient of the loss w.r.t the labels to get the residuals \\tilde{\\mathbf{y}}:  \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}} Note \\tilde{\\mathbf{y}} has the same shape as \\mathbf{y}. 1. Use X and residuals \\tilde{\\mathbf{y}} as the new training set to train a CART g(x). 1. Add the new weak leaner into the current model:  G(x) = G(x) + \\alpha g(x)"
  },
  {
    "objectID": "Notes/ML Q&A.html#ml-basics",
    "href": "Notes/ML Q&A.html#ml-basics",
    "title": "ML Q & A",
    "section": "",
    "text": "We always want the model to have low bias and low variance at the same time, but it is difficult.\nBias is the average difference between the predictions and the label.\nVariance refers to the sensitivity of our model to the fluctuations (noise) in the training set. OR variance describes how much a random variable of your model differs from its expected value.\nSince we only have training set that is a sample from the overall distribution, training our model to reduce bias will generally increase variance at the same time because we don’t know what is the noise.\nHigh bias and low variance lead to under-fitting, where your model is too simple to capture the regularities in your data. This will result in both low training accuracy and testing accuracy.\nLow bias and high variance lead to over-fitting, where your model is too complex that it captures all the patterns in your training data including noise. It will result in high training accuracy, but low testing accuracy.\n\n\n\n\n\nUnder-fitting is when your model is too simple to fit the training set correctly. This will result in low training accuracy and low testing accuracy, assuming that our training set is a reasonable sample of the overall distribution.\nTo combat under-fitting, the first we can do is to choose the model with more complexity. If decision tree under-performs on a dataset, we can try random forest. Second we can increase the parameters that our model have. Like increasing number of trees or the maximum depth of the decision tree. Last, for some models that uses iterative learning procedure such as neural network, we can increase the training time.\nOver-fitting is when your model is too complex such it can fit the training set very accurately. This means that it also fit the noise correctly and will result in high training accuracy and low testing accuracy.\nTo combat over-fitting, we first can reduce the number of parameters. Second, using ensemble of your current models is proven to reduce over-fitting. Third, without changing models, we can do data augmentation such as over-sampling more data to make the training better represent the actual distribution.\n\n\n\n\n\nSupervised learning task is usually provided with labeled dataset and goal is to predict the correct label for a given instance.\nUnsupervised learning only gives the instances themselves, without their labels. The goal is to learn patterns from those unlabeled data.\nSupervised learning include classification and regression. Classification is to predict a categorical label while regression is to predict a continuous value.\nUnsupervised learning include clustering and dimension reduction. Clustering is to group similar instances together and dimension reduction is to select the important features from all features.\n\n\n\n\n\nDimensionality refers to the number of features in your dataset.\nIt is harder for the models to search through a space as the number of features grows. The required number of training instances to achieve the same accuracy grows exponentially with the number of features. Since in practice the number of training instances are fixed, the performance of the models will typically decrease as the dimension increases.\nWe can use feature selection such as manual feature selection by human or feature extraction technique like PCA to reduce the dimensionality. The difference between selection and extraction is that selection selected subset of the original features and extraction extracts a set of new features from data.\n\n\n\n\n\nThe confusion matrix is used to evaluate the performance of an supervised classifier on a dataset. It has N rows and N columns where N is the number of classes in the dataset.\nEach row of the matrix gives the number of predictions from the classifier for a specific label and each column of the matrix gives the number of actual labels. If the classifier is for a binary classification dataset, the first row gives the number of true positives and the number of false positives and the second row will give false negatives and true negatives.\nOther performance evaluation methods such as accuracy or F1-score will be misleading on unbalanced dataset. For example, if we have a dataset that has 95 dogs and 5 cats and a classifier than always predict dog. We have 95% accuracy and 97.5% F1-score, which tells that our classifier is very good. Confusion matrix will give a whole picture of metrics that include true positive, true negative, false positive and false negative. If we add up true positive and false positive in the matrix, we can see that our classifier only predict positive labels.\n\nNotes: 1. For a binary classification problem, we can have several terms: 1. True positive: classifier gives positive class and the prediction is correct. 2. False positive: classifier gives positive class and the prediction is incorrect. 3. True negative: classifier gives negative class and the prediction is correct. 4. False negative: classifier gives negative class and the prediction is incorrect. 2. For a binary classification problem, we have several performance measures: 1. Sensitivity (true positive rate): number of positive predictions correctly labeled / number of instances with positive labels.  \\mathrm{\\frac{TP}{TP + FN}}  False positive rate: number of positive predictions incorrectly labeled / number of instances with negative labels.  \\mathrm{\\frac{FP}{FP + TN}}  2. Specificity (true negative rate): number of negative predictions correctly labeled / number of instances with negative labels.  \\mathrm{\\frac{TN}{TN + FP}}  3. Precision: number of positive instances correctly predicted / number of positive predictions.  \\mathrm{\\frac{TP}{TP + FP}}  4. Recall: same as sensitivity.  \\mathrm{\\frac{TP}{TP + FN}}  5. F1-score: harmonic mean of the precision and recall.  \\mathrm{2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}}\n\n\n\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc 1. An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The x axis is false positive rate and y axis is true positive rate. 2. An ROC curve plots TPR vs. FPR at different classification thresholds. For all models that first produce a score and then thresholded to give the classification, different thresholds mean different number of positive and negative predictions. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives and the curve will always go higher. 3. A perfect model will have a straight horizontal line at y = 1 while a perfectly wrong model will have a horizontal line at y = 0. A random guessing model will have a diagonal line, which means that the model has no class separation capacity. 4. AOC stands for area under the curve and measures the area under the ROC curve. It is used to measure a model’s performance across all possible classification thresholds.\n\n\n\n\nCalculate moving average of a set of data points is creating a series of averages of different subsets of the dataset. The dataset to be analyzed usually contains time series data (data that is indexed by timestamps). Exponential moving average (EMA) is one type of moving average algorithm.\nThe EMA value at timestamp t calculates the average of the data from the beginning to timestamp t, which is calculated by adding up the weighted value of the data at timestamp t and the weighted EMA value on the previous timestamp t-1.\nThe weighted part is called the smoothing factor and is a hyper-parameter that is between 0 and 1. Higher smoothing factor means the current data value is weighted more and the previous EMA value is weighted less in the calculation of the new EMA value.\nThis techniques is usually used by gradient optimizer to calculate new learning rate.\n\nNotes: 1. [EMA equation]: If the current timestamp is t, then the equation for EMA is:  y_{t} = \\alpha x_{t} + (1 - \\alpha)y_{t-1}  where y_{t} is the moving average at timestamp t, x_{t} is the data point at timestamp t, and \\alpha is the smoothing factor, which is the range [0, 1].\n\n\n\nTODO"
  },
  {
    "objectID": "Notes/ML Q&A.html#neural-network",
    "href": "Notes/ML Q&A.html#neural-network",
    "title": "ML Q & A",
    "section": "",
    "text": "Gradient descent is an optimization algorithm that can iteratively minimize the target function to find its local minimum. If the target function is convex, then gradient descent can also find its global minimum.\nFirst we calculate the derivatives of the target function with respect to the parameters of the model. This is the gradient.\nSecond we update the parameters to the opposite direction of the gradients to minimize the value of the target function. This is the descent.\nGradient descent will minimize the target function, while gradient ascent, which updates the parameters to the same direction of the gradients, will maximize the target function.\n\n\n\n\n\nTODO\n\n\n\n\nhttps://ruder.io/optimizing-gradient-descent/index.html#fn4 https://cs231n.github.io/neural-networks-3/#update 1. Normal gradient descent is batch gradient descent. One batch means one complete run of the training set. Thus, we need to evaluate all instances of the training set before one gradient update. Gradient descent is more slow, but guaranteed to converge to the local minimum. 2. Stochastic gradient descent means that one gradient update is performed for each instance evaluated. This approach converges faster and can be used on the fly as the new instance comes in, but can cause target function to fluctuate. 3. Mini-batch is the combination of the two above. Instead of whole batch or single instance, we take subset of training set as the mini-batch and evaluate them to get gradient for a single gradient update. This combines the benefits of two method above.\nNotes: 1. [Different gradient descent optimization algorithms] 1. SGD  \\theta_{t+1} = \\theta_{t} - \\lambda \\partial_{t}(\\theta)  where \\lambda is the learning rate and \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t. 2. SGD with Momentum: It will help the convergence speed of SGD because it reduces the oscillations of the SGD near the local minimas, which is done by building up the velocity in the correct direction that has consistent gradients  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\mu is the momentum parameter that is typically 0.9 and v_{t} is the correct accumulated gradient direction at time t. 3. Adagrad: It will make the learning rates of the weights that receive high gradients reduced, and the learning rates of the weights that receive small or infrequent updates increased. Thus we don’t have to manually tune the learning rates in the training progress. However, since there is no way to reduce the accumulated squared gradients in the denominator of the learning rate, the monotonically decreasing learning rate in the training process will eventually stop the training.  g_{t} = g_{t-1} + \\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{g_{t} + \\epsilon}} \\partial_{t}(\\theta)  where g_{t} is the accumulation of the squared gradient for \\theta until time t and \\epsilon is a small value used to prevent the division by 0. 4. RMSprop: RMSprop improves on Adagrad by replacing the accumulation of the past squared gradients with the exponential moving average of the past squared gradients. This can solve the issue of Adagrad that the learning rates are monotonically decreasing.  e_{t} = \\beta e_{t-1} + (1-\\beta)\\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{e_{t} + \\epsilon}} \\partial_{t}(\\theta) where e_{t} is the exponential moving average of the squared gradient for \\theta until time t and \\beta is like momentum that controls degree of weighting decay and is usually set to 0.9. 5. Adam: Adam improves on RMSprop by replacing the raw gradient with the exponential moving average of the past gradients in the update step. It thus combines the benefits of RMSprop and SGD with momentum.  v_{t} = \\beta_{1} v_{t-1} + (1-\\beta_{1})\\partial_{t}(\\theta)   e_{t} = \\beta_{2} e_{t-1} + (1-\\beta_{2})\\partial_{t}^{2}(\\theta)   \\hat{v}_{t} = \\frac{v_{t}}{1-\\beta_{1}^{t}}   \\hat{e}_{t} = \\frac{e_{t}}{1-\\beta_{2}^{t}}   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda \\hat{v}_{t}}{\\sqrt{\\hat{e}_{t} + \\epsilon}}  where \\hat{v}_{t} and \\hat{e}_{t} are the bias corrected versions of v_{t} and e_{t} and \\beta^{t} means the \\beta to the power of t.\n\n\n\n\nVanishing gradient happens to the parameters in the earlier layers of a deep neural network where the gradients are so small in the back-propagation process that the weights are not really changed.\nThe primary reason for this problem is the choice of activation functions such as sigmoid or hyperbolic tangent function, whose gradients are very small and are always much less than 1. If multiple layers with such activations are stacked together, the gradients to the earlier layers of the networks are multiplied lots of times with the loss gradient in the back-propagation process. Each layer reduce the original loss gradient by a fraction and in the end the gradient to the earlier layers are very small. Therefore, the large number of layers are also an important reason for vanishing gradient problem.\nOne effective solution to the problem is to use other activation functions such as ReLU.\n\n\n\n\n\nIt can help solve the vanishing gradient problem. The gradient of the ReLU for inputs larger than 0 is 1 and thus the gradient won’t be reduced in the back-propagation process.\nReLU is computationally more efficient because it only needs to cut the negative input to 0.\nHistorically speaking, ReLU is just good enough for neural network to be trained stably.\n\n\n\n\n\nL_1 regularization is also called Lasso regularization. It adds the sum of the absolute value of all weights in the neural network as a penalty term to the loss function.\nL_2 regularization is also called Ridge regularization. It adds the sum of the squared value of all weights in the neural network as a penalty term to the loss function.\nThey both are used to reduce over-fitting issue of the large neural network.\nThe key difference is the gradient of each penalty. The gradient of Lasso is a 1 or -1 depending on the sign of each weight, while the gradient of Ridge is 2 times the value of the parameter. The weights with Lasso can possibly shrink to exactly 0, while weights with Ridge can only shrink to a very small value instead of exact 0 because the gradients also decrease as the weights decrease. Thus Lasso can be used to train sparse neural network.\n\n\n\n\n\nDropout is to randomly drop neurons of the network in the training process to avoid over-fitting. A neuron is dropped means that the data and the gradient don’t go through that neuron in both forward or backward process.\nTypically dropout is applied per layer and we can set a probability p for each layer to indicate what percentage of neurons we want to drop in that layer. p is usually selected for 0.5 for hidden layer, but a less value for input layer like 0.1 because randomly dropping an entire column of input data is very risky.\nThe dropout neurons are changed every instance or mini-batch depending on what type of gradient descent we are using. We only apply dropout in the training process and we will use all neurons for testing to have consistent output.\n\n\n\n\n\nBatch normalization is to normalize the inputs to each layer for each mini-batch. Batch normalization is proved to help neural network training converges faster.\nBatch normalization first normalize the inputs to each layer by subtracting the mini-batch mean from each value and then divide it by the mini-batch standard deviation. This process will make each input value to be in the range between 0 to 1. Then we need to scale and shift the normalized value into a desirable range. The coefficients for scaling and shifting are also two parameters that are needed to be learned in the backward propagation.\nThe challenge that batch normalization is trying to solve is called internal covariate shift. Since each layer’s output is fed into next layer’s input, the change of the weights in the first layer due to backward process after a mini-batch will cause the change of its output distribution. The internal covariate shift slows down the training process because the learning in the next iteration needs to accommodate for this change.\n\nNotes: 1. [Batch Normalization Layer]: Batch normalization layer can be appended before each layer and outputs the same dimension as the inputs 1. Get the mean \\mu_{B} and standard deviation \\sigma_{B} of the inputs in a mini-batch:  \\mu_{B}=\\frac{1}{m}\\sum_{i=1}^{m}x_i   \\sigma_{B}=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_{B})^{2}}  2. Normalize the input:  \\hat{x}_i=\\frac{x_i-\\mu_{B}}{\\sigma_{B}}  3. Scale and shift back, where \\gamma is the scaling parameter and \\beta is the shifting parameter:  y_{i} = \\gamma\\hat{x}_{i}+\\beta \n\n\n\nhttps://www.deeplearning.ai/ai-notes/initialization/index.html 1. Xavier initialization is a specific way of initialize the weights and bias in the neural network such that the variance of the activations are relatively the same across all layers. 2. If we use Xavier initialization method, the bias will be initialized to 0 and the weights are randomly sampled from a normal distribution that has mean of 0 and variance of 1 over the the number of neurons in the last layer. 3. If we initialize the weights to have the same value, all neurons will have the same activations. Same activations mean that all neurons will have the same gradients and will evolve the same throughout the training. 4. If we initialize the weights to be too small or too large, the activations of each layer in the first several iterations will also be very small or large. Since gradients of weights for each layer are calculated based on the activations, large weights will result in gradient exploding and small weights result in gradient vanishing, both of which prevent the neural from efficiently learning."
  },
  {
    "objectID": "Notes/ML Q&A.html#unsupervised-learning",
    "href": "Notes/ML Q&A.html#unsupervised-learning",
    "title": "ML Q & A",
    "section": "",
    "text": "https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning#clustering\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html 1. K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed. 2. We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change. 3. K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.\nNotes 1. [K-means algorithm]: 1. Initialize cluster centroids \\mu_{1}, ..., \\mu_{k} randomly. 2. Repeat until convergence: 1. Get the centroid for each instance x_{i}:  c_{i} = \\underset{\\mu_{i}}{\\operatorname{argmin}} \\lVert x_{i}-\\mu_{i} \\rVert^{2}  2. Update the centroid based on the instances:  \\mu_{j} = \\frac{\\sum_{i}^{m}1_{\\{c_i=j\\}}x_{i}}{\\sum_{i}^{m}1_{\\{c_i=j\\}}} \n\n\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c 1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. 2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.\nNotes: 1. [Eigenvectors, Eigenvalues]: Given a matrix A\\in\\mathbb{R}^{n\\times n}, \\lambda is said to be an eigenvalue of A if there exists a eigenvector z\\in\\mathbb{R}^n \\neq 0, such that:  Az = \\lambda z  2. [Eigendecomposition (spectral decomposition]: Let M be a real symmetric d \\times d matrix with eigenvalues \\lambda_{1}, ... , \\lambda_{d} and corresponding orthonormal eigenvectors u_{1}, ..., u_{d}. Then:  M = Q \\Lambda Q^T  where \\Lambda is a diagonal matrix with \\lambda_{1}, ... , \\lambda_{d} in diagonal and 0 elsewhere and Q matrix has u_{1}, ..., u{d} vectors as columns."
  },
  {
    "objectID": "Notes/ML Q&A.html#supervised-learning",
    "href": "Notes/ML Q&A.html#supervised-learning",
    "title": "ML Q & A",
    "section": "",
    "text": "[Bayes’ theorem]: the conditional possibility of event A given the event B is true P(A|B) can be computed as:  P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}  which in the Bayesian term is written as:  \\mathrm{Posterior} = \\frac{\\mathrm{Likelihood} \\cdot \\mathrm{Prior}}{\\mathrm{Evidence}}  If we think A as a label and B as a set of features:\n\nP(A|B) is the posterior probability of a label given a set of features.\nP(B|A) is the likelihood which is the probability of a set of features given a label.\nP(A) is the prior probability of a label.\nP(B) is the evidence probaility of a set of features.\n\n[Naive Bayes]: Naive Bayes is a classifier that selects the label \\hat{y} from all possible labels y \\in Y that has maximum conditional possibility given the instance \\mathbf{x}.  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} P(y|\\mathbf{x})  Applying Bayes’ theorem, we have:  P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\cdot P(y)}{P(\\mathbf{x})}  Since P(\\mathbf{x}) is a constant and is independent from P(y_{i}), we can simply drop it:  P(y|\\mathbf{x}) \\propto P(\\mathbf{x}|y) \\cdot P(y)  If we assume that each feature is independent from each other (naive conditional independence assumption), the possibility that the features values are all in \\mathbf{x} is the product of their possibilities:  P(\\mathbf{x}|y) = \\prod_{i}^{m}P(x_{i}|y)  Put them together, we have:  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} \\prod_{i}^{m}P(x_{i}|y) \\cdot P(y) \n\n\n\n\n\nhttps://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html 1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. 2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. 3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model’s output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction’s error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. 4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression. 5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels.\nNotes: 1. [Mean squared Error (MSE)]:\n \\mathrm{MSE} = \\frac{1}{n}\\sum_{i}^{n}(y_{i}-\\hat{y}_{i})^2  where y_{i} is the actual label and \\hat{y}_{i} is the prediction given by the classifier. 1. [Binary cross entropy (BCE)]: only works if the labels y_{i} are 0 or 1 and the values of predictions \\hat{y}_{i} are between 0 and 1.  \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y})))  which can be decomposed to two cases for each prediction and label pair:  -\\log(\\hat{y}_{i}) \\;\\mathrm{if}\\; y=1   -\\log(1 - \\hat{y}_{i}) \\;\\mathrm{if}\\; y=0  1. [Sigmoid (logistic) function and logic function]:  \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}   \\mathrm{logic}(x) = \\log(\\frac{x}{1 - x}) The inverse of sigmoid function is the logic function: \n    \\begin{alignat}{2}\n    x &= \\frac{1}{1 + e^{-y}} \\\\\n    \\frac{1}{x} &= 1 + e^{-y} \\\\\n    e^{-y} &= \\frac{1 - x}{x} \\\\\n    e^{y} &= \\frac{x}{1 - x} \\\\\n    y &= \\log(\\frac{x}{1 - x}) \\\\\n    \\end{alignat}\n     1. [How to solve the parameters for linear regression and logistic regression] 1. Linear regression can be solved mathematically by setting partial derivative of loss w.r.t each weight to 0: (bias is removed for simplification)  \\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{N} \\sum_{i}^{N} x_{i,k} \\bigg(\\sum_{j}^{D}w_{j}x_{i,j} - \\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} - x_{i, k}\\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}   = \\frac{2}{N} \\sum_{j}^{D} w_{j} \\bigg( \\sum_{i}^{N} x_{i,j}x_{i,k} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}  1. Gradient descent can be applied to solve both linear regression and logistic regression. Logistic regression doesn’t have a closed-form solution because of the non-linearity that the sigmoid function imposes.\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/extend-lm.html 1. Generalized linear models build on linear regression models to predict a non-Gaussian distribution. It keeps the weighted sum of the features of the linear regression, but connect the weighted sum and the expected mean of the output distribution through a possibly nonlinear function. 2. For example, the logistic regression is a type of the generalized linear models and it assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logic function. 3. Generalized additive models further relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature. It allows to model the potentially non-linear relations between the features and the output.\nNotes: 1. [Assumptions of linear regression]: 1. The input features are independent from each other (no interactions between the features). 2. The output distribution y given the input features X follows a Gaussian distribution. This follows the following theorem: &gt; Let X_1, ..., X_n be n mutually independent normal random variables, having means \\mu_1, ..., \\mu_n and variances \\sigma_1^2, ... \\sigma_n^2. If the random variable Y is a linear combinations of the X with w_1, ..., w_n coefficients: Y=\\sum_{i=1}^{n}w_iX_i, then Y is a Gaussian distribution with the mean \\mathrm{E}[Y] = \\sum_{i=1}^{n}b_i\\mu_i and variance \\mathrm{Var}[Y] = \\sum_{i=1}^{n}b_i^2\\sigma_i^2. 3. The true relationship between each feature X_i and y is linear. 2. [Components of GLM] 1. Random component: the probability distribution of the output variable Y. It’s expected value (mean value) is \\mathrm{E}(Y). 2. Systematic component: the weighted sum \\sum_{1}^{n}w_ix_i + w_0. 3. Link function: the relation between the random component and the systematic component g.  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}w_ix_i + w_0  3. [Equation of GAM]  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}f(x_i)  where f() can be arbitrarily defined function.\n\n\n\nhttps://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/ https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf 1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters. 1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane. 1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. 1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.\nNotes: 1. [SVM without slacks (hard margin SVM)]: Given a dataset with n instances x_{i} \\in R^{d} and n labels y_{i} \\in \\{-1, 1\\}, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights w \\in R^{d} and a bias b \\in R, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem: \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     1. Solving the above optimization problem will give us two parallel hyperplanes (w x + b = 1 and w x + b = -1) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between. 1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as  \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert w \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert w \\rVert} = \\frac{2}{\\lVert w \\rVert}  1. The constraints specify that the instances must be on the correct side of the two hyperplanes:  w x_{i} + b \\geq 1 \\quad \\mathrm{if} y_{i} = 1   w x_{i} + b \\leq -1 \\quad \\mathrm{if} y_{i} = -1  and y_{i}(w x_{i} + b) \\geq 1 summarizes the above two conditions. 1. [SVM with slacks (soft margin SVM)]: In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} + C \\sum_{i}^{n} \\xi_{i} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots n \\\\\n    \\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     where \\xi_{i} is the slack variable for the instance x_{i} and C is a hyperparameter that penalizes the misclassification of x_{i}. 1. If \\xi_{i} is nonzero for x_{i}, it means that x_{i} is on the misclassified side of w x_{i} + b = 1 (or w x_{i} + b = -1) and the distance is \\xi_{i}. 1. If C = 0, \\xi_{i} can be arbitrary large for each x_{i}. If C \\to \\inf, it is the same as hard margin SVM because any misclassification can induce infinite loss.\n\n[Solving hard margin SVM]\n\nRewrite the primal program for easier Lagrangian computation below: \n\\begin{alignat}{2}\n\\min \\quad & \\frac{1}{2} ww \\\\\n\\text{s.t. } \\quad & -(y_{i}(w x_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots n \\\\\n\\end{alignat}\n\nWe can derive the Lagrangian primal function from the primal program: \n\\begin{alignat}{2}\nL(w, b, \\alpha) & = f(w, b) + \\sum_{i}^{n} \\alpha h_{i}(w, b) \\\\\n& = \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n where \\alpha is a new variable called Lagrangian multiplier.\nThen we can write and solve Lagrangian dual function: \n\\begin{alignat}{2}\ng(\\alpha) & = \\min_{w, b} L(w, b, \\alpha) \\\\\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over w: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} & = 0 \\\\\nw - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} & = 0 \\\\\nw & = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over b: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial b} & = 0 \\\\\n\\sum_{i}^{n} \\alpha_{i}y_{i} & = 0 \\\\\n\\end{alignat}\n Plug in w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} back to g(\\alpha): \n\\begin{alignat}{2}\ng(\\alpha)\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right)\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 \\right) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\  \n\\end{alignat}\n Since we know that \\alpha_{i}y_{i} = 0, then b\\sum_{i}^{n} \\alpha_{i}y_{i} = 0, and thus the final Lagrange dual function is:  g(\\alpha) = \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Notice that \\alpha_{i}y_{i} = 0 is added as part of the constraint.\nSince strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:\n\nThe Lagrange dual problem only involves \\alpha_{i}, but primal problem has w and b, which are much more parameters.\nThe Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.\n\n\n[Solving soft margin SVM]\n\nSimilar as hard margin SVM, we can write Lagrangian dual function as: \n\\begin{alignat}{2}\ng(\\alpha, \\beta) & = \\min_{w, b} \\frac{1}{2} ww\n- \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n\\end{alignat}\n where a new Lagrange multiplier is introduced for the constraint \\xi_{i} \\geq 0.\nSimilar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the w, b, and \\xi_i: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} = 0 & \\Rightarrow w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i}^{n} \\alpha_{i}y_{i} = 0 \\\\\n\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n\\end{alignat}\n and plug the w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} and C = \\alpha_{i} + \\beta_{i} back in g(\\alpha, \\beta). \n\\begin{alignat}{2}\ng(\\alpha, \\beta)\n& = \\min_{w, b} \\frac{1}{2} ww + C\\sum_{i}^{n}\\xi_{i} - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right)\n     + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n} \\alpha_{i}\\xi_{i} + \\sum_{i}^{n} \\beta_{i}\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\sum_{i}^{n}\\alpha_{i}  - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\end{alignat}\n which has exactly the same form as Lagrangian dual function of hard margin SVM.\nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\beta_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Since we know C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}, the constraint \\beta_{i} \\geq 0 can be removed by merging into \\alpha_{i} \\geq 0: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n The only difference with Lagrange dual problem of hard margin SVM is the addition of C \\geq \\alpha_{i}.\n\n[Kernel trick]\n\nKernel trick\n\n[Duality and KKT conditions]\n\nThe Lagrangian dual problem:\n\nGiven a minimization primal problem: \n\\begin{alignat}{2}\n\\min_{x} \\quad & f(x) \\\\\n\\text{s.t. } \\quad & h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\quad & l_{j}(x) = 0, \\quad j = 1, \\dots, m \\\\\n\\end{alignat}\n\nThe Lagrangian is defined as:  L(x, u, v) = f(x) + \\sum_{i}^{n} u_{i}h_{i}(x) + \\sum_{j}^{m} v_{j}l_{j}(x)  where u_{i} and v_{j} are new variables called Lagrangian multipliers.\nThe Lagrange dual function is:  g(u, v) = \\min_{x} L(x, u, v) \nThe Lagrange dual problem is: \n\\begin{alignat}{2}\n\\max_{u, v} \\quad & g(u, v) \\\\\n\\text{s.t. } \\quad & u \\geq 0 \\\\\n\\end{alignat}\n\nThe properties of dual problem:\n\nThe dual problem is always convex even if the primal problem is not convex.\nFor any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).\n\n\nKarush-Kuhn-Tucker (KKT) conditions\n\nGiven the Lagrange dual problem stated above, the KKT conditions are:\n\nStationarity condition:  0 \\in \\partial \\left( f(x) + \\sum_{i=1}^{n} u_{i} h_{i}(x) + \\sum_{j=1}^{m} v_{j}l_{j}(x) \\right) \nComplementary Slackness:  u_{i}h_{i}(x) = 0, \\quad i = 1, \\dots, n \nPrimal feasibility:  h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n   l_{j}(x) = 0,  \\quad j = 1, \\dots, m \nDual feasibility:  u_{i} \\geq 0, \\quad i = 1, \\dots, n \n\nIf a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the x^{*} and u^{*}, v^{*} are primal and dual solutions if and only if x^{*} and u^{*}, v^{*} satisfy the KKT conditions.\n\n\n\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. 2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. 3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.\nNotes 1. [Gini Index and Information Entropy]: Both applies to a dataset (instances with labels) to measure its uncertainty. They both become 0 when there is only one class in the set.\nGini Index (Gini impurity) measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.  G(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)(1-\\textrm{P}(c)) = 1 - \\sum_{c=1}^{C} \\textrm{P}(c)^2  Information Entropy can be roughly thought as the dataset’s variance.  E(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)\\log_2\\textrm{P}(c)  In both cases, \\mathcal{D} is the dataset to be evaluated, C is the total number of classes in \\mathcal{D} and \\textrm{P}(c) is the probability of picking an instance with the class c (fraction of instances with class c in \\mathcal{D}). 2. [Gini Gain and Information Gain]: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset.  G(\\mathcal{D}, S) = M(\\mathcal{D}) - \\sum_{s\\in S}\\frac{\\lvert s \\rvert}{\\lvert D \\rvert} M(s) where \\mathcal{D} is the dataset before splitting, S are subsets of \\mathcal{D} created from all possible splitting of \\mathcal{d}, M is Gini Index (G) or Information Entropy (E), and \\lvert \\cdot \\rvert gives the number of items in a set. 3. [Decision tree training algorithm]: We consider binary classification decision tree. Given a dataset \\mathcal{D}, 1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. 2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. 3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset. 4. The splitting stops when no further splitting can be made (the dataset contains only one class).\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Random Forest contains many decision trees and combine all their outputs to give a final decision. 2. A particular goal in training a random forest is to make each tree in the forest different from each other. First is to use bootstrapping, which means that each decision tree is trained on different dataset that is randomly sampled with replacement from the original dataset. Then to further inject randomness, random subset of the features instead of all features are considered in each split of the decision tree. Then the final output of random forest is to take the majority vote or average each output. 3. The goal of randomize the decision trees and taking the aggregation result is to reduce the variance and thus prevent over-fitting of the single decision tree. By taking an average of the random predictions, some errors can cancel out. Using multiple trees in the prediction make random forest a black box and the explanation for a prediction is hard to be understood by the users.\nNotes: 1. [Bagging]: Bagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output.\n\n\n\nhttps://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html\nhttps://arxiv.org/pdf/1403.1452.pdf 1. Adaboost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (&gt;50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. 2. Adaboost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. 3. Adaboost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner.\nNotes: 1. [Adaboost algorithm] Here we show the adaboost algorithm for binary classification problems (y \\in \\{-1, 1\\}). 1. For the dataset with N instances, initialize the observation weights for each instance w_i=\\frac{1}{N}, i=1,2, ... ,N. 2. For m = 1 ... M, 1. Fit a classifier G_m(x) to the training instances with weights w_i. 2. Compute  E_m=\\frac{\\sum_{i=1}^{N} w_i \\mathcal{1}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^{N}w_i}  3. Compute  \\alpha_m = \\log(\\frac{1-E_m}{E_m})  4. Set  w_i \\gets w_i \\cdot e^{\\alpha_m y_i G_m(x_i)}  3. Final output of Adaboost:  G(x) = \\textrm{sign} (\\sum_{m=1}^M \\alpha_m G_m(x)) \n\n\n\n\nGradient boosting can be seen as the generalized version of boosting, i.e. Adaboost is one special case of gradient boosting. GB can be seen as\nGB is a generalized additive model of n weak learners.  G(x) = g_{1}(x) + \\dots + g_{n}(x)  where G(x) is the final gradient boosting model and g(x) is one type of weak learners.\nThe weak learner g(x) can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting.\n$g_{1}(x) g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets.\n\nGiven a loss function L(\\cdot), a training set X = \\{\\mathbf{x_{i}}\\}, \\mathbf{y} = \\{y_{i}\\}, a learning rate \\alpha, and a number of iterations M, the algorithm to train a GBRT is as follows: 1. Intiailize G(x) by fitting CART on D 1. For m = 1 \\dots M, 1. Evaluate the loss over the current G(x) 1. Calculate the gradient of the loss w.r.t the labels to get the residuals \\tilde{\\mathbf{y}}:  \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}} Note \\tilde{\\mathbf{y}} has the same shape as \\mathbf{y}. 1. Use X and residuals \\tilde{\\mathbf{y}} as the new training set to train a CART g(x). 1. Add the new weak leaner into the current model:  G(x) = G(x) + \\alpha g(x)"
  },
  {
    "objectID": "Notes/SSGD.html",
    "href": "Notes/SSGD.html",
    "title": "SSGD",
    "section": "",
    "text": "03-05-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/SSGD.html#proposed-ssgd-framework",
    "href": "Notes/SSGD.html#proposed-ssgd-framework",
    "title": "SSGD",
    "section": "Proposed SSGD framework",
    "text": "Proposed SSGD framework\nTypically, a loss function for a neural network function is as follows:\n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda G(\\boldsymbol{\\theta})\nIn which, J({\\boldsymbol{\\theta}}) is the accuracy loss function and G(\\boldsymbol{\\theta}) is the regularization function.\nIn the SSR literature, G(·) is usually referred to as the general diversity measure that serves as an alternative to the L_0 “norm” for encouraging sparsity.\nWe further define a separable diversity measure that has the form\nG(\\theta) = \\sum g(\\theta_i)\nwhere g(·) has the following properties:\n\ng(u) is symmetric.\ng(\\lvert u \\rvert) is monotonically increasing with \\lvert u \\rvert.\ng(u) is finite.\ng(u) is strictly concave in \\lvert u \\rvert or u^2.\n\nAny function that holds the above properties is a candidate for effective SSR algorithm and thus a candidate for the proposed SSGD framework."
  },
  {
    "objectID": "Notes/SSGD.html#iterative-reweighting-frameworks.",
    "href": "Notes/SSGD.html#iterative-reweighting-frameworks.",
    "title": "SSGD",
    "section": "Iterative reweighting frameworks.",
    "text": "Iterative reweighting frameworks.\nIterative reweighted l_2 and l_1 frameworks are two popular frameworks in SSR literature to solve the above problem.\nFor each timestamp t, we have these two target functions to minimize, respectively for l_2 and l_1 frameworks:\n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_2^2 \n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_1 \nwhere \\Omega is a vector of w_i, which is pre-computed based on \\theta_i as discussed below and \\theta_i is the variable that we are solving for.\nIn both cases, they need to satisfy g(u)=f(u^2) and g(u)=f(|u|) respectively, where f(z) is concave for z \\in R_+.\nIn each iteration, after \\theta_i is solved, we can use it to update w_i. Their update rules for l_2 and l_1 frameworks are:\nw_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-\\frac{1}{2}} \\mathrm{where} \\; z=\\theta_i^2\nw_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-1} \\mathrm{where} \\; z=\\lvert \\theta_i \\rvert"
  },
  {
    "objectID": "Notes/SSGD.html#log-sum-as-the-diversity-measure",
    "href": "Notes/SSGD.html#log-sum-as-the-diversity-measure",
    "title": "SSGD",
    "section": "Log-sum as the diversity measure",
    "text": "Log-sum as the diversity measure\nLog-sum penalties for the reweighted l_2 and l_1 frameworks:\ng(\\theta_i) = \\log(\\theta_i^2 + \\epsilon)\ng(\\theta_i) = \\log(\\lvert \\theta_i \\rvert + \\epsilon)\nwhere \\epsilon &gt; 0 and smaller \\epsilon induces stronger sparsity.\nTheir w_i can be computed as:\nw_i = (\\theta_i^2 + \\epsilon) ^ \\frac{1}{2}\nw_i = \\lvert \\theta_i \\rvert + \\epsilon"
  },
  {
    "objectID": "Notes/SSGD.html#ssgd-sparsity-promoting-stochastic-gradient-descent",
    "href": "Notes/SSGD.html#ssgd-sparsity-promoting-stochastic-gradient-descent",
    "title": "SSGD",
    "section": "SSGD: Sparsity-promoting Stochastic Gradient Descent",
    "text": "SSGD: Sparsity-promoting Stochastic Gradient Descent\nSparsity-promoting matrix store all the coefficients that can be multiplied with the gradient for each parameter during gradient descent update to promote sparsity.\nThe coefficient can be obtained from w_i by\ns_i = \\frac{w_i^2}{\\frac{1}{|\\mathcal{I}^k|} \\sum_{j\\in \\mathcal{I}^k} w_j^2} \\mathrm{, for} \\; i \\in \\mathcal{I}^k\nwhere \\mathcal{I}^k is the set of parameters (weights) in layer k and |\\mathcal{I}^k| is the number of parameters in the layer k.\nComplete SSGD training algorithm for DNN:\n\nApply one of the general diversity measure in the loss function.\nAnd for each gradient update: 1. Compute scaling factors: w_i based on \\theta_i. 2. Compute s_i by the equation above. 3. Update parameters: \\theta_i = \\theta_i - \\mathrm{lr} \\times s_i \\times \\nabla_i, where \\nabla_i is the gradient for \\theta_i."
  },
  {
    "objectID": "Notes/RIPPER.html",
    "href": "Notes/RIPPER.html",
    "title": "RIPPER",
    "section": "",
    "text": "This page contains my reading notes on\nSome of the knowledge are also from:"
  },
  {
    "objectID": "Notes/RIPPER.html#introduction",
    "href": "Notes/RIPPER.html#introduction",
    "title": "RIPPER",
    "section": "Introduction",
    "text": "Introduction\n\n\nIn this paper, Cohen first implements his own version of IREP (Incremental Reduced Error Pruning) with some minor differences and has support multi-label problems and missing attributes.\nThen he proposes several major changes to IREP and names the improved version IREP*.\nFinally, based on IREP*, he proposes a new rule mining algorithm called RIPPER (Repeated Incremental Pruning to Produce Error Reduction)."
  },
  {
    "objectID": "Notes/RIPPER.html#irep-cohen-version",
    "href": "Notes/RIPPER.html#irep-cohen-version",
    "title": "RIPPER",
    "section": "## IREP (Cohen version)",
    "text": "## IREP (Cohen version)\n\nIREP algorithm\nThe characteristics of IREP have two fold: 1. Separate and conquer: the covered instances in the training set are removed after a rule is found; thus in the next iteration, a new rule will be learned on the training instances that have not been covered by the previously found rules. 1. Integration of pre-pruning and post-pruning: 1. Pre-pruning: some training examples are deliberately ignored to in the training process (early stopping condition). 1. Post-pruning: first the model is trained to fit the training set perfectly and then some parts of the model are deleted after the training (branch cutting).\n\nFunction: IREP.\nInput: the training set \\mathcal{D} with binary labels and all possible features \\mathcal{F}.\nOutput: the learned rule set \\mathcal{R}. 1. Initialize an empty rule set \\mathcal{R}. 1. While there are still positive instances in \\mathcal{D}: 1. Randomly choose 2/3 from \\mathcal{D} as the growing set \\mathcal{G} and the rest 1/3 becomes the pruning set \\mathcal{P}. 1. R = GrowRule(\\mathcal{G}) 1. R = PruneRule(\\mathcal{P}, R) 1. If the accuracy of R &lt; 0.5 on \\mathcal{P}: break 1. Add R to \\mathcal{R}. 1. Remove instances that are covered by R from \\mathcal{D}. 1. Return \\mathcal{R}\n\nThe minor differences between the Cohen’s implementation of IREP and the original version are: 1. stopping condition. The original IREP stopped when the accuracy of the learned rule is less than the accuracy of the empty rule instead of 50%. 1. PruneRule algorithm, which is to be detailed later.\n\n\nGrow a Rule\nIn each iteration, the feature f with value v that has the maximum FOIL score is selected to the rule and the iterations terminate when the rule doesn’t cover any negative instances in the growing set.\n\nFunction: GrowRule.\nInput: the growing set \\mathcal{G} with binary labels and all possible features \\mathcal{F}.\nOutput: the unpruned rule R. 1. Initialize an empty rule R. 1. Until all instances in \\mathcal{G} that satisfy R are positive (accuracy of R is 1 in \\mathcal{G}) or there is no feature to add: 1. For every feature f \\in \\mathcal{F} not in R and every possible value v \\in \\mathcal{V}(f): 1. Create a temp rule R_{t} by copying current R. 1. Add (f, v) to R_{t}. 1. Calculate FOIL’s information gain of R_{t}: \\mathrm{Foil}(R, R_{t}) based on \\mathcal{G}. 1. Get the R_{t}^{max} with the max value of \\mathrm{Foil}(R_{t}). 1. R=R_{t}^{max}. 1. Return R.\n\n[Support for categorical and continuous features]: the definition of \\mathcal{V}(f) for different feature f is different for categorical and numerical features. 1. For a categorical feature f_{c}, \\mathcal{V}(f) is the collection of all possible values that f_{c} can take. 1. For a numerical feature f_{n}, \\mathcal{V}(f) is the Cartesian product of \\{\\leq, \\geq\\} and all values of f that appear in the training set. For example, if all values that appear in the training set for feature age is \\{10, 20, 30\\}, then \\mathcal{V}(\\text{age}) is \\{\\leq 10, \\geq 10, \\leq 20, \\geq 20, \\leq 30, \\geq 30\\}\n[FOIL’s information gain]: it gives how much information entropy is reduced from R_{old} to R_{new}.\n \\operatorname{Foil}(R_{old}, R_{new}) = P(R_{new}) \\left( \\log_{2} \\left( \\frac{P(R_{new})}{P(R_{new}) + N(R_{new})} \\right) - \\log_{2} \\left( \\frac{P(R_{old})}{P(R_{old}) + N(R_{old})} \\right) \\right) \nwhere P(R) (N(R)) is the number of positive (negative) instances covered by R.\n\n\nPrune a Rule\nPruneRule considers deleting any final sequence of conditions in the order they are grown from the rule and chooses the deletion that maximizes the Rule-Value metric on the pruning set.\n\nFunction: PruneRule.\nInput: the pruning set \\mathcal{P} and the unpruned rule R.\nOutput: the pruned rule R_{p}. 1. Initialize R_{p} = R. 1. For all (f, v)_{i} \\in R starting from the last added one to the first one: 1. Removing (f, v)_{i} from R. 1. If \\operatorname{Value}(R) \\geq \\operatorname{Value}(R_{p}): then R_{p} = R. 1. Return R_{p}\n\nThe original implementation of IREP only considers the “deletions of a single final condition”.\n[IREP Rule-Value metric]:\n \\operatorname{Value}(R) = \\frac{P(R) + (N - N(R))}{P + N} \nwhere P (N) is the total number of positive (negative) instances and P(R) (N(R)) is the number of positive (negative) instances covered by R."
  },
  {
    "objectID": "Notes/RIPPER.html#irep-as-an-improved-version-of-irep",
    "href": "Notes/RIPPER.html#irep-as-an-improved-version-of-irep",
    "title": "RIPPER",
    "section": "IREP* as an improved version of IREP",
    "text": "IREP* as an improved version of IREP\n\nThe support for multi-class and missing value allows IREP to be applied on a wide range of benchmarks and Cohen further improves on his implementation of IREP on the stopping condition and pruning metric.\n\nNew Rule-Value metric\nThe IREP Rule-Value metric sometimes is highly unintuitive. Assuming P and N are fixed to be 3000, the IREP Rule-Value metric prefers R_{1} over R_{2} in the following example, but R_{2} is obviously more predictive. - R_{1}: P(R_{1}) = 2000, N(R_{1}) = 1000, \\operatorname{Value}(R) = \\frac{4000}{6000} - R_{2}: P(R_{2}) = 1000, N(R_{2}) = 1, \\operatorname{Value}(R) = \\frac{3999}{6000}\nCohen’s solution doesn’t have the issue mentioned above.\n[IREP* Rule-Value metric]:\n \\operatorname{Value}(R) = \\frac{P(R) - N(R)}{P(R) + N(R)} \nwhere P(R) is the number of positive instances covered by R and N(R) is the number of negative instances covered by R\n\n\nNew Stopping condition\nThe IREP stops adding rules when the current learned rule has a bad (&lt; 50%) accuracy on the pruning set. This estimate often makes the algorithm stop too early especially if the current learned rule has very low coverage (the algorithm will stop if, for example, the rule only covers 2 instances and 1 instance has negative label).\nIREP* defines the stopping condition based on the total description length value of the currently learned rule set on the pruning set.\n\n\nCalculate the total description length of \\mathcal{R}: \\operatorname{MDL}(\\mathcal{R}).\nIf \\operatorname{MDL}(\\mathcal{R}) &gt; \\operatorname{MDL}_{min} + d: break\nIf \\operatorname{MDL}(\\mathcal{R}) &lt; \\operatorname{MDL}_{min}: \\operatorname{MDL}_{min} = \\operatorname{MDL}(\\mathcal{R})\n\n\nwhere \\operatorname{MDL}(\\mathcal{R}) is the total Description Length of the rule set \\mathcal{R} and d is a hyperparameter with the default value of 64 in the paper’s experiment.\n[MDL Principle (Minimum Description Length Principle)]: From the Machine Learning perspective, each model derived from the dataset can be characterized by a description length, which is defined as the number of bits required to encode the model and the data from which it was learned. MDL Principle states that the model with the minimum description length is generally preferred to avoid over-fitting.\nDescription length consists of model description length (theory cost) and exceptions description length (exceptions cost). - Model description length measures the complexity of the model. Higher model description length means that the model is more complex and thus more prone to over-fitting. - Exceptions description length measures the degree to which the model incorrectly fit to the data. The large the exceptions description length, the more error-prone the model is.\nFor RIPPER, the description length of a rule set is defined as the sum of the model description length of each rule plus the exceptions description length of the whole rule set:\n \\operatorname{MDL}(\\mathcal{R}) = \\sum_{R_{i} \\in \\mathcal{R}} \\operatorname{MDL}_{M}(R_{i}) + \\operatorname{MDL}_{E}(\\mathcal{R}) \nModel description length of each rule calculates how many bits are needed to encode a rule:\n \\operatorname{MDL}_{M}(R) = 0.5(k\\log_{2}\\frac{1}{p} + (n - k) \\log_2\\frac{1}{1 - p} + \\lVert k \\rVert) \nwhere k is the number of features in the rule, n is the number of all features, and p = \\frac{k}{n}. \\lVert k \\rVert = \\log_{2}(k) is the number of bits required to encode the number k. The 0.5 factor is to “account for possible redundancies”.\nExceptions description length evaluates the errors of the rule set on a given dataset:\n \\operatorname{MDL}_{E}(\\mathcal{R}) = \\log_{2}{P(\\mathcal{R}) \\choose \\mathit{FP}(\\mathcal{R})} + \\log_{2}{N(\\mathcal{R}) \\choose \\mathit{FN}(\\mathcal{R})}\nwhere P(\\mathcal{R}) (N(\\mathcal{R})) is the number of positive (negative) instances covered by the rule set \\mathcal{R} and \\mathit{FP}(\\mathcal{R}) (\\mathit{FN}(\\mathcal{R})) is the number of false positives (false negatives) covered by the rule set \\mathcal{R}."
  },
  {
    "objectID": "Notes/RIPPER.html#ripper",
    "href": "Notes/RIPPER.html#ripper",
    "title": "RIPPER",
    "section": "## RIPPER",
    "text": "## RIPPER\nTODO: Many of the implementation details are from this public Github implementation, since the original paper doesn’t elaborate on how exactly they are implemented.\n\nRIPPER = IREP* + a post-processing optimization\nRIPPER further improves on IREP* by post-pruning the rules generated by IREP*.\n\nFunction: RIPPER.\nInput: a training set \\mathcal{D}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Run IREP* on \\mathcal{D} to get \\mathcal{R}. 1. \\mathcal{R}_{o} = Optimize(\\mathcal{D}, \\mathcal{R}). Note that \\mathcal{D} here is a copy of the original training set (no removal from IREP). 1. While there are still positive instances in \\mathcal{D}: 1. R = GrowRule*(\\mathcal{D}). 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. Add R to \\mathcal{R}_{o}. 1. Return \\mathcal{R}_{o}.\n\nThe optimize procedure proposes 2 more versions of the rule for each rule learned from IREP* and select the best version using MDL metric to add to the final rule set.\n\nFunction: Optimize.\nInput: a training set \\mathcal{D} and a rule set \\mathcal{R}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Initialize an empty rule set \\mathcal{R}_{o}. 1. For each R_{i} \\in \\mathcal{R} in the order R_{i} is learned in \\mathcal{R}: 1. Randomly choose 2/3 from \\mathcal{D} as the growing set \\mathcal{G} and the rest 1/3 becomes the pruning set \\mathcal{P}. 1. Grow the replacement rule from scratch using FOIL’s information gain: \\hat{R}_{i} = GrowRule(\\mathcal{G}). 1. Form a new rule set by replacing R_{i} in \\mathcal{R} with \\hat{R}_{i}: \\mathcal{\\hat{R}} = \\{R_{1}, \\dots, \\hat{R}_{i}, \\dots, R_{n}}. 1. Prune the replacement rule: \\hat{R}_{i} = PruneRule(\\mathcal{P}, \\hat{R}_{i}), but using accuracy of the rule set \\mathcal{\\hat{R}} instead of Rule-Value metric as the maximizing objective. 1. Update the \\hat{R}_{i} in \\hat{\\mathcal{R}} with the pruned version. 1. Grow the revision rule from the current rule R_{i} using FOIL’s information gain: \\bar{R}_{i} = GrowRule(\\mathcal{G}). 1. Form a new rule set by replacing R_{i} in \\mathcal{R} with \\bar{R}_{i}: \\mathcal{\\bar{R}} = \\{R_{1}, \\dots, \\bar{R}_{i}, \\dots, R_{n}}. 1. Prune the revision rule: \\bar{R}_{i} = PruneRule(\\mathcal{P}, \\bar{R}_{i}), but using accuracy of the rule set \\mathcal{\\bar{R}} instead of Rule-Value metric as the maximizing objective. 1. Update the \\bar{R}_{i} in \\bar{\\mathcal{R}} with the pruned version. 1. The best rule from the 3 versions is the one whose corresponding rule set has the smallest minimum total description length: R_{i}^{best} = \\arg \\min_{R \\in \\{R_{i}, \\hat{R}_{i}, \\bar{R}_{i}\\}} \\operatorname{MTDL} (\\{R_{1}, \\dots, R, \\dots, R_{n}\\}). 1. Add R_{i}^{best} to \\mathcal{R}_{o}. 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. If there is no positive instances in \\mathcal{D}: break. 1. Return \\mathcal{R}_{o}.\n\n[Minimum total description length]: Given a rule set, the minimum total description length is the description length of the rule set after deleting the rules that increase the total description length of the rule set.\n\n\nRIPPER2 and RIPPERk\nRIPPER2 is just running Optimize again on the output of RIPPER, while RIPPERk is to run RIPPER k times.\n\nFunction: RIPPER2.\nInput: a training set \\mathcal{D}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Run RIPPER on \\mathcal{D} to get \\mathcal{R}. 1. \\mathcal{R}_{o} = Optimize(\\mathcal{D}, \\mathcal{R}). Note that \\mathcal{D} here is a copy of the original training set (no removal from IREP). 1. While there are still positive instances in \\mathcal{D}: 1. R = GrowRule*(\\mathcal{D}). 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. Add R to \\mathcal{R}_{o}. 1. Return \\mathcal{R}_{o}."
  },
  {
    "objectID": "Notes/CG Decision Rules.html",
    "href": "Notes/CG Decision Rules.html",
    "title": "CG Decision Rules",
    "section": "",
    "text": "09-19-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/CG Decision Rules.html#problem-formulation",
    "href": "Notes/CG Decision Rules.html#problem-formulation",
    "title": "CG Decision Rules",
    "section": "Problem formulation",
    "text": "Problem formulation\nThese paper try to find a decision rule set as a binary classifier by fomulating a integer programming problem.\n\nA decision rule set is a binary classifier that only takes binary inputs.\n\nThe decision rule set is in the form of DNF (OR-of-ANDs). Each rule is an AND gate of part of the binary inputs and all rules are connected using an OR gate.\nIf a instance satisfies any of the rules, it is predicted to be positive. Otherwise, the instance is predicted to be negative.\n\nLearning a set of decision rule set can be formulated as a large mixed integer programming (MIP) problem.\n\nA rule is a subset of input features. If there are m binary input features, a rule can be represented by a binary vector of length m (1 means the feature is included in the rule, 0 otherwise).\nLet \\mathcal{K} denotes all possible DNF rules. \\mathcal{K} has in total 2^{m} - 1 of rules (a vector of all 0 is not a rule).\nGiven a training set that has a set of positive instances \\mathcal{P} and a set of negative instances \\mathcal{N}, we can minimize a loss function that characterize the classification error."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#loss",
    "href": "Notes/CG Decision Rules.html#loss",
    "title": "CG Decision Rules",
    "section": "0-1 Loss",
    "text": "0-1 Loss\nThe 0-1 loss is the most direct loss the counts the numbers of false positives and false negatives.\n\n\\begin{aligned}\n\\min \\quad & \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} + \\sum_{i \\in \\mathcal{N}} \\mathcal{L}_{i} \\\\\n\\text{s.t. } \\quad & \\mathcal{L}_{i} + \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\geq 1, && i \\in \\mathcal{P} \\\\\n& w_{k} \\leq \\mathcal{L}_{i}, && i \\in \\mathcal{N}, k \\in \\mathcal{K}_{i} \\\\\n& w_{k}, \\mathcal{L}_{i} \\in \\{0, 1\\}, && k \\in \\mathcal{K}, i \\in \\mathcal{P} \\cup \\mathcal{N} \\\\\n\\end{aligned}\n\nApart from \\mathcal{K}, \\mathcal{P}, \\mathcal{N} defined above, some of new terms are defined:\n\nLet w_{k} \\in \\{0, 1\\} be a variable of if rule k \\in \\mathcal{K} is selected in the rule set.\nLet \\mathcal{L}_{i} \\in \\{0, 1\\} be a variable indicating if an instance i \\in \\mathcal{P} \\cup \\mathcal{N} is misclassified.\nLet \\mathcal{K}_{i} \\subset \\mathcal{K} be the set of rules met by the instance i.\n\nExplanation:\n\nObjective \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} is the number of false positives and \\sum_{i \\in \\mathcal{N}} \\mathcal{L}_{i} is the number of false negatives. Thus the loss function above is directly minimizing the classification errors.\nConstraint 1 defines the misclassification of a positive instance (false negative) by forcing \\mathcal{L}_{i} to take value 1 if no rule that is satisfied by the instance i \\in \\mathcal{P} is in the rule set.\nConstraint 2 defines the misclassification of a negative instance (false positive) by forcing \\mathcal{L}_{i} to take value 1 if any rule that is satisfied by the instance i \\in \\mathcal{N} is in the rule set.\nConstraint 3 defines types of each variables.\n\nThe first constraint has \\lvert \\mathcal{P} \\rvert number of equations while the second one has at most \\lvert \\mathcal{N} \\mathcal{K} \\rvert (assuming each negative instance satisfies all possible rules) equations. The second constraint is very expensive and can be avoided using the hamming loss introduced below."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#hamming-loss",
    "href": "Notes/CG Decision Rules.html#hamming-loss",
    "title": "CG Decision Rules",
    "section": "Hamming Loss",
    "text": "Hamming Loss\nHamming loss also considers characterizing the numbers of false positives and false negatives. While Hamming loss incurs the same loss for each false negative, Hamming loss considers the loss to be the number of selected rules a negative instance satisfy for each false positive.\n\n\\begin{aligned}\n\\min \\quad & \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} + \\sum_{i \\in \\mathcal{N}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\\\\n\\text{s.t. } \\quad & \\mathcal{L}_{i} + \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\geq 1, && i \\in \\mathcal{P} \\\\\n& w_{k}, \\mathcal{L}_{i} \\in \\{0, 1\\}, && k \\in \\mathcal{K}, i \\in \\mathcal{P} \\cup \\mathcal{N} \\\\\n\\end{aligned}\n\nExplanation:\n\nObjective \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} counts the number of false positives (same as that of the 0-1 loss), while \\sum_{i \\in \\mathcal{N}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} counts the number of selected rules of each negative instance.\nConstraint 1 defines a false negative (same as the 1st constraint of 0-1 loss).\nConstraint 2 defines types of each variables (same as the 3rd constraint of 0-1 loss).\n\nHamming loss eliminate the 2nd constraint in 0-1 loss by converting the counting of the false positives to be the hamming distances between true positives and false positives, thus reducing the problem size."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#fairness-metrics-and-constraints",
    "href": "Notes/CG Decision Rules.html#fairness-metrics-and-constraints",
    "title": "CG Decision Rules",
    "section": "Fairness metrics and constraints",
    "text": "Fairness metrics and constraints\n\nGiven a set of protected features (e.g. sex, red), the training instances can be divided into different groups \\mathcal{G} (e.g. 4 groups: \\mathcal{G} = {sex=male, sex=female, red=True, red=False}).\nTwo types of fairness metrics:\n\nEquality of opportunity requires the false negative rate to be equal for all groups.\nEqualized odds requires both the false positive and false negative rate to be equal for all groups. Equalized odds is stricter version of equality of opportunity.\n\nThe maximum violation can be used to measure the unfairness of the classifier:  \\Delta(d) = \\max_{g, g' \\in \\mathcal{G}} \\lvert \\mathbb{P}(d(X)=-1 \\mid Y=1, G=g) - \\mathbb{P}(d(x)=-1 \\mid Y=1, G=g') \\rvert This equation gives the maximum difference of the equality of opportunity (false negative rate) between the instances in any two groups g and g'.\nAssuming \\mathcal{G} = \\{1,2\\} (there are only two groups), we can incorporate the following constraints to the Hamming loss to impose the fairness:\n\nEquality of opportunity (false negative rate):  \\frac{1}{\\lvert \\mathcal{P}_{1} \\rvert} \\sum_{i \\in \\mathcal{P}_{1}} \\mathcal{L}_{i} - \\frac{1}{\\lvert \\mathcal{P}_{2} \\rvert} \\sum_{i \\in \\mathcal{P}_{2}} \\mathcal{L}_{i} \\leq \\epsilon_{1}   \\frac{1}{\\lvert \\mathcal{P}_{2} \\rvert} \\sum_{i \\in \\mathcal{P}_{2}} \\mathcal{L}_{i} - \\frac{1}{\\lvert \\mathcal{P}_{1} \\rvert} \\sum_{i \\in \\mathcal{P}_{1}} \\mathcal{L}_{i} \\leq \\epsilon_{1} \nEquality odds (false positive rate):  \\frac{1}{\\lvert \\mathcal{N}_{1} \\rvert} \\sum_{i \\in \\mathcal{N}_{1}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} - \\frac{1}{\\lvert \\mathcal{N}_{2} \\rvert} \\sum_{i \\in \\mathcal{N}_{2}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\leq \\epsilon_{2}   \\frac{1}{\\lvert \\mathcal{N}_{2} \\rvert} \\sum_{i \\in \\mathcal{N}_{2}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} - \\frac{1}{\\lvert \\mathcal{N}_{1} \\rvert} \\sum_{i \\in \\mathcal{N}_{1}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\leq \\epsilon_{2}  where in both cases, \\mathcal{P}_{1} is the positives instances that are in group 1 (similar for \\mathcal{P}_{2}, \\mathcal{N}_{1}, \\mathcal{N}_{2}) and \\epsilon_{1}, \\epsilon_{2} are two constants that bound the maximum allowed unfairness for false positives and false negatives."
  },
  {
    "objectID": "Notes/L0 Regularization.html",
    "href": "Notes/L0 Regularization.html",
    "title": "L0 Regularization",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nplt.style.use(\"ggplot\")\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef hard_sigmoid(x):\n    return min(1, max(0, x))\n\ndef logit_dist():\n    u = np.random.random()\n    logit = np.log(u) - np.log(1 - u)\n    \n    return logit\n\ndef binary_concrete(loc, temp):\n    logit = logit_dist()\n    bc = sigmoid((logit + loc) / temp) \n    \n    return bc\n\ndef stretch_binary_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n    bc = binary_concrete(loc, temp)\n    stretch_bc = bc * (zeta - gamma) + gamma\n    \n    return stretch_bc\n\ndef hard_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n    stretch_bc = stretch_binary_concrete(loc, temp, gamma, zeta)\n    hc = hard_sigmoid(stretch_bc) \n\n    return hc\n    \ndef plot_probability(list_samples, bins=100, **kwargs):\n    plt.figure(figsize=(16, 8))\n    for samples in list_samples:\n        weights = np.ones_like(samples) / len(samples)\n        plt.hist(samples, weights=weights, bins=bins, alpha=0.5, **kwargs)\n05-03-2021 (Updated 06-02-2022)\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/L0 Regularization.html#problem-formulation",
    "href": "Notes/L0 Regularization.html#problem-formulation",
    "title": "L0 Regularization",
    "section": "Problem formulation",
    "text": "Problem formulation\nGiven a vector x of length n (matrix can also be seen as a vector by stacking up the rows/cols), the common vector norms are:\n\nL_{0} norm:\n \\sum_{i=1}^{n} \\mathbb{1}[x_{i} \\neq 0] \nL_{1} norm:\n \\sum_{i=1}^{n} \\lvert x_{i} \\rvert \nwhich is also called ridge regularization in neural network.\nL_{2} norm:\n \\sum_{i=1}^{n} x_{i}^2 \nwhich is also called lasso regularization in neural network.\nL_{\\infty} norm:\n \\max_{i=1}^{n} \\lvert x_i \\rvert \n\nThe normal way to prune the edges of the neural network is to use L_{1} or L_{2} regularization to drive weights to near 0 (not exactly 0), and then directly set all weights that are less than threshold to 0.\n\nL_{0} is not used because the operation of counting the number of 0s is not differentiable.\nHowever, L_{0} regularization is still desired because it won’t affect the magnitude of the weights in the pruning process."
  },
  {
    "objectID": "Notes/L0 Regularization.html#general-recipe-of-l_0-regularization",
    "href": "Notes/L0 Regularization.html#general-recipe-of-l_0-regularization",
    "title": "L0 Regularization",
    "section": "General recipe of L_{0} regularization",
    "text": "General recipe of L_{0} regularization\nThe loss function used to train a neural network with L_{0} regularization is:\n \\mathcal{L}(f(x, \\theta), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[\\theta_{i} \\neq 0] \nwhere\n\n\\mathcal{L} is a standard loss function (cross-entropy loss, softmax)\n\\theta are the parameters in the network\nx, y are training instances\n\\lambda is a hyper-parameter that balance loss and the regularization.\n\nIf we attach a trainable binary random variable z_{i} to each element of the model parameter \\theta_{i}, then the weights used in the feed-forward operation of the neural network can be replaced by \\theta \\odot z. The loss function then becomes:\n \\mathcal{L}(f(x, \\theta \\odot z), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[z_{i} \\neq 0]\nwhere\n\nz \\in \\{0, 1\\}^{\\lvert \\theta \\rvert} is randomly sampled in each forward propagation according to some distribution.\n\\odot corresponds to the elementwise product.\n\nIf we assume each z_{i} as a binary random variable with a Bernoulli distribution that has a parameter \\pi_{i}, i.e. z_{i} = \\mathrm{Bern}(\\pi_{i}), then the loss function becomes:\n \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} \nwhere \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} [\\cdot] gives an expectation value of a function that has a Bernoulli distribution z as the input.\nThe reformulation of the above loss function can be established because\n\nSince the minimum of a function is upper bounded by the expectation of the function, minimizing \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] is the same as minimizing the upper bound of \\mathcal{L}(f(x, \\theta \\odot z), y).\nAccording to the definition of the Bernoulli distribution, \\pi gives the probability of z being 1 (non zero). Thus, minimizing \\pi is to increase the probability of z being 0.\n\nIn the equation above,\n\n\\pi_{i} is a parameter that we want to be learned using gradient descent.\nThus, the second term \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} can be directly minimized to regularize \\pi because gradient of the second term w.r.t to pi_{i} can be easily calculated.\nHowever, the first term \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] is still problematic because z as a categorical random variable cannot be differentiated with respect to \\pi."
  },
  {
    "objectID": "Notes/L0 Regularization.html#hard-concrete-distribution",
    "href": "Notes/L0 Regularization.html#hard-concrete-distribution",
    "title": "L0 Regularization",
    "section": "Hard concrete distribution",
    "text": "Hard concrete distribution\n\nBinary Concrete distribution\nThe binary concrete distribution can be seen as a continuous approximation of the Bernoulli distribution. It has 2 parameters:\n\nlocation \\alpha: similar to the probability parameter of the Bernoulli distribution.\ntemperature \\beta: it controls how similar the binary concrete distribution is with the Bernoulli distribution.\n\nUsing the the reparametrization trick, the binary concrete distribution s can be represented as:\n s = \\operatorname{sigmoid} \\left( \\frac{\\alpha + l}{\\beta} \\right) \nwhere l is a sample from the logistic distribution.\n\ndef plot_logit(size=100000, **kwargs):\n    logit_samples = [logit_dist() for _ in range(size)]\n    plot_probability([logit_samples])\n    \ninteract(plot_logit);\n\n\n\n\n\ndef plot_bc(size=100000, **kwargs):\n    ber_samples = np.random.binomial(1, 0.5, size=size)\n    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([ber_samples, bc_samples])\n    \ninteract(plot_bc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\n\n\nFrom binary concrete to hard concrete\nWe cannot use s (binary concrete distribution) to directly replace z (Bernoulli distribution) - The range of s is (0, 1) and never touches 0 or 1. - However, we want z to be either 0 or 1.\nA simple trick to solve this problem is\n\nFirst “stretch” the binary concrete distribution from interval (0, 1) to interval (\\gamma, \\zeta) with \\gamma &lt; 0 and \\zeta &gt; 1\n \\bar{s} = s(\\zeta - \\gamma) + \\gamma \nThen clip the stretch binary concrete distribution into the range [0, 1]\n \\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1) \nwhere\n\n\\mathrm{clip}(x, \\mathrm{min}, \\mathrm{max}) means to clip x between the range [\\mathrm{min}, \\mathrm{max}].\n\\bar{z} is a random variable that follows the hard concrete distribution and it can be used to replace z.\n\n\n\ndef plot_sbc(size=100000, **kwargs):\n    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([bc_samples, sbc_samples])\n    \ninteract(plot_sbc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\n\ndef plot_hc(size=100000, **kwargs):\n    hc_samples = [hard_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([hc_samples, sbc_samples])\n    \ninteract(plot_hc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\nAlso, we cannot use \\alpha (location parameter of binary concrete distribution) to replace \\pi (probability parameter of Bernoulli distribution) in the regularization term of the loss function. - Remember that the regularization term above measures the sum of the probabilities of the z (Bernoulli distribution) being non-zero.\n$$ \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} $$\nMeasuring the probability of \\bar{z} of being non-zero is the same as measuring the probability of \\bar{s} being positive.\n\nBecause all negative values of \\bar{s} is clipped to be 0.\nSince we know that the total probability of a random variable being all values is 1, the probability of \\bar{s} being positive is written as:\n \\textrm{q}(\\bar{s} &gt; 0 | \\phi) = 1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi) \nwhere\n\n\\textrm{Q}(\\cdot) is the cumulative density function (CDF)\n\\mathrm{Q}(s \\leq 0 | \\phi) gives the probability of s being negative.\n\n\nThus, the loss function above can be rewritten using the hard concrete distribution by:\n \\mathbb{E}_{\\bar{s}} \\big[ \\mathcal{L}(f(x, \\theta \\odot \\bar{z}), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} (1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi)) \nThe loss function is “fully” differentiable with respect to \\alpha\n\n\\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1) and \\bar{s} is a differentiable function with respect to \\alpha.\n\\mathrm{Q}(\\bar{s} \\leq 0 | \\phi) can be written as a differentiable function with respect to \\alpha."
  },
  {
    "objectID": "Notes/Confident Learning.html",
    "href": "Notes/Confident Learning.html",
    "title": "Confident Learning",
    "section": "",
    "text": "This page contains my reading notes on\nNotations:\nThe procedure needs 2 inputs:"
  },
  {
    "objectID": "Notes/Confident Learning.html#five-methods-to-identify-instances-with-noisy-labels",
    "href": "Notes/Confident Learning.html#five-methods-to-identify-instances-with-noisy-labels",
    "title": "Confident Learning",
    "section": "Five Methods to identify instances with noisy labels",
    "text": "Five Methods to identify instances with noisy labels\n\n1. CL baseline 1: C_{confusion}\nThe instance is considered to have the noisy label if its given label is different from the label with largest predicted probability.\n\n\n2. CL method 2: C_{\\tilde{y}, y^{*}}\nIn this method, a matrix called confident joint C_{\\tilde{y}, y^{*}} will be calculated using \\hat{\\mathbf{P}} and \\tilde{\\mathbf{y}}.\n\n\n\nC_{\\tilde{y}, y^{*}}\ny^{*} = 0\ny^{*} = 1\ny^{*} = 2\n\n\n\n\n\\tilde{y} = 0\n100\n40\n20\n\n\n\\tilde{y} = 1\n56\n60\n0\n\n\n\\tilde{y} = 2\n32\n12\n80\n\n\n\nTo calculate this matrix:\n\nFor each label j, calculate the average predicted probability t_{j} using \\hat{\\mathbf{P}}.\nFor each instance \\mathbf{x}_{k} with the given label i in the training set, the entry at row i and column j of the confident joint matrix C_{\\tilde{y}=i, y^{*}=j} will be added 1, where the true label j is the one that has the largest predicted probability among all the labels whose predicted probabilities are above the respected t_{j}.\n\nThis basically means that the true label for a given instance is the label whose predicted probability by a model is larger than the average predicted probability.\nIf there are more than one such labels, chose the one that has the largest predicted probability.\nIt is possible that no such label exists, and thus the instance won’t be counted in the matrix.\n\n\nThus, each entry in C_{\\tilde{y}, y^{*}} is corresponding to a set of training instances.\nAll instances that fall in the off-diagonal of the C_{\\tilde{y}, y^{*}} are considered to have noisy labels.\n\n\n3. CL method 3: Prune by Class (PBC)\nIn this method and all methods below, another matrix called Estimate of joint \\hat{Q}_{\\tilde{y}, y^{*}} will be calculated using C_{\\tilde{y}, y^{*}}.\n\n\n\n\n\n\n\n\n\n\\hat{Q}_{\\tilde{y}, y^{*}}\ny^{*} = 0\ny^{*} = 1\ny^{*} = 2\n\n\n\n\n\\tilde{y} = 0\n0.25\n0.1\n0.05\n\n\n\\tilde{y} = 1\n0.14\n0.15\n0\n\n\n\\tilde{y} = 2\n0.08\n0.03\n0.2\n\n\n\n\\hat{Q}_{\\tilde{y}, y^{*}} basically is the normlized C_{\\tilde{y}, y^{*}}: each entry in C_{\\tilde{y}, y^{*}} is divided by the total number of training instances.\nFor each class i, the a number of instances with lowest predicted probabilities for label i are considered to have noisy labels, where a is calculated as the product of n and the sum of off-diagonal entries on row i of \\hat{Q}_{\\tilde{y}, y^{*}}.\n\n\n4. CL method 4: Prune by Noise Rate (PBNR)\nFor each off-diagonal entry in \\hat{Q}_{\\tilde{y}, y^{*}}, the n \\times \\hat{Q}_{\\tilde{y}=i, y^{*}=j} number of instances with largest margin are considered to have noisy labels, where the margin of an instance \\mathbf{x}_{k} with respect to given label i and true label j is \\hat{\\mathbf{P}}_{k, j} - \\hat{\\mathbf{P}}_{k, i}.\n\n\n5. CL method 5: C + NR\nThe instance is considered to have a noisy label if both PBC and PBNR consider it to have a noisy label."
  },
  {
    "objectID": "Notes/MobileNets.html",
    "href": "Notes/MobileNets.html",
    "title": "Mobile Nets",
    "section": "",
    "text": "05-01-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary",
    "href": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary",
    "title": "Mobile Nets",
    "section": "Standard 2D convolution layer (Preliminary)",
    "text": "Standard 2D convolution layer (Preliminary)\n\nConvolution layer takes as input M matrices (M input channels) of shape D_i \\times D_i, which can be seen as a 3D volume of size D_i \\times D_i \\times M.\nEach convolution layer consists of N sets of learnable weights (N filters or N output channels), each of which corresponds to a learnable bias. Each set of learnable weights (a filter) is a small 3D volume with size D and same depth as the input volume M, i.e. M matrices of shape D \\times D.\nThe output activation of the convolution layer has the N matrices of shape D_o \\times D_o, which can also be seen as a 3D volume of size D_o \\times D_o \\times M.\n\nD_o is computed as:  D_o = \\frac{D_i - D}{S} + 1 \n\nIn the equation above, S is stride, which tells how many pixels the filter moves each time. The input matrices will usually be zero-padded to control the output size.\nThe animation below shows exactly how the output of a convolution layer is computed."
  },
  {
    "objectID": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary-1",
    "href": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary-1",
    "title": "Mobile Nets",
    "section": "Standard 2D convolution layer (Preliminary)",
    "text": "Standard 2D convolution layer (Preliminary)\n\nConvolution layer takes as input M matrices (M input channels) of shape D_i \\times D_i, which can be seen as a 3D volume of size D_i \\times D_i \\times M.\nEach convolution layer consists of N sets of learnable weights (N filters or N output channels), each of which corresponds to a learnable bias. Each set of learnable weights (a filter) is a small 3D volume with size D and same depth as the input volume M, i.e. M matrices of shape D \\times D.\nThe output activation of the convolution layer has the N matrices of shape D_o \\times D_o, which can also be seen as a 3D volume of size D_o \\times D_o \\times M.\n\nD_o is computed as:  D_o = \\frac{D_i - D}{S} + 1 \n\nIn the equation above, S is stride, which tells how many pixels the filter moves each time. The input matrices will usually be zero-padded to control the output size.\nThe animation below shows exactly how the output of a convolution layer is computed.\n\n\nfrom IPython.display import IFrame\nIFrame('https://cs231n.github.io/assets/conv-demo/index.html', width=792, height=700)"
  },
  {
    "objectID": "Notes/MobileNets.html#depthwise-separable-convolution-mobilenetv1",
    "href": "Notes/MobileNets.html#depthwise-separable-convolution-mobilenetv1",
    "title": "Mobile Nets",
    "section": "Depthwise separable convolution (MobileNetV1)",
    "text": "Depthwise separable convolution (MobileNetV1)\n\nInstead of using the traditional convolution layers as the building blocks, MobileNetV1 uses layers called depthwise separable convolutions.\nEach depthwise separable convolution consists of two layers:\n\nDepthwise convolution layer: it has M separate filters of shape D \\times D \\times 1, where the mth filter is applied to the mth input channel. This layer can also be thought to have 1 filter of shape D \\times D \\times M, but the output channels will not be added together. In this way, the output size is D_o \\times D_o \\times M.\nPointwise convolution layer: it has N separate filters of shape 1 \\times 1 \\times M. Each filter is doing a linear combination of the matrices (input channels) of the input volume, and thus N filters will have N matrices (output channels). It takes the output of depthwise convolution as the input, and the output of pointwise convolution has the shape of D_o \\times D_o \\times N, just like the output of a standard convolution layer.\n\nEach filter in a convolution layer both filters and combines inputs in one step. Depthwise separable convolution splits this into two layers: depthwise convolution does filtering while pointwise convolution combines.\nComputational cost comparison:\n\nStandard convolution layer (assuming stride is 1): D_i \\times D_i \\times D \\times D \\times M \\times N\nDepthwise convolution layer (assuming stride is 1): D_i \\times D_i \\times D \\times D \\times M\nPointwise convolution layer: D \\times D \\times M \\times N\nThus the computation cost of depthwise separable convolution is less than that of the standard convolution layer:  \\frac{D_i \\times D_i \\times D \\times D \\times M + D \\times D \\times M \\times N}{D_i \\times D_i \\times D \\times D \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_i^2} \n\n\n\nfrom IPython.display import Image\nImage(filename='./1.png', width=480)"
  },
  {
    "objectID": "Notes/MobileNets.html#inverted-residual-and-linear-bottleneck-block-mobilenetv2",
    "href": "Notes/MobileNets.html#inverted-residual-and-linear-bottleneck-block-mobilenetv2",
    "title": "Mobile Nets",
    "section": "Inverted residual and linear bottleneck block (MobileNetV2)",
    "text": "Inverted residual and linear bottleneck block (MobileNetV2)\n\nThe improvement of this layer over the depthwise separable convolution is based on the following two assumptions:\n\nAlthough Non-linear activations (Relu) can increase representational complexity, it will result in information loss if the input doesn’t have lots of channels.\nActivation maps are able to be encoded in low-dimensional subspaces (less channels).\n\nAn expansion layer is appended to the front of the depthwise separable convolution. If the input has the shape of D_i \\times D_i \\times M, the expansion layer is a pointwise convolution layer that has tM filters of size 1 \\times 1 \\times M, where t is called expansion factor/ratio and should be larger than 1. The output will have the shape D_i \\times D_i \\times tM. The layer expands the low-dimensional (less channels) input activation map to a higher-dimensional (more channels) space suited to non-linear activation functions.\nNon-linear activation functions (Relu6) can only be applied to the high-dimensional activation maps. The last pointwise convolution layer projects the high-dimensional space back into low-dimensional space, so it can only have linear activation functions. Therefore, linear bottleneck block has three layers:\n\nExpansion layer: tM filters of 1 \\times 1 \\times M, Relu6.\nDepthwise convolution layer: 1 filter of D \\times D \\times tM (Dwise), Relu6.\nPointwise convolution layer: M' filters of 1 \\times 1 \\times tM, Linear.\n\nIf the initial and final activation maps of the block are of the same dimensions, a residual connection between the input and output of the block (between expansion layers from two consecutive blocks) is added to aid gradient flow during back-propagation, which is achieved by:\n\nDepthwise convolution stride equals (S = 1).\nInput and output channels are equal (M' = M).\n\nThe paper emphasizes that the connection is added between the expansion layers (inverted residual connection) instead of between depthwise convolution layer (residual connection), so that the connections are created between the low-dimensional activation maps.\n\n\nfrom IPython.display import Image\nImage(filename='./2.png', width=480)\n\n\n\n\n\nfrom IPython.display import Image\ni = Image(filename='./3.png', width=480)\ni"
  },
  {
    "objectID": "Notes/MobileNets.html#squeeze-and-excite-mobilenetv3",
    "href": "Notes/MobileNets.html#squeeze-and-excite-mobilenetv3",
    "title": "Mobile Nets",
    "section": "Squeeze-and-Excite (MobileNetV3)",
    "text": "Squeeze-and-Excite (MobileNetV3)\n\nMobileNetV3 uses the block that modifies the inverted residual block by adding a Squeeze-and-Excite block between the depthwise convolution layer and pointwise convolution layer.\nGiven an input that has the shape D_i \\times D_i \\times M, Squeeze-and-Excite block in the original paper consists of three parts:\n\nSqueeze: apply global average pooling to the input, so that all information in each input channel is “squeezed” into a single channel descriptor. Each descriptor is a single value that summarizes each channel of the input. The output shape of this layer is 1 \\times 1 \\times M.\nExcite: apply two fully connected layers to the output of the Squeeze layer. Use Relu activation function for the first layer and Sigmoid activation function for the second layer. The first layer contains \\frac{M}{r} number of neurons, and thus the output of the first layer reduces the dimension by a reduction ratio r. The second layer contains M number of neurons to restore the output dimension to be the same as the output of the Squeeze layer.\nScale: the M coefficients of the output of the Excite layer are timed with M channels of input of the block to form the output of the block.\n\n\n\nfrom IPython.display import Image\nImage(filename='./4.png', width=960)\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='./5.png', width=480)"
  },
  {
    "objectID": "Notes/index.html",
    "href": "Notes/index.html",
    "title": "Research Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nBirkhoff+\n\n\n\n\n\n\n\nCG Decision Rules\n\n\n\n\n\n\n\nConfident Learning\n\n\n\n\n\n\n\nL0 Regularization\n\n\n\n\n\n\n\nML Q & A\n\n\n\n\n\n\n\nMLIC IMLI\n\n\n\n\n\n\n\nMobile Nets\n\n\n\n\n\n\n\nQuantization Survey\n\n\n\n\n\n\n\nRIPPER\n\n\n\n\n\n\n\nSGD Warm Restarts\n\n\n\n\n\n\n\nSSGD\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]