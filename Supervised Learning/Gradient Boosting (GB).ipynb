{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3fcd1e-28c3-476f-9506-c4bc68315baf",
   "metadata": {},
   "source": [
    "# Gradient Boosting (GB)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3120f1-44a9-4218-b2f7-27691c36558f",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Gradient boosting approximates the given distribution by building stage-wise weak learners.\n",
    "    1. GB is a generalized additive model of n weak learners. \n",
    "        $$ G(x) = g_{1}(x) + \\dots + g_{n}(x) $$\n",
    "        where $G(x)$ is the final gradient boosting model and $g(x)$ is one type of weak learners. \n",
    "    1. The weak learner $g(x)$ can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting. \n",
    "1. Each new weak leaner compensates the errors made by the sum of the previous weak learners by fitting only the residuals. \n",
    "    1. $g_{1}(x) \\dots g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets. \n",
    "1. Residuals (gradients) can be seen as the gaps between the current model and the final distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad71dac-068f-4d39-b6c7-21b19e519337",
   "metadata": {},
   "source": [
    "## Gradient boosting regression tree (GBRT) algorithm\n",
    "***\n",
    "\n",
    "Given a loss function $L(\\cdot)$, a training set $X = \\{\\mathbf{x_{i}}\\}$, $\\mathbf{y} = \\{y_{i}\\}$, a learning rate $\\alpha$, and a number of iterations $M$, the algorithm to train a GBRT is as follows:\n",
    "1. Intiailize $G(x)$ by fitting CART on $D$ \n",
    "1. For $m = 1 \\dots M$,\n",
    "    1. Evaluate the loss over the current $G(x)$\n",
    "    1. Calculate the gradient of the loss w.r.t the labels to get the **residuals** $\\tilde{\\mathbf{y}}$:\n",
    "        $$ \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}}$$\n",
    "        Note $\\tilde{\\mathbf{y}}$ has the same shape as $\\mathbf{y}$.\n",
    "    1. Use $X$ and **residuals** $\\tilde{\\mathbf{y}}$ as the new training set to train a CART $g(x)$.\n",
    "    1. Add the new weak leaner into the current model:\n",
    "        $$ G(x) = G(x) + \\alpha g(x) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
