{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4bb9b4-f86d-4667-9cca-f606d338f47b",
   "metadata": {},
   "source": [
    "# Decision Tree (DT)\n",
    "***\n",
    "\n",
    "1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. \n",
    "2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. \n",
    "3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2bd27-427b-4bd8-b90e-3d461ae447e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "### Statistics\n",
    "\n",
    "#### Kullback-Leibler Divergence (KL Divergence)\n",
    "\n",
    "KL Divergence is a method to measure **difference between two probability distributions** over the same random variable $X$. Given a discrete random variable $X$ and two probability distributions $P(X)$ and $Q(X)$, KL Divergence is defined as:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "D_{KL}(P \\Vert Q) & = \\mathbb{E}_{X \\sim P}[\\log P(X) - \\log Q(X)] \\\\ \n",
    "& = \\sum_{x \\in X} P(x) (\\log P(x) - \\log Q(x)) & [\\text{definition of expected value of } \\log P(x) - \\log Q(x)] \\\\\n",
    "& = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}  & [\\log a - \\log b = \\log \\frac{a}{b}] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- KL Divergence is not symmetric and thus cannot be used as an distance metric\n",
    "    $$ D_{KL}(P \\Vert Q) \\neq D_{KL}(Q \\Vert P) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834f42a-1fd3-4ab7-9aaa-167b5553fcf6",
   "metadata": {},
   "source": [
    "## Impurity function\n",
    "---\n",
    "\n",
    "Given a dataset $\\mathbf{D}$ with $C$ unique labels, impurity functions measure the impureness of the labels in $\\mathbf{D}$. \n",
    "- When there is only one class in $\\mathbf{D}$, the dataset is pure and thus impurity functions should return 0. \n",
    "- On the contrary, if all possible labels are in $\\mathbf{D}$ and also have the same number of instances, $\\mathbf{D}$ is the most impure and impurity functions should return a very large value.\n",
    "\n",
    "#### Gini Impurity Index (Gini Impurity)\n",
    "\n",
    "Gini impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. \n",
    "    \n",
    "$$ \n",
    "\\begin{align}\n",
    "G(\\mathbf{D}) & = \\sum_{c \\in C} P(c)(1 - P(c)) \\\\ \n",
    "& = \\sum_{c \\in C} P(c) - P(c)^{2} \\\\\n",
    "& = 1 - \\sum_{c \\in C} P(c)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$P(c)$ is the probability of label $c$ in the dataset, which is computed by dividing the number of instances with label $c$ by the total number instances in $\\mathbf{D}$. \n",
    "    \n",
    "#### Shannon Entropy (Entropy)\n",
    "\n",
    "$$ E(\\mathbf{D}) = \\sum_{c \\in C} P(c) \\log P(c) $$\n",
    "\n",
    "Entropy can be thought as the difference measured by KL Divergence between the probability distribution of the unique labels represented in the current dataset $\\mathbf{D}$ and the distribution of the most impure dataset. \n",
    "- The larger the entropy, the more far away from a uniform distribution is the distribution of the labels represented by $\\mathbf{D}$.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "D_{KL}(P \\Vert Q) & = \\sum_{x \\in X} P(x)(\\log P(x) - \\log Q(x)) & [\\text{KL Divergence definition}] \\\\\n",
    "& = \\sum_{c \\in C} P(c)(\\log P(c) - \\log Q(c)) & [\\text{substitute } x \\text{ with label } c] \\\\ \n",
    "& = \\sum_{c \\in C} P(c)(\\log P(c) - \\log \\frac{1}{C}) & [Q(c) = \\frac{1}{C} \\text{ is uniform distribution}] \\\\\n",
    "& = \\sum_{c \\in C} P(c)(\\log P(c) - (\\log 1 - \\log C)) \\\\\n",
    "& = \\sum_{c \\in C} P(c)(\\log P(c) + \\log C) \\\\\n",
    "& = \\sum_{c \\in C} P(c)\\log P(c) + \\log C \\sum_{c \\in C} P(c) \\\\\n",
    "& = \\sum_{c \\in C} P(c) \\log P(c) + \\log C & [\\sum_{c \\in C} P(c) = 1] \\\\\n",
    "& = \\sum_{c \\in C} P(c) \\log P(c) & [\\log C \\text{ is a constant and can be dropped}] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6dfa84-950b-4304-a3ee-aff52b51bed2",
   "metadata": {},
   "source": [
    "## Tree building\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f4e3f-a60d-4af3-950c-73d0f273cc78",
   "metadata": {},
   "source": [
    "  \n",
    "2. **[Gini Gain and Information Gain]**: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset. \n",
    "    $$ G(\\mathcal{D}, S) = M(\\mathcal{D}) - \\sum_{s\\in S}\\frac{\\lvert s \\rvert}{\\lvert D \\rvert} M(s)$$\n",
    "    where $\\mathcal{D}$ is the dataset before splitting, $S$ are subsets of $\\mathcal{D}$ created from all possible splitting of $\\mathcal{d}$, $M$ is Gini Index ($G$) or Information Entropy ($E$), and $\\lvert \\cdot \\rvert$ gives the number of items in a set. \n",
    "3. **[Decision tree training algorithm]**: We consider binary classification decision tree. Given a dataset $\\mathcal{D}$, \n",
    "    1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. \n",
    "    2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. \n",
    "    3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset.\n",
    "    4. The splitting stops when no further splitting can be made (the dataset contains only one class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ac827-4614-43a0-85da-b16d8f3d11f3",
   "metadata": {},
   "source": [
    "## References\n",
    "***\n",
    "\n",
    "1. https://victorzhou.com/blog/intro-to-random-forests/\n",
    "1. https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf\n",
    "1. https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
