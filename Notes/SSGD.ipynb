{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Created 02-27-2022 (Last commited 02-27-2022)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSGD\n",
    "\n",
    "*03-05-2021*\n",
    "\n",
    "This page contains my reading notes on \n",
    "\n",
    "- [**SSGD: Sparsity-Promoting Stochastic Gradient Descent Algorithm for Unbiased Dnn Pruning**](https://www.semanticscholar.org/paper/SSGD%3A-Sparsity-Promoting-Stochastic-Gradient-for-Lee-Fedorov/a1a7d9f694a74dbff79d6fa3d7c1f1f0cde65256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed SSGD framework\n",
    "\n",
    "Typically, a loss function for a neural network function is as follows:\n",
    "\n",
    "$$\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda G(\\boldsymbol{\\theta})$$\n",
    "\n",
    "In which, $J({\\boldsymbol{\\theta}})$ is the accuracy loss function and $G(\\boldsymbol{\\theta})$ is the regularization function.\n",
    "\n",
    "In the SSR literature, $G(·)$ is usually referred to as the **general diversity measure** that serves as an alternative to the $L_0$ \"norm\" for encouraging sparsity. \n",
    "\n",
    "We further define a separable diversity measure that has the form \n",
    "\n",
    "$$G(\\theta) = \\sum g(\\theta_i)$$\n",
    "\n",
    "where $g(·)$ has the following properties:\n",
    "\n",
    "1. $g(u)$ is symmetric.\n",
    "2. $g(\\lvert u \\rvert)$ is monotonically increasing with $\\lvert u \\rvert$.\n",
    "3. $g(u)$ is finite.\n",
    "4. **$g(u)$ is strictly concave in $\\lvert u \\rvert$ or $u^2$.**\n",
    "\n",
    "Any function that holds the above properties is a candidate for effective SSR algorithm and thus a candidate for the proposed SSGD framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative reweighting frameworks.\n",
    "\n",
    "Iterative reweighted $l_2$ and $l_1$ frameworks are two popular frameworks in SSR literature to solve the above problem. \n",
    "\n",
    "For each timestamp t, we have these two target functions to minimize, respectively for $l_2$ and $l_1$ frameworks:\n",
    "\n",
    "$$\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_2^2 $$\n",
    "\n",
    "$$\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_1 $$\n",
    "\n",
    "where $\\Omega$ is a vector of $w_i$, which is pre-computed based on $\\theta_i$ as discussed below and $\\theta_i$ is the variable that we are solving for. \n",
    "\n",
    "In both cases, they need to satisfy $g(u)=f(u^2)$ and $g(u)=f(|u|)$ respectively, where $f(z)$ is concave for $z \\in R_+$.\n",
    "\n",
    "In each iteration, after $\\theta_i$ is solved, we can use it to update $w_i$. Their update rules for $l_2$ and $l_1$ frameworks are:\n",
    "\n",
    "$$w_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-\\frac{1}{2}} \\mathrm{where} \\; z=\\theta_i^2$$\n",
    "\n",
    "$$w_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-1} \\mathrm{where} \\; z=\\lvert \\theta_i \\rvert$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-sum as the diversity measure\n",
    "\n",
    "Log-sum penalties for the reweighted $l_2$ and $l_1$ frameworks: \n",
    "\n",
    "$$g(\\theta_i) = \\log(\\theta_i^2 + \\epsilon)$$\n",
    "\n",
    "$$g(\\theta_i) = \\log(\\lvert \\theta_i \\rvert + \\epsilon)$$\n",
    "\n",
    "where $\\epsilon > 0$ and smaller $\\epsilon$ induces stronger sparsity.\n",
    "\n",
    "Their $w_i$ can be computed as: \n",
    "\n",
    "$$w_i = (\\theta_i^2 + \\epsilon) ^ \\frac{1}{2}$$\n",
    "\n",
    "$$w_i = \\lvert \\theta_i \\rvert + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSGD: Sparsity-promoting Stochastic Gradient Descent\n",
    "\n",
    "Sparsity-promoting matrix store all the coefficients that can be multiplied with the gradient for each parameter during gradient descent update to promote sparsity. \n",
    "\n",
    "The coefficient can be obtained from $w_i$ by \n",
    "\n",
    "$$s_i = \\frac{w_i^2}{\\frac{1}{|\\mathcal{I}^k|} \\sum_{j\\in \\mathcal{I}^k} w_j^2} \\mathrm{, for} \\; i \\in \\mathcal{I}^k$$\n",
    "\n",
    "where $\\mathcal{I}^k$ is the set of parameters (weights) in layer $k$ and $|\\mathcal{I}^k|$ is the number of parameters in the layer $k$. \n",
    "\n",
    "Complete SSGD training algorithm for DNN:\n",
    "\n",
    "> Apply one of the general diversity measure in the loss function. \n",
    "> \n",
    "> And for each gradient update:\n",
    "> 1. Compute scaling factors: $w_i$ based on $\\theta_i$. \n",
    "> 2. Compute $s_i$ by the equation above.\n",
    "> 3. Update parameters: $\\theta_i = \\theta_i - \\mathrm{lr} \\times s_i \\times \\nabla_i$, where $\\nabla_i$ is the gradient for $\\theta_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
