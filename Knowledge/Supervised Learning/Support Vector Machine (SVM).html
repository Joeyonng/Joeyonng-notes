<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Joeyonng - Support Vector Machine (SVM)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Joeyonng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../Knowledge" rel="" target="">
 <span class="menu-text">Knowledge</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Notes" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng-backyard/" rel="" target="">
 <span class="menu-text">Backyard</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Support Vector Machine (SVM)</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Learning Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/1_Statistical_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/2_Bayesian_Classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Classifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/3_Effective_Class_Size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Effective Class Size</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Empirical Risk Minimization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/5_Uniform_Convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Uniform Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/6_PAC_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PAC Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Knowledge/Learning Theory/7_Rademacher_Complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rademacher Complexity</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary">Preliminary</a>
  <ul class="collapse">
  <li><a href="#convex-optimization" id="toc-convex-optimization" class="nav-link" data-scroll-target="#convex-optimization">Convex Optimization</a></li>
  </ul></li>
  <li><a href="#hyperplane-and-margin" id="toc-hyperplane-and-margin" class="nav-link" data-scroll-target="#hyperplane-and-margin">## Hyperplane and margin</a>
  <ul class="collapse">
  <li><a href="#hyperplane" id="toc-hyperplane" class="nav-link" data-scroll-target="#hyperplane">Hyperplane</a></li>
  <li><a href="#margin" id="toc-margin" class="nav-link" data-scroll-target="#margin">Margin</a></li>
  </ul></li>
  <li><a href="#hard-margin-svm-svm-without-slacks" id="toc-hard-margin-svm-svm-without-slacks" class="nav-link" data-scroll-target="#hard-margin-svm-svm-without-slacks">Hard margin SVM (SVM without slacks)</a>
  <ul class="collapse">
  <li><a href="#formulation" id="toc-formulation" class="nav-link" data-scroll-target="#formulation">Formulation</a></li>
  <li><a href="#solving-hard-margin-svm" id="toc-solving-hard-margin-svm" class="nav-link" data-scroll-target="#solving-hard-margin-svm">Solving hard margin SVM</a></li>
  </ul></li>
  <li><a href="#soft-margin-svm-svm-with-slacks" id="toc-soft-margin-svm-svm-with-slacks" class="nav-link" data-scroll-target="#soft-margin-svm-svm-with-slacks">## Soft margin SVM (SVM with slacks)</a>
  <ul class="collapse">
  <li><a href="#formulation-1" id="toc-formulation-1" class="nav-link" data-scroll-target="#formulation-1">Formulation</a></li>
  <li><a href="#solving-soft-margin-svm" id="toc-solving-soft-margin-svm" class="nav-link" data-scroll-target="#solving-soft-margin-svm">Solving soft margin SVM</a></li>
  </ul></li>
  <li><a href="#inference-of-svm" id="toc-inference-of-svm" class="nav-link" data-scroll-target="#inference-of-svm">Inference of SVM</a>
  <ul class="collapse">
  <li><a href="#support-vectors" id="toc-support-vectors" class="nav-link" data-scroll-target="#support-vectors">Support vectors</a></li>
  <li><a href="#calculate-mathbfw-and-b-using-alpha_is" id="toc-calculate-mathbfw-and-b-using-alpha_is" class="nav-link" data-scroll-target="#calculate-mathbfw-and-b-using-alpha_is">Calculate <span class="math inline">\mathbf{w}^{*}</span> and <span class="math inline">b^{*}</span> using <span class="math inline">\alpha_{i}^{*}</span>’s</a></li>
  </ul></li>
  <li><a href="#kernel-trick" id="toc-kernel-trick" class="nav-link" data-scroll-target="#kernel-trick">## Kernel trick</a>
  <ul class="collapse">
  <li><a href="#why-higher-dimensions" id="toc-why-higher-dimensions" class="nav-link" data-scroll-target="#why-higher-dimensions">Why higher dimensions</a></li>
  <li><a href="#kernel-function" id="toc-kernel-function" class="nav-link" data-scroll-target="#kernel-function">Kernel function</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Support Vector Machine (SVM)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<hr>
<ol type="1">
<li>The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters.</li>
<li>To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane.</li>
<li>If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized.</li>
<li>A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.</li>
</ol>
<section id="preliminary" class="level2">
<h2 class="anchored" data-anchor-id="preliminary">Preliminary</h2>
<hr>
<section id="convex-optimization" class="level3">
<h3 class="anchored" data-anchor-id="convex-optimization">Convex Optimization</h3>
<section id="lagrangian" class="level4">
<h4 class="anchored" data-anchor-id="lagrangian">Lagrangian</h4>
<p>Given a (possibly non-convex) minimization primal problem:</p>
<p><span class="math display">
\begin{aligned}
\min_{x} \quad &amp; f(x) \\
\text{s.t. } \quad &amp; g_{i}(x) \leq 0, \quad i = 1, \dots, n \\
\quad &amp; h_{i}(x) = 0, \quad j = 1, \dots, m \\
\end{aligned}
</span></p>
<p>where <span class="math inline">x</span> here is used to represent all the input variables.</p>
<p>The <strong>Lagrangian</strong> of the primal problem above is defined as:</p>
<p><span class="math display"> \mathcal{L}(x, \lambda, \nu) = f(x) + \sum_{i=1}^{n} \lambda_{i} g_{i}(x) + \sum_{j=1}^{m} \nu_{j} h_{i}(x) </span></p>
<p>where <span class="math inline">\{ \lambda_{1}, \dots, \lambda_{n} \}</span> and <span class="math inline">\{ \nu_{1}, \dots, \nu_{m} \}</span> are two sets of new variables called Lagrangian multipliers.</p>
<p>The Lagrangian can be used to convert the primal problem with constraints to the following <strong>unconstrained</strong> problem</p>
<p><span class="math display">
\begin{aligned}
\min_{x} \mathcal{P}(x) = \min_{x} \quad \max_{\lambda, \nu} \quad &amp; \mathcal{L}(x, \lambda, \nu) \\
\text{s.t. } \quad &amp; \lambda_{i} \geq 0, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>Since the values of <span class="math inline">\{ \lambda_{1}, \dots, \lambda_{n} \}</span> and <span class="math inline">\{ \nu_{1}, \dots, \nu_{m} \}</span> can be freely chosen to maximize the Lagrangian, <span class="math inline">\mathcal{P}(x)</span> is the same as <span class="math inline">f(x)</span> when <span class="math inline">x</span> satisfies the constraints in the original primal problem. Otherwise, <span class="math inline">\mathcal{P}(x)</span> becomes infinity.</p>
<p><span class="math display">
\mathcal{P}(x) =
\begin{cases}
\begin{aligned}
&amp; f(x) &amp;&amp; \text{if } g_{i}(x) \leq 0, i = 1, \dots, n \text{ and } h_{i}(x) = 0, j = 1, \dots, m \\
&amp; \infty &amp;&amp; \text{otherwise} \\
\end{aligned}
\end{cases}
</span></p>
</section>
<section id="duality" class="level4">
<h4 class="anchored" data-anchor-id="duality">Duality</h4>
<p>We can create a new optimization problem by reverting the order of <span class="math inline">\min</span> and <span class="math inline">\max</span> in the Lagrangian unconstrained optimization problem.</p>
<p><span class="math display">
\begin{aligned}
\max_{\lambda, \nu} \mathcal{D}(\lambda, \nu) = \max_{\lambda, \nu} \quad &amp; \min_{x} \quad \mathcal{L}(x, \lambda, \nu) \\
\text{s.t. } \quad &amp; \lambda_{i} \geq 0, i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\mathcal{D}(\lambda, \nu) = \min_{x} \mathcal{L}(x, \lambda, \nu)</span> is the <strong>Lagrange dual function</strong>.</p></li>
<li><p><span class="math inline">\max_{\lambda, \nu} \mathcal{D}(\lambda, \nu)</span> is the <strong>Lagrange dual problem</strong>.</p></li>
</ul>
<p>The properties of the dual problem:</p>
<ol type="1">
<li>The dual problem is always convex even if the primal problem is not convex.</li>
<li>For any primal problem and its dual problem, the weak duality always holds.</li>
</ol>
</section>
<section id="weak-duality" class="level4">
<h4 class="anchored" data-anchor-id="weak-duality">Weak duality</h4>
<p><strong>Weak duality</strong> states that the optimal value of the primal problem is greater or equal to the optimal value of its dual problem.</p>
<p>Weak duality holds for any primal problem and its dual problem, even if the primal problem is not convex.</p>
</section>
<section id="strong-duality" class="level4">
<h4 class="anchored" data-anchor-id="strong-duality">Strong duality</h4>
<p><strong>Strong duality</strong> states that the optimal value of the primal problem is the same as the optimal value of its dual problem.</p>
<p>For convex primal problems, the strong duality holds if <strong>Slater’s conditions</strong> holds. Slater’s conditions test whether there exists an <span class="math inline">x</span> that meet all the constraints of the primal problem.</p>
</section>
<section id="karush-kuhn-tucker-kkt-conditions" class="level4">
<h4 class="anchored" data-anchor-id="karush-kuhn-tucker-kkt-conditions">Karush-Kuhn-Tucker (KKT) conditions</h4>
<p>Given the Lagrange dual problem stated above, the KKT conditions are:</p>
<ol type="1">
<li><p>Stationarity condition:</p>
<p><span class="math display"> \frac{\partial}{\partial x}  \mathcal{L}(x, \lambda, \nu) = 0 </span></p></li>
<li><p>Complementary slackness condition:</p>
<p><span class="math display"> \lambda_{i} g_{i}(x) = 0, \quad i = 1, \dots, n </span></p></li>
<li><p>Primal feasibility condition:</p>
<p><span class="math display"> g_{i}(x) \leq 0, \quad i = 1, \dots, n </span></p>
<p><span class="math display"> h_{i}(x) = 0, \quad j = 1, \dots, m </span></p></li>
<li><p>Dual feasibility condition:</p>
<p><span class="math display"> \lambda_{i} \geq 0, \quad i = 1, \dots, n </span></p></li>
</ol>
<p>If a strong duality holds, the <span class="math inline">x^{*}</span> and <span class="math inline">\lambda^{*}, \nu^{*}</span> are primal and dual solutions if and only if <span class="math inline">x^{*}</span> and <span class="math inline">\lambda^{*}, \nu^{*}</span> satisfy the KKT conditions.</p>
</section>
</section>
</section>
<section id="hyperplane-and-margin" class="level2">
<h2 class="anchored" data-anchor-id="hyperplane-and-margin">## Hyperplane and margin</h2>
<section id="hyperplane" class="level3">
<h3 class="anchored" data-anchor-id="hyperplane">Hyperplane</h3>
<p>In the following context, a hyperplane in a <span class="math inline">d</span>-dimensional space is represented by</p>
<p><span class="math display"> \mathbf{w} \cdot \mathbf{x} + b = 0 </span></p>
<p>where <span class="math inline">\mathbf{w} \in \mathbb{R}^{d}</span> and <span class="math inline">b \in \mathbb{R}</span> are constants defining the hyperplane.</p>
<ol type="1">
<li><p>Given a point <span class="math inline">\mathbf{x}_{i} \in \mathbb{R}^{d}</span>,</p>
<ul>
<li><p><span class="math inline">\mathbf{x}_{i}</span> is <strong>on</strong> the hyperplane if <span class="math inline">\mathbf{w} \cdot \mathbf{x}_{i} + b = 0</span>.</p></li>
<li><p><span class="math inline">\mathbf{x}_{i}</span> is <strong>above</strong> the hyperplane if <span class="math inline">\mathbf{w} \cdot \mathbf{x}_{i} + b &gt; 0</span>.</p></li>
<li><p><span class="math inline">\mathbf{x}_{i}</span> is <strong>below</strong> the hyperplane if <span class="math inline">\mathbf{w} \cdot \mathbf{x}_{i} + b &lt; 0</span>.</p></li>
</ul></li>
<li><p>The hyperplane doesn’t change if both <span class="math inline">\mathbf{w}</span> and <span class="math inline">b</span> are multiplied by the same scaling factor.</p>
<p><span class="math display"> \mathbf{w} \cdot \mathbf{x} + b = 0 \Leftrightarrow k \mathbf{w} \cdot \mathbf{x} + k b = 0 </span></p>
<p>where <span class="math inline">k</span> is an arbitrary non-zero scaling factor.</p></li>
<li><p>The distance of a point <span class="math inline">\mathbf{x}_{i}</span> to the hyperplane defined by <span class="math inline">\mathbf{w}</span> and <span class="math inline">b</span> is the perpendicular distance of the point to the hyperplane:</p>
<p><span class="math display"> d(\mathbf{x}_{i}) = \frac{\lvert \mathbf{w} \cdot \mathbf{x}_{i} + b \rvert}{\lVert \mathbf{w} \rVert} </span></p>
<p>where <span class="math inline">\lVert \mathbf{w} \rVert</span> is the <span class="math inline">L_{2}</span> norm of <span class="math inline">\mathbf{w}</span>.</p></li>
</ol>
</section>
<section id="margin" class="level3">
<h3 class="anchored" data-anchor-id="margin">Margin</h3>
<p>We define <strong>geometric margin</strong> of an instance <span class="math inline">\mathbf{x}_{i}</span> and label <span class="math inline">y_{i}</span> with respect to a hyperplane defined by <span class="math inline">\mathbf{w}</span> and <span class="math inline">b</span> to be</p>
<p><span class="math display"> \gamma_{i} = \frac{y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b )}{\lVert \mathbf{w} \rVert} </span></p>
<ul>
<li><p>Due to the definition of label to be <span class="math inline">\{ -1, 1 \}</span> instead of <span class="math inline">\{ 0, 1 \}</span>, the sign of the geometric margin indicates whether the instance is classified correctly by the hyperplane. The geometric margin of <span class="math inline">\mathbf{x}_{i}</span> is positive only if the <span class="math inline">\mathbf{x}_{i}</span> is on the correct side of the hyperplane.</p></li>
<li><p>Note the similarity between the definition of geometric margin and the definition of the distance of the point to the hyperplane. The magnitude of the geometric margin is the distance between the instance and the hyperplane.</p></li>
</ul>
</section>
</section>
<section id="hard-margin-svm-svm-without-slacks" class="level2">
<h2 class="anchored" data-anchor-id="hard-margin-svm-svm-without-slacks">Hard margin SVM (SVM without slacks)</h2>
<hr>
<section id="formulation" class="level3">
<h3 class="anchored" data-anchor-id="formulation">Formulation</h3>
<p>Given a dataset with <span class="math inline">n</span> instances <span class="math inline">\mathbf{x}_{i} \in \mathbb{R}^{d}</span> and <span class="math inline">n</span> labels <span class="math inline">y_{i} \in \{-1, 1\}</span>, we assume that there exists at least a hyperplane that can perfectly separates all training instances. That is,</p>
<ul>
<li><p>All instances with label 1 are above the hyperplane.</p></li>
<li><p>All instances with label -1 are below the hyperplane.</p></li>
</ul>
<p>In case there are more than one hyperplanes that can perfectly separates the training instances, a hard margin SVM model will choose the hyperplane that has the <strong>largest geometric margin to the training instances that are closest (minimum geometric margin) to the hyperplane</strong>.</p>
<p><span class="math display">
\begin{aligned}
\max_{\mathbf{w}, b} \quad &amp; \hat{\gamma} \\
\text{s.t. } \quad &amp; \gamma_{i} \geq \hat{\gamma}, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>where <span class="math inline">\hat{\gamma}</span> is the minimum geometric margin that the hyperplane has with respect to all the training instances.</p>
<p>We can get the following convex optimization problem by simplifying the optimization problem above:</p>
<p><span class="math display">
\begin{aligned}
\min_{\mathbf{w}} \quad &amp; \frac{1}{2} \lVert \mathbf{w} \rVert^{2} \\
\text{s.t. } \quad &amp; y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) \geq 1, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>:::{admonition} Proof: derivation of the SVM primal problem :class: dropdown</p>
<p>Expand the definition of the geometric margin in the original optimization problem:</p>
<p><span class="math display">
\begin{aligned}
\max_{\mathbf{w}, b} \quad &amp; \frac{y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b )}{\lVert \mathbf{w} \rVert} \\
\text{s.t. } \quad &amp; \frac{y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b )}{\lVert \mathbf{w} \rVert} \geq \frac{y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b )}{\lVert \mathbf{w} \rVert}, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>where <span class="math inline">\hat{\mathbf{x}}</span> represents the instance that achieves the minimum geometric margin to the hyperplane.</p>
<p>Since <span class="math inline">\lVert \mathbf{w} \rVert</span> is non-negative, we can multiply <span class="math inline">\lVert \mathbf{w} \rVert</span> on both sides of the constraint to get</p>
<p><span class="math display">
\begin{aligned}
\max_{\mathbf{w}, b} \quad &amp; \frac{y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b )}{\lVert \mathbf{w} \rVert} \\
\text{s.t. } \quad &amp; y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) \geq y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b ), \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>Since the value of <span class="math inline">\geq y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b )</span> can be arbitrarily scaled up or down by multiplying with a scaling factor, we can introduce a implicit constraint that <span class="math inline">y_{i} ( \mathbf{w} \cdot \hat{\mathbf{x}} + b ) = 1</span></p>
<p><span class="math display">
\begin{aligned}
\max_{\mathbf{w}} \quad &amp; \frac{1}{\lVert \mathbf{w} \rVert} \\
\text{s.t. } \quad &amp; y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) \geq 1, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>To help simplifying in the solving process, maximizing <span class="math inline">\frac{1}{\lVert \mathbf{w} \rVert}</span> is the same as minimizing <span class="math inline">\frac{\lVert \mathbf{w} \rVert^{2}}{2}</span>:</p>
<p><span class="math display">
\begin{aligned}
\max_{\mathbf{w}} \quad &amp; \frac{1}{2} \lVert \mathbf{w} \rVert \\
\text{s.t. } \quad &amp; y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) \geq 1, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>:::</p>
<p>Solving the above optimization problem will give us two parallel hyperplanes (<span class="math inline">\mathbf{w} x + b = 1</span> and <span class="math inline">\mathbf{w} x + b = -1</span>) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between.</p>
<ol type="1">
<li><p>The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as</p>
<p><span class="math display"> \frac{\lvert b_{2} - b_{1} \rvert}{\lVert \mathbf{w} \rVert} = \frac{\lvert (b + 1) - (b - 1) \rvert}{\lVert \mathbf{w} \rVert} = \frac{2}{\lVert \mathbf{w} \rVert} </span></p></li>
<li><p>The constraints specify that the instances must be on the correct side of the two hyperplanes:</p>
<p><span class="math display">
y_{i}(\mathbf{w} \cdot \mathbf{x}_{i} + b) \geq 1 \Leftrightarrow
\begin{cases}
\begin{aligned}
&amp; \mathbf{w} \cdot \mathbf{x}_{i} + b \geq 1 &amp;&amp; \text{if } y_{i} = 1 \\
&amp; \mathbf{w} \cdot \mathbf{x}_{i} + b \leq -1 &amp;&amp; \text{if } y_{i} = -1 \\
\end{aligned}
\end{cases}
</span></p></li>
</ol>
</section>
<section id="solving-hard-margin-svm" class="level3">
<h3 class="anchored" data-anchor-id="solving-hard-margin-svm">Solving hard margin SVM</h3>
<p>Rewrite the primal problem:</p>
<p><span class="math display">
\begin{aligned}
\min_{\mathbf{w}} \quad &amp; \frac{1}{2} \mathbf{w} \cdot \mathbf{w} \\
\text{s.t. } \quad &amp; -(y_{i}(\mathbf{w} \cdot \mathbf{x}_{i} + b) - 1) \leq 0, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>Derive the Lagrangian from the primal problem:</p>
<p><span class="math display">
\begin{aligned}
L(\mathbf{w}, b, \alpha) &amp; = f(\mathbf{w}, b) + \sum_{i=1}^{n} \alpha_{i} g_{i}(\mathbf{w}, b) \\
&amp; = \frac{1}{2} \mathbf{w} \cdot \mathbf{w} - \sum_{i=1}^{n} \alpha_{i} ( y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) - 1 ) \\
\end{aligned}
</span></p>
<p>Then we can write the dual function:</p>
<p><span class="math display">
\begin{aligned}
\mathcal{D}(\alpha) &amp; = \min_{\mathbf{w}, b} L(\mathbf{w}, b, \alpha) \\
&amp; = \min_{\mathbf{w}, b} \frac{1}{2} \mathbf{w} \cdot \mathbf{w} - \sum_{i=1}^{n} \alpha_{i} ( y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) - 1 ) \\
\end{aligned}
</span></p>
<ol type="1">
<li><p>Solving the dual function by taking the derivative of <span class="math inline">L(\mathbf{w}, b, \alpha)</span> over <span class="math inline">\mathbf{w}</span> and <span class="math inline">b</span>:</p>
<p><span class="math display">
\begin{aligned}
&amp; \frac{\partial L}{\partial \mathbf{w}} = 0 \Rightarrow \mathbf{w} - \sum_{i=1}^{n} \alpha_{i}y_{i} \mathbf{x}_{i} = 0 \Rightarrow \mathbf{w} = \sum_{i=1}^{n} \alpha_{i}y_{i}\mathbf{x}_{i} \\
&amp; \frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^{n} \alpha_{i}y_{i} = 0
\end{aligned}
</span></p></li>
<li><p>Plug in <span class="math inline">\mathbf{w} = \sum_{i=1}^{n} \alpha_{i}y_{i}\mathbf{x}_{i}</span> back and simplify <span class="math inline">\mathcal{D}(\alpha)</span>:</p>
<p><span class="math display">
\begin{aligned}
\mathcal{D}(\alpha)
&amp; = b \sum_{i=1}^{n} \alpha_{i} y_{i} + \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} (\mathbf{x}_{i} \cdot \mathbf{x}_{j}) \\
&amp; = \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
\quad &amp; [\text{since } \sum_{i=1}^{n} \alpha_{i}y_{i} = 0] \\
\end{aligned}
</span></p></li>
</ol>
<p>:::{admonition} Proof: simplify <span class="math inline">\mathcal{D}(\alpha)</span> for hard margin SVM :class: dropdown</p>
<p><span class="math display">
\begin{aligned}
\mathcal{D}(\alpha)
&amp; = \min_{\mathbf{w}, b}
    \frac{1}{2} \mathbf{w} \cdot \mathbf{w}
    - \sum_{i=1}^{n} \alpha_{i} ( y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) - 1 ) \\
&amp; = \frac{1}{2} \left( \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} \right) \cdot \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right)
    - \sum_{i=1}^{n} \alpha_{i} \left( y_{i} \left( \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i} + b \right) - 1 \right) \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    - \sum_{i=1}^{n} \alpha_{i} y_{i} \left( \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i} + b \right)
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    - \sum_{i=1}^{n} \alpha_{i} y_{i} \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i}
    + b\sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    - \left( \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} \right) \cdot \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right)
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    - \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = b \sum_{i=1}^{n} \alpha_{i} y_{i} + \sum_{i=1}^{n} \alpha_{i}
    - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} ) \\
\end{aligned}
</span></p>
<p>:::</p>
<p>The Lagrange dual problem is written as:</p>
<p><span class="math display">
\begin{aligned}
\max_{\alpha} \quad &amp; \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} ) \\
\text{s.t. } \quad &amp; \alpha_{i} \geq 0, \quad i = 1, \dots, n \\
&amp; \sum_{i=1}^{n} \alpha_{i} y_{i} = 0 \\
\end{aligned}
</span></p>
<p>Since the primal problem is a convex function and there definitely exists at least one solution to the primal problem, Slater’s condition proves that strong duality holds.</p>
<p>Since solving the dual problem is the same as solving the primal problem, the benefits of solving the dual problem are:</p>
<ol type="1">
<li><p>The Lagrange dual problem only involves <span class="math inline">\alpha_{i}</span> and most of them are 0, but primal problem has <span class="math inline">\mathbf{w}</span> and <span class="math inline">b</span>, which are much more parameters.</p></li>
<li><p>The Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.</p></li>
</ol>
</section>
</section>
<section id="soft-margin-svm-svm-with-slacks" class="level2">
<h2 class="anchored" data-anchor-id="soft-margin-svm-svm-with-slacks">## Soft margin SVM (SVM with slacks)</h2>
<section id="formulation-1" class="level3">
<h3 class="anchored" data-anchor-id="formulation-1">Formulation</h3>
<p>In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances.</p>
<p><span class="math display">
\begin{aligned}
\min_{\mathbf{w}} \quad &amp; \frac{1}{2} \lVert \mathbf{w} \rVert^{2} + C \sum_{i=1}^{n} \xi_{i} \\
\text{s.t. } \quad &amp; y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) \geq 1 - \xi_{i}, \quad i = 1, \dots, n \\
\quad &amp; \xi_{i} \geq 0, \quad i = 1, \dots, n \\
\end{aligned}
</span></p>
<p>where <span class="math inline">\xi_{i}</span> is the slack variable for the instance <span class="math inline">\mathbf{x}_{i}</span> and <span class="math inline">C</span> is a hyperparameter that penalizes the misclassification of <span class="math inline">\mathbf{x}_{i}</span>.</p>
<ol type="1">
<li>If <span class="math inline">\xi_{i}</span> is nonzero for <span class="math inline">\mathbf{x}_{i}</span>, it means that <span class="math inline">\mathbf{x}_{i}</span> is on the misclassified side of <span class="math inline">\mathbf{w} \cdot \mathbf{x}_{i} + b = 1</span> (or <span class="math inline">\mathbf{w} \cdot \mathbf{x}_{i} + b = -1</span>) and the distance is <span class="math inline">\xi_{i}</span>.</li>
<li>If <span class="math inline">C = 0</span>, <span class="math inline">\xi_{i}</span> can be arbitrary large for each <span class="math inline">\mathbf{x}_{i}</span>. If <span class="math inline">C \to \inf</span>, it is the same as hard margin SVM because any misclassification can induce infinite loss.</li>
</ol>
</section>
<section id="solving-soft-margin-svm" class="level3">
<h3 class="anchored" data-anchor-id="solving-soft-margin-svm">Solving soft margin SVM</h3>
<p>Similar as hard margin SVM, we can write Lagrangian dual function as:</p>
<p><span class="math display">
\begin{aligned}
\mathcal{D}(\alpha, \beta) &amp; = \min_{\mathbf{w}, b} \frac{1}{2} \mathbf{w} \cdot \mathbf{w}
    - \sum_{i=1}^{n} \alpha_{i} \left( y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) - 1 + \xi_{i} \right) - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
\end{aligned}
</span></p>
<p>Again, we can solve Lagrangian dual function by taking the derivatives over the <span class="math inline">\mathbf{w}</span>, <span class="math inline">b</span>, and <span class="math inline">\xi_i</span>:</p>
<p><span class="math display">
\begin{aligned}
\frac{\partial L}{\partial \mathbf{w}} = 0 &amp; \Rightarrow \mathbf{w} - \sum_{i=1}{n} \alpha_{i} y_{i} \mathbf{x}_{i} = 0 \Rightarrow \mathbf{w} = \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} \\
\frac{\partial L}{\partial b} = 0 &amp; \Rightarrow \sum_{i=1}^{n} \alpha_{i} y_{i} = 0 \\
\frac{\partial L}{\partial \xi_{i}} = 0 &amp; \Rightarrow C - \alpha_{i} - \beta_{i} = 0 \Rightarrow C = \alpha_{i} + \beta_{i} \\
\end{aligned}
</span></p>
<p>Plug the <span class="math inline">\mathbf{w} = \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i}</span> and <span class="math inline">C = \alpha_{i} + \beta_{i}</span> back and simplify <span class="math inline">\mathcal{D}(\alpha, \beta)</span>.</p>
<p><span class="math display"> \mathcal{D}(\alpha, \beta) = \sum_{i=1}^{n} \alpha_{i}  - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j}(\mathbf{x}_{i}\mathbf{x}_{j}) </span></p>
<p>which has exactly the same form as Lagrangian dual function of hard margin SVM.</p>
<p>:::{admonition} Proof: simplify <span class="math inline">\mathcal{D}(\alpha)</span> for soft margin SVM :class: dropdown</p>
<p><span class="math display">
\begin{aligned}
\mathcal{D}(\alpha, \beta)
&amp; = \min_{\mathbf{w}, b}
    \frac{1}{2} \mathbf{\mathbf{w}} \mathbf{\mathbf{w}}
    + C \sum_{i=1}^{n} \xi_{i} - \sum_{i=1}^{n} \alpha_{i} \left( y_{i} ( \mathbf{w} \cdot \mathbf{x}_{i} + b ) - 1 + \xi_{i} \right)
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
&amp; = \frac{1}{2} \left( \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} \right) \cdot \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right)
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i} ) \xi_{i}
    - \sum_{i=1}^{n} \alpha_{i} \left( y_{i} \left( \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i} + b \right) - 1 + \xi_{i} \right)
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i} ) \xi_{i}
    - \sum_{i=1}^{n} \alpha_{i} y_{i} \left( \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i} + b \right)
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i}) \xi_{i}
    - \sum_{i=1}^{n} \alpha_{i} y_{i} \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right) \cdot \mathbf{x}_{i}
    + b \sum_{i=1}^{n} \alpha_{i}y_{i}
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i} ) \xi_{i}
    - \left( \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} \right) \cdot \left( \sum_{j=1}^{n} \alpha_{j} y_{j} \mathbf{x}_{j} \right)
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\
&amp; = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i} ) \xi_{i}
    - \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + b \sum_{i=1}^{n} \alpha_{i}y_{i}
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\    
&amp; = - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i}\mathbf{x}_{j} )
    + \sum_{i=1}^{n} ( \alpha_{i} + \beta_{i} ) \xi_{i}
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\  
&amp; = - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    + \sum_{i=1}^{n} \beta_{i} \xi_{i}
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i}
    - \sum_{i=1}^{n} \alpha_{i} \xi_{i}
    - \sum_{i=1}^{n} \beta_{i} \xi_{i} \\  
&amp; = - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} )
    + b \sum_{i=1}^{n} \alpha_{i} y_{i}
    + \sum_{i=1}^{n} \alpha_{i} \\
&amp; = \sum_{i=1}^{n}\alpha_{i}
    - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} ( \mathbf{x}_{i} \cdot \mathbf{x}_{j} ) \\
\end{aligned}
</span></p>
<p>:::</p>
<p>The Lagrange dual problem is written as:</p>
<p><span class="math display">
\begin{aligned}
\max_{\alpha} \quad &amp; \sum_{i=1}^{n}\alpha_{i} - \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(\mathbf{x}_{i}\mathbf{x}_{j}) \\
\text{s.t. } \quad &amp; \alpha_{i} \geq 0, \quad i = 1, \dots, n \\
&amp; \beta_{i} \geq 0, \quad i = 1, \dots, n \\
&amp; \sum_{i=1}^{n} \alpha_{i} y_{i} = 0 \\
\end{aligned}
</span></p>
<p>Since we know <span class="math inline">C = \alpha_{i} + \beta_{i} \Rightarrow \alpha_{i} = C - \beta_{i}</span>, the constraint <span class="math inline">\beta_{i} \geq 0</span> can be removed by merging into <span class="math inline">\alpha_{i} \geq 0</span>:</p>
<p><span class="math display">
\begin{aligned}
\max_{\alpha} \quad &amp; \sum_{i=1}^{n}\alpha_{i} - \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(\mathbf{x}_{i}\mathbf{x}_{j}) \\
\text{s.t. } \quad &amp; C \geq \alpha_{i} \geq 0, \quad i = 1, \dots, n \\
&amp; \sum_{i=1}^{n} \alpha_{i} y_{i} = 0 \\
\end{aligned}
</span></p>
<p>The only difference with Lagrange dual problem of hard margin SVM is the addition of <span class="math inline">C \geq \alpha_{i}, \quad i = 1, \dots, n</span>.</p>
</section>
</section>
<section id="inference-of-svm" class="level2">
<h2 class="anchored" data-anchor-id="inference-of-svm">Inference of SVM</h2>
<hr>
<p>Solving the SVM dual problem will generate an optimal Lagrange multiplier <span class="math inline">\alpha_{i}^{*}</span> for each training instance <span class="math inline">\mathbf{x}_{i}</span>. All <span class="math inline">\alpha_{i}^{*}</span>’s, <span class="math inline">\mathbf{x}_{i}</span>’s and <span class="math inline">y_{i}</span>’s can be used to calculate the optimal <span class="math inline">\mathbf{w}^{*}</span> and <span class="math inline">b^{*}</span> that define the hyperplane as the classifier.</p>
<section id="support-vectors" class="level3">
<h3 class="anchored" data-anchor-id="support-vectors">Support vectors</h3>
<p><strong>Support vectors</strong> are the training instances whose optimal Lagrange multipliers <span class="math inline">\alpha_{i}^{*}</span>’s are positive.</p>
<p>For hard margin SVM:</p>
<ul>
<li><p>All support vectors are the training instances that are the closest to the decision hyperplane <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = 0</span>.</p>
<ul>
<li><p>All positive support vectors are on the hyperplane <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = 1</span>.</p></li>
<li><p>All negative support vectors are on the hyperplane <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = -1</span>.</p></li>
</ul></li>
<li><p>While there are some training instances on the hyperplanes <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = \pm 1</span> have <span class="math inline">\alpha_{i}^{*} = 0</span>, <strong>the training instances that are NOT on the hyperplanes must have <span class="math inline">\alpha_{i}^{*} = 0</span></strong>.</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &gt; 1 \Rightarrow \alpha_{i}^{*} = 0 </span></p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1 \Rightarrow \alpha_{i}^{*} \geq 0 </span></p>
<p>Note that there won’t be any training instances that have <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &lt; 1</span> because of the constraints specified in the primal problem.</p></li>
</ul>
<p>:::{admonition} Proof: the values of <span class="math inline">\alpha_{i}^{*}</span>’s for the support vectors in the hard margin SVM :class: dropdown</p>
<p>Strong duality implies that the optimal solutions <span class="math inline">\alpha^{*}</span>, <span class="math inline">\mathbf{w}^{*}</span> and <span class="math inline">b^{*}</span> meet the KKT conditions. The complementary slackness condition states that</p>
<p><span class="math display"> \alpha_{i} g_{i}(x) = 0 \Rightarrow - \alpha_{i} ( y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) - 1 ) = 0, \quad i = 1, \dots, n </span></p>
<ul>
<li><p><span class="math inline">\alpha_{i}^{*}</span> must be 0 if <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &gt; 1</span></p></li>
<li><p><span class="math inline">\alpha_{i}^{*}</span> can be non-zero if <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1</span></p></li>
</ul>
<p>:::</p>
<p>For soft margin SVM,</p>
<ul>
<li><p>All support vectors satisfy the constraint:</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1 - \xi_{i}, \quad i \in \mathcal{S} </span></p>
<p>where <span class="math inline">\mathcal{S} = \{ i \mid \alpha_{i}^{*} &gt; 0 \}</span>.</p>
<ul>
<li><p>All positive support vectors are <strong>either on or below</strong> the hyperplane <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = 1</span>.</p></li>
<li><p>All negative support vectors are <strong>either on or above</strong> the hyperplane <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = -1</span>.</p></li>
</ul></li>
<li><p>Again the training instances on the hyperplanes <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = \pm 1</span> can have <span class="math inline">0 \leq \alpha_{i}^{*} \leq C</span>, but the training instances that are NOT on the hyperplanes must have <span class="math inline">\alpha_{i}^{*} = 0</span> or <span class="math inline">\alpha_{i}^{*} = C</span>.</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1 \Rightarrow 0 \leq \alpha_{i}^{*} \leq 0 </span></p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &gt; 1 \Rightarrow \alpha_{i}^{*} = 0 </span></p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &lt; 1 \Rightarrow \alpha_{i}^{*} = C </span>,</p></li>
</ul>
<p>:::{admonition} Proof: the values of <span class="math inline">\alpha_{i}^{*}</span>’s for the support vectors in the soft margin SVM :class: dropdown</p>
<p>Strong duality implies that the optimal solutions <span class="math inline">\alpha^{*}</span>, <span class="math inline">\mathbf{w}^{*}</span> and <span class="math inline">b^{*}</span> meet the KKT conditions. The complementary slackness condition states that</p>
<p><span class="math display"> - \alpha_{i}^{*} ( y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) - 1 + \xi_{i} ) = 0, \quad i = 1, \dots, n </span></p>
<p><span class="math display"> - \beta_{i} \xi_{i} = 0, \quad i = 1, \dots, n </span></p>
<ul>
<li><p>If <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1</span>, <span class="math inline">\alpha_{i}^{*}</span> is a non-negative number between <span class="math inline">0</span> and <span class="math inline">C</span>.</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) = 1 \Rightarrow 0 \leq \alpha_{i}^{*} \leq 0 </span></p></li>
<li><p>If <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &gt; 1</span>, <span class="math inline">\alpha_{i}^{*}</span> must be <span class="math inline">0</span>,</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &gt; 1 \Rightarrow \alpha_{i}^{*} = 0 </span></p>
<p>which follows by the equation 1 and the constraint <span class="math inline">\xi_{i} \geq 0</span> in the primal problem.</p></li>
<li><p>If <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &lt; 1</span>, <span class="math inline">\alpha_{i}^{*}</span> must be <span class="math inline">C</span>,</p>
<p><span class="math display"> y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) &lt; 1 \Rightarrow \alpha_{i}^{*} = C </span>,</p>
<p>which is explained as follows:</p>
<ol type="1">
<li>Since the constraint <span class="math inline">y_{i} ( \mathbf{w}^{*} \cdot \mathbf{x}_{i} + b^{*} ) - 1 + \xi_{i} \geq 0</span> in the primal problem, <span class="math inline">\xi_{i} &gt; 0</span>.</li>
<li>Since the <span class="math inline">\beta_{i} \xi_{i} = 0</span> from the equation 2, <span class="math inline">\beta_{i} = 0</span>.</li>
<li>Since we know that <span class="math inline">\alpha_{i}^{*} + \beta_{i} = C</span>, <span class="math inline">\alpha_{i}^{*} = C</span>.</li>
</ol></li>
</ul>
<p>:::</p>
</section>
<section id="calculate-mathbfw-and-b-using-alpha_is" class="level3">
<h3 class="anchored" data-anchor-id="calculate-mathbfw-and-b-using-alpha_is">Calculate <span class="math inline">\mathbf{w}^{*}</span> and <span class="math inline">b^{*}</span> using <span class="math inline">\alpha_{i}^{*}</span>’s</h3>
<p>In solving both hard and soft margin SVM, we have</p>
<p><span class="math display"> \mathbf{w} = \sum_{i=1}^{n} \alpha_{i} y_{i} \mathbf{x}_{i} </span></p>
<p>Since only <span class="math inline">\alpha_{i}^{*}</span>’s for the support vectors are non-zero, we can derive <span class="math inline">\mathbf{w}^{*}</span> by:</p>
<p><span class="math display"> \mathbf{w}^{*} = \sum_{i \in \mathcal{S}} \alpha_{i}^{*} y_{i} \mathbf{x}_{i} </span></p>
<p>where <span class="math inline">\mathcal{S} = \{ i \mid \alpha_{i}^{*} &gt; 0 \}</span>.</p>
<p>To calculate <span class="math inline">b^{*}</span>, we can select any training instance <span class="math inline">\hat{\mathbf{x}}</span> with label <span class="math inline">\hat{y}</span> that is on the hyperplanes <span class="math inline">\mathbf{w}^{*} \cdot \mathbf{x} + b^{*} = \pm 1</span>. Thus, we have</p>
<p><span class="math display">
\begin{aligned}
\hat{y} ( \mathbf{w}^{*} \cdot \hat{\mathbf{x}} + b^{*} ) &amp; = 1 \\
\hat{y}^{2} ( \mathbf{w}^{*} \cdot \hat{\mathbf{x}} + b^{*} ) &amp; = \hat{y} &amp; [\text{multiply both sides by } \hat{y}] \\
\mathbf{w}^{*} \cdot \hat{\mathbf{x}} + b^{*} &amp; = \hat{y} &amp; [\hat{y}^{2} = 1] \\
b^{*} &amp; = \hat{y} - \mathbf{w}^{*} \cdot \hat{\mathbf{x}}  \\
\end{aligned}
</span></p>
<p>Assuming <span class="math inline">b^{*}</span> has been calculated, we can express the inference of a SVM model as follows:</p>
<p><span class="math display">
\begin{aligned}
\operatorname{svm}(\mathbf{x}) &amp; = \mathbf{w}^{*} \cdot \mathbf{x} + b^{*} \\
&amp; = \left( \sum_{i \in \mathcal{S}} \alpha_{i} y_{i} \mathbf{x}_{i} \right) \cdot \mathbf{x} + b^{*} \\
&amp; = \sum_{i \in \mathcal{S}} \alpha_{i}^{*} y_{i} \mathbf{x}_{i} \cdot \mathbf{x} + b^{*} \\
\end{aligned}
</span></p>
</section>
</section>
<section id="kernel-trick" class="level2">
<h2 class="anchored" data-anchor-id="kernel-trick">## Kernel trick</h2>
<p>Kernel trick is a method that can efficiently make the learning algorithm (e.g.&nbsp;SVM, KNN) <strong>learn in a higher dimensional space</strong> without explicitly transforming the training instances to the higher dimensional space.</p>
<section id="why-higher-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="why-higher-dimensions">Why higher dimensions</h3>
<p>Take SVM for an example. Most of the time, the training set cannot be perfectly separated using a hyperplane, but can be perfectly separated if the feature space of the training set is mapped to a higher dimensional space.</p>
<p>In the following example, the training set <span class="math inline">\mathbf{X} \in \mathbb{R}</span> contains 3 training instances that are all scalars (each instance only has 1 feature).</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>f1</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math display">x_{1}</span></td>
<td>-2</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math display">x_{2}</span></td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td><span class="math display">x_{3}</span></td>
<td>2</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Clearly the positive instance <span class="math inline">x_{2}</span> cannot be linearly separated from the negative instances <span class="math inline">x_{1}</span> and <span class="math inline">x_{3}</span>. However, we can use a <strong>feature mapping function</strong> <span class="math inline">\Phi</span> to map the training instances from scalars to vectors of length 2.</p>
<p><span class="math display">
\Phi(x) =
\begin{bmatrix}
x &amp; x^{2}
\end{bmatrix}
</span></p>
<p>After applying the mapping function, we can get the transformed dataset in the 2-dimensional instead of the 1-dimensional space.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>f1</th>
<th>f2</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math display">\Phi(x_{1})</span></td>
<td>-2</td>
<td>4</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math display">\Phi(x_{2})</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td><span class="math display">\Phi(x_{3})</span></td>
<td>2</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>We can easily find a hyperplane (e.g.&nbsp;<span class="math inline">y = 2</span>) to separate the positive instances from the negative instances in the transformed high-dimensional space.</p>
<div class="cell" data-jupyter="{&quot;source_hidden&quot;:true}" data-tags="[&quot;remove-input&quot;]" data-execution_count="71">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">0</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.zeros(x0.shape[<span class="dv">0</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y0_indices <span class="op">=</span> np.array([<span class="dv">1</span>])</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y1_indices <span class="op">=</span> np.delete(np.arange(x1.shape[<span class="dv">0</span>]), y0_indices)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>[<span class="dv">16</span>, <span class="dv">4</span>], sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ax1.scatter(x0[y0_indices], x1[y0_indices], marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>ax1.scatter(x0[y1_indices], x1[y1_indices], marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> x0 <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax2.scatter(x0[y0_indices], x1[y0_indices], marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>ax2.scatter(x0[y1_indices], x1[y1_indices], marker<span class="op">=</span><span class="st">'x'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>&lt;matplotlib.collections.PathCollection at 0x7f800b9cd9a0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Support Vector Machine (SVM)_files/figure-html/cell-2-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="kernel-function" class="level3">
<h3 class="anchored" data-anchor-id="kernel-function">Kernel function</h3>
<p>If we have defined a feature mapping function <span class="math inline">\Phi(\mathbf{x})</span>, we can apply it to SVM (and other machine learning models) by replacing every <span class="math inline">\mathbf{x}</span> by <span class="math inline">\Phi(\mathbf{x})</span> in all equations. Then we can find that in all equations of SVM (hard margin SVM, soft margin SVM and SVM inference), all transformed training instances <span class="math inline">\Phi(\mathbf{x}_{i})</span> are involved in the inner product (dot product) with another transformed <span class="math inline">\Phi(\mathbf{x}_{j})</span></p>
<p>Furthermore, given a feature mapping function <span class="math inline">\Phi</span>, we define the corresponding <strong>Kernel function</strong> to be</p>
<p><span class="math display"> K(\mathbf{x}_{1}, \mathbf{x}_{2}) = \Phi(\mathbf{x}_{1}) \cdot \Phi(\mathbf{x}_{2}) </span></p>
<p>Taking the SVM inference equation for an example,</p>
<p><span class="math display">
\begin{aligned}
\operatorname{svm}(\mathbf{x}) &amp; = \sum_{i \in \mathcal{S}} \alpha_{i}^{*} y_{i} \mathbf{x}_{i} \cdot \mathbf{x} + b^{*} \\
&amp; = \sum_{i \in \mathcal{S}} \alpha_{i}^{*} y_{i} \Phi(\mathbf{x}_{i}) \cdot \Phi(\mathbf{x}) + b^{*} \\
&amp; = \sum_{i \in \mathcal{S}} \alpha_{i}^{*} y_{i} K(\mathbf{x}_{i}, \mathbf{x}_{j}) + b^{*} \\
\end{aligned}
</span></p>
<p>we can see that we don’t necessarily need to compute <span class="math inline">\Phi(\mathbf{x})</span> if we have a way to compute <span class="math inline">K(\mathbf{x}_{i}, \mathbf{x}_{j})</span> directly. - The assumption here is that computing <span class="math inline">\Phi(\mathbf{x})</span> can be quite expensive if the output is an extremely high dimensional vector.</p>
</section>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<hr>
<ol type="1">
<li>https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture7.pdf</li>
<li>https://www.stat.cmu.edu/~ryantibs/convexopt-F16/scribes/kkt-scribed.pdf</li>
<li>https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/</li>
<li>https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf</li>
<li>https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf</li>
<li>https://www-ai.cs.tu-dortmund.de/LEHRE/SEMINARE/SS09/AKTARBEITENDESDM/LITERATUR/PlattSMO.pdf</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>