<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>joeyonng-notebook - K-means</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Joeyonng's Notebook
      </li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Joeyonng’s Notebook</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Learning Theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/1_Statistical_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/2_Effective_Class_Size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Effective Class Size</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/3_PAC_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PAC Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/4_Agnostic_Learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agnostic Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Learning Theory/5_Rademacher_Complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Rademacher Complexity</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Statistical Learning/1_Bayesian_Decision_Theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Decision Theory (BDT)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Statistical Learning/2_Maximum_Likelihood_Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Maximum Likelihood Estimation (MLE)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Statistical Learning/3_Bayesian_Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Statistical Learning/4_Expectation_Maximization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Expectation-maximization</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ML Q&amp;A.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Q &amp; A</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary">Preliminary</a>
  <ul class="collapse">
  <li><a href="#statistics" id="toc-statistics" class="nav-link" data-scroll-target="#statistics">Statistics</a></li>
  </ul></li>
  <li><a href="#problem-formulation" id="toc-problem-formulation" class="nav-link" data-scroll-target="#problem-formulation">## Problem formulation</a></li>
  <li><a href="#k-means-using-lloyds-algorithm" id="toc-k-means-using-lloyds-algorithm" class="nav-link" data-scroll-target="#k-means-using-lloyds-algorithm">K-means using Lloyd’s algorithm</a>
  <ul class="collapse">
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm">Algorithm</a></li>
  <li><a href="#convergence-of-lloyds-algorithm" id="toc-convergence-of-lloyds-algorithm" class="nav-link" data-scroll-target="#convergence-of-lloyds-algorithm">Convergence of Lloyd’s algorithm</a></li>
  </ul></li>
  <li><a href="#the-k-means-initializer" id="toc-the-k-means-initializer" class="nav-link" data-scroll-target="#the-k-means-initializer">## The K-means++ initializer</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">K-means</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><em>Updated 01-12-2023 (First commited 02-27-2022)</em></p>
<hr>
<ol type="1">
<li>K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed.</li>
<li>We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change.</li>
<li>K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.</li>
</ol>
<section id="preliminary" class="level2">
<h2 class="anchored" data-anchor-id="preliminary">Preliminary</h2>
<hr>
<section id="statistics" class="level3">
<h3 class="anchored" data-anchor-id="statistics">Statistics</h3>
<section id="estimator" class="level4">
<h4 class="anchored" data-anchor-id="estimator">Estimator</h4>
<p>In statistics, an estimator is a <strong>function that takes as inputs a set of observations</strong> sampled from an unknown probability distribution <span class="math inline">P_{\theta}(X)</span> with the true parameter <span class="math inline">\theta</span> and <strong>outputs the best guess of the parameter</strong> <span class="math inline">\hat{\theta}</span> of <span class="math inline">P_{\theta}(X)</span>.</p>
<ul>
<li>An estimator is also a random variable.</li>
</ul>
</section>
<section id="bias" class="level4">
<h4 class="anchored" data-anchor-id="bias">Bias</h4>
<p>The bias of an estimator measures <strong>whether the expectation of the estimator is the same as the true parameter</strong>. Let <span class="math inline">\hat{\theta}</span> be the output of an estimator for <span class="math inline">\theta</span>. The bias of <span class="math inline">\hat{\theta}</span> as an estimator for <span class="math inline">\theta</span> is</p>
<p><span class="math display"> \operatorname{Bias}(\hat{\theta}, \theta) = \mathbb{E}[\hat{\theta}] - \theta </span></p>
<ul>
<li><span class="math inline">\operatorname{Bias}(\hat{\theta}, \theta) = 0</span>, then we say <span class="math inline">\hat{\theta}</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\theta</span>.</li>
<li><span class="math inline">\operatorname{Bias}(\hat{\theta}, \theta) &gt; 0</span>, <span class="math inline">\hat{\theta}</span> typically <strong>overestimates</strong> <span class="math inline">\theta</span>.</li>
<li><span class="math inline">\operatorname{Bias}(\hat{\theta}, \theta) &lt; 0</span>, <span class="math inline">\hat{\theta}</span> typically <strong>underestimates</strong> <span class="math inline">\theta</span>.</li>
</ul>
</section>
<section id="variance-of-an-estimator" class="level4">
<h4 class="anchored" data-anchor-id="variance-of-an-estimator">Variance (of an estimator)</h4>
<p>The variance of an estimator measures the degree to which the <strong>estimated parameters vary depending on the sampled observations</strong>. Let <span class="math inline">\hat{\theta}</span> be the output of an estimator for <span class="math inline">\theta</span>. The variance of <span class="math inline">\hat{\theta}</span> as an estimator is</p>
<p><span class="math display"> \operatorname{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^{2}] </span></p>
</section>
<section id="mean-squared-error" class="level4">
<h4 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h4>
<p>In statistics, the mean squared error (MSE) is a risk (loss) function measures the <strong>difference between an estimator <span class="math inline">\hat{\theta}</span> with the true parameter <span class="math inline">\theta</span></strong>.</p>
<p><span class="math display"> \operatorname{MSE}(\hat{\theta}, \theta) = \mathbb{E}[(\hat{\theta} - \theta)^{2}] </span></p>
<p>If the estimator <span class="math inline">\hat{\boldsymbol{\theta}}</span> and the true parameter <span class="math inline">\boldsymbol{\theta}</span> are vectors,</p>
<p><span class="math display"> \operatorname{MSE}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \mathbb{E}[\lVert \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} \rVert^{2}] </span></p>
</section>
<section id="bias-variance-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="bias-variance-decomposition">Bias-variance decomposition</h4>
<p>A risk (loss) function can be often decomposed into a <strong>bias, a variance and a noise term</strong>. Here we take MSE as an example and expand it into a bias and variance term (noise is omitted).</p>
<p><span class="math display"> \operatorname{MSE}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \operatorname{Var}(\hat{\boldsymbol{\theta}}) + \operatorname{Bias}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta})^{2} </span></p>
<p>:::{admonition} Proof :class: dropdown</p>
<p><span class="math display">
\begin{align}
\operatorname{MSE}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) &amp; = \mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} \rVert^{2} ] \\
&amp; = \mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] + \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert^{2}] &amp; [\text{add and subtract } \mathbb{E}[\hat{\boldsymbol{\theta}}] ] \\
&amp; = \mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert^{2} + 2 \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert + \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert^{2} ] &amp; [(a + b)^{2} = a^{2} + 2ab + b^{2}] \\
&amp; = \mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert^{2} ] + \mathbb{E}[ 2 \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert ] + \mathbb{E}[ \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert^{2} ] &amp; [\text{Linearity of Expectation}] \\
&amp; = \operatorname{Var}(\hat{\boldsymbol{\theta}}) + 2\mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert] \mathbb{E}[ \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta} \rVert ] + \mathbb{E}[ \operatorname{Bias}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta})^{2} ] \\
&amp; = \operatorname{Var}(\hat{\boldsymbol{\theta}}) + \operatorname{Bias}(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta})^{2} &amp; [\mathbb{E}[ \lVert \hat{\boldsymbol{\theta}} - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert] = \lVert \mathbb{E}[\hat{\boldsymbol{\theta}}] - \mathbb{E}[\hat{\boldsymbol{\theta}}] \rVert = 0] \\
\end{align}
</span></p>
<p>:::</p>
</section>
</section>
</section>
<section id="problem-formulation" class="level2">
<h2 class="anchored" data-anchor-id="problem-formulation">## Problem formulation</h2>
<ul>
<li><p>K-means clustering algorithm is a unsupervised learning algorithm that aims to <strong>cluster similar instances into the same group</strong>.</p></li>
<li><p>The input to the algorithm is <span class="math inline">n</span> instances <span class="math inline">\mathbf{x}_{1}, \mathbf{x}_{2}, \dots, \mathbf{x}_{n} \in \mathbb{R}^{d}</span> <strong>without labels</strong> and the output of the algorithm is <span class="math inline">k</span> centroids <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{k} \in \mathbb{R}^{d}</span>, where <span class="math inline">k</span> is a hyperparameter used to control the number of centroids desired.</p></li>
<li><p>The goal of K-means clustering is to find the best <span class="math inline">k</span> centroids that minimizes a loss function (often <strong>MSE loss</strong> of the <strong><span class="math inline">L_{2}</span> norm or euclidean distance</strong> of the difference vector) that captures the overall distances between the instances and the centroids assigned.</p>
<p><span class="math display"> \operatorname{loss}(\mathbf{u}_{1}, \dots, \mathbf{u}_{k}) = \sum_{i = 1}^{n} \lVert \mathbf{x}_{i} - \mathbf{u}_{x_{i}} \rVert^{2} </span></p>
<p>where <span class="math inline">\mathbf{u}_{x_{i}}</span> is the centroid that instance <span class="math inline">\mathbf{x}_{i}</span> is assigned to.</p></li>
<li><p>Another way to write the loss function is to consider the distances between each centroid and the instances in the cluster that the centroid represents. The benefit of this formulation is that the clusters that the centroids represent are separated as variables, which is easier to do algorithm analyzing.</p>
<p><span class="math display"> \operatorname{loss}(\mathbf{u}_{1}, \dots, \mathbf{u}_{k}, C_{\mathbf{u}_{1}}, \dots, C_{\mathbf{u}_{k}}) = \sum_{i = 1}^{k} \sum_{\mathbf{x} \in C_{\mathbf{u}_{i}}} \lVert \mathbf{x} - \mathbf{u}_{i} \rVert^{2} </span></p>
<p>where <span class="math inline">C_{\mathbf{u}_{i}}</span> is the cluster that centroid <span class="math inline">\mathbf{u}_{i}</span> represents.</p></li>
</ul>
</section>
<section id="k-means-using-lloyds-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="k-means-using-lloyds-algorithm">K-means using Lloyd’s algorithm</h2>
<hr>
<section id="algorithm" class="level3">
<h3 class="anchored" data-anchor-id="algorithm">Algorithm</h3>
<blockquote class="blockquote">
<p><strong>Function</strong>: K-means<br>
<strong>Input</strong>: a set of instances <span class="math inline">\mathbf{x}_{1}, \mathbf{x}_{2}, \dots, \mathbf{x}_{n}</span> and a hyperparameter <span class="math inline">k</span>.<br>
<strong>Output</strong>: a set of centroids <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{k}</span>. 1. Initialize cluster centroids <span class="math inline">\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_{k}</span> by randomly drawing <span class="math inline">k</span> instances as the centroids. 2. Repeat until convergence or a fixed number of iterations: 1. <strong>Assignment step</strong>: get the nearest centroid <span class="math inline">\mathbf{c}_{i}</span> for each instance <span class="math inline">\mathbf{x}_{i}</span>:</p>
<pre><code>    $$ \mathbf{c}_{i} = \arg \min_{j} \lVert \mathbf{x}_{i} - \mathbf{u}_{j} \rVert^{2} $$

2. **Refitting step**: update each centroid based on the instances in its cluster:

    $$ \mathbf{u}_{j} = \frac{\sum_{i}^{m} \mathbb{1}_{\mathbf{c}_i = j} \mathbf{x}_{i}}{\sum_{i}^{m} \mathbb{1}_{\mathbf{c}_{i} = j}} $$</code></pre>
</blockquote>
</section>
<section id="convergence-of-lloyds-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="convergence-of-lloyds-algorithm">Convergence of Lloyd’s algorithm</h3>
<p>K-means solved using Lloyd’s algorithm is guaranteed to converge to a local minimum because: - The loss value is guaranteed to be smaller or stay the same in the assignment step because each instance <span class="math inline">\mathbf{x}_{i}</span> gets the nearest centroid.</p>
<pre><code>$$ \operatorname{loss}( \mathbf{u}_{1}, \dots, \mathbf{u}_{k}, (C_{\mathbf{u}_{1}}, \dots, C_{\mathbf{u}_{k}})^{t + 1} ) \leq \operatorname{loss}( \mathbf{u}_{1}, \dots, \mathbf{u}_{k}, (C_{\mathbf{u}_{1}}, \dots, C_{\mathbf{u}_{k}})^{t} ) $$
    </code></pre>
<ul>
<li><p>The loss value is guaranteed to be smaller or stay the same in the refitting step.</p>
<p><span class="math display"> \operatorname{loss}( (\mathbf{u}_{1}, \dots, \mathbf{u}_{k})^{t + 1}, (C_{\mathbf{u}_{1}}, \dots, C_{\mathbf{u}_{k}})^{t + 1} ) \leq \operatorname{loss}( (\mathbf{u}_{1}, \dots, \mathbf{u}_{k})^{t}, (C_{\mathbf{u}_{1}}, \dots, C_{\mathbf{u}_{k}})^{t + 1} ) </span></p>
<p>To see why this is true, consider a single centroid-cluster pair <span class="math inline">\mathbf{u}</span> and <span class="math inline">C_{\mathbf{u}}</span>, for all instances <span class="math inline">\mathbf{x}_{1}, \mathbf{x}_{2}, \dots, \mathbf{x}_{n}</span> that belongs to the cluster <span class="math inline">C_{\mathbf{u}}</span>, the loss function <span class="math inline">\operatorname{loss}(\mathbf{u}, C_{\mathbf{u}})</span> will be minimized when <span class="math inline">\mathbf{u}</span> is the average of the instances in <span class="math inline">C_{\mathbf{u}}</span>:</p>
<p><span class="math display"> \boldsymbol{\mu} = \frac{1}{n} \sum_{\mathbf{x} \in C_{\mathbf{u}}} \mathbf{x}_{i} = \arg \min_{\mathbf{u} \in \mathbb{R}^{d}} \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} - \mathbf{u} \rVert^{2} = \arg \min_{\mathbf{u} \in \mathbb{R}^{d}} \operatorname{loss}(\mathbf{u}, C_{\mathbf{u}}) </span></p>
<p>because of the equation below derived from the bias-variance decomposition of MSE function:</p>
<p><span class="math display"> \operatorname{loss}(\mathbf{u}, C_{\mathbf{u}}) = \operatorname{loss}(\mathbf{\boldsymbol{\mu}}, C_{\mathbf{\mathbf{u}}}) + n \lVert \boldsymbol{\mu} - \mathbf{u} \rVert </span></p>
<p>where <span class="math inline">n</span> is the number of instances in the cluster <span class="math inline">C_{\mathbf{u}}</span>.</p>
<p>:::{admonition} Proof :class: dropdown</p>
<p><span class="math display">
  \begin{align}
  \operatorname{loss}(\mathbf{u}, C_{\mathbf{u}}) &amp; = \operatorname{loss}(\boldsymbol{\mu}, C_{\mathbf{u}}) + n\lVert \boldsymbol{\mu} - \mathbf{u} \rVert^{2} \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} - \boldsymbol{\mu} \rVert^{2} + n\lVert \boldsymbol{\mu} - \mathbf{u} \rVert^{2} \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \left( \lVert \mathbf{x} \rVert^{2} - 2\lVert \mathbf{x} \rVert \lVert \boldsymbol{\mu} \rVert + \lVert \boldsymbol{\mu} \rVert^{2} \right) + n\left( \lVert \boldsymbol{\mu} \rVert^{2} - 2 \lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + \lVert \mathbf{u} \rVert^{2} \right) \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \left( \lVert \mathbf{x} \rVert^{2} - 2\lVert \mathbf{x} \rVert \lVert \frac{S}{n} \rVert + \lVert \frac{S}{n} \rVert^{2} \right) + n\left( \lVert \frac{S}{n} \rVert^{2} - 2 \lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + \lVert \mathbf{u} \rVert^{2} \right) &amp; \left[ \text{replace some } \boldsymbol{\mu} \text{ with } \frac{S}{n} \text{ where } S = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \mathbf{x} \right] \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} \rVert^{2} - \sum_{\mathbf{x} \in C_{\mathbf{u}}} 2\lVert \mathbf{x} \rVert \lVert \frac{S}{n} \rVert + \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \frac{S}{n} \rVert^{2} + n\lVert \frac{S}{n} \rVert^{2} - 2n\lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + n\lVert \mathbf{u} \rVert^{2} \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} \rVert^{2} - 2 \lVert S \rVert \lVert \frac{S}{n} \rVert + n\lVert \frac{S}{n} \rVert^{2} + n\lVert \frac{S}{n} \rVert^{2} - 2n\lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + n\lVert \mathbf{u} \rVert^{2} &amp; \left[ \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert x \rVert = \lVert S \rVert \text{ and } \sum_{\mathbf{x} \in C_{\mathbf{u}}} 1 = n \right] \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} \rVert^{2} - 2n\lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + n\lVert \mathbf{u} \rVert^{2} &amp; \left[ \lVert S \rVert \lVert \frac{S}{n} \rVert = n\lVert \frac{S}{n} \rVert^{2}  \right] \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} ( \lVert \mathbf{x} \rVert^{2} - 2\lVert \boldsymbol{\mu} \rVert \lVert \mathbf{u} \rVert + \lVert \mathbf{u} \rVert^{2} ) \\
  &amp; = \sum_{\mathbf{x} \in C_{\mathbf{u}}} \lVert \mathbf{x} - \mathbf{u} \rVert^{2} \\
  \end{align}
  </span></p>
<p>Since <span class="math inline">\lvert C_{\mathbf{\mathbf{u}}} \rvert \cdot \lVert \boldsymbol{\mu} - \mathbf{u} \rVert</span> is always positive,</p>
<p><span class="math display"> \operatorname{loss}(\boldsymbol{\mu}, C_{\mathbf{u}}) \leq \operatorname{loss}(\mathbf{\mathbf{u}}, C_{\mathbf{\mathbf{u}}}) </span></p>
<p>:::</p></li>
</ul>
</section>
</section>
<section id="the-k-means-initializer" class="level2">
<h2 class="anchored" data-anchor-id="the-k-means-initializer">## The K-means++ initializer</h2>
<p>Although the default behavior of the K-means algorithm is to initialize the centroids randomly, the quality of the final solution depends heavily on the initialization because K-means is only guaranteed to converge to a local point.</p>
<p>The K-means++ initializer is a special way of initializing the centroids so that - the convergence of K-means is faster, - the final loss is bounded (the quality of the final solution won’t be very bad).</p>
<blockquote class="blockquote">
<ol type="1">
<li>Pick an instance <span class="math inline">\mathbf{x}</span> uniformly at random and set <span class="math inline">T \gets \{\mathbf{x}\}</span></li>
<li>While <span class="math inline">\lvert T \rvert &lt; k</span>:
<ol type="1">
<li><p>Pick an instance <span class="math inline">\mathbf{x}</span> at random, with probability proportional to</p>
<p><span class="math display"> \operatorname{cost}(\mathbf{x}, T) = \min_{\mathbf{u} \in T} \lVert \mathbf{x} - \mathbf{u} \rVert^{2} </span></p></li>
<li><p>Add <span class="math inline">\mathbf{x}</span> to <span class="math inline">T</span>.</p></li>
</ol></li>
</ol>
</blockquote>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<hr>
<ol type="1">
<li>https://stanford.edu/~cpiech/cs221/handouts/kmeans.html</li>
<li>https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf</li>
</ol>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<hr>
<div class="cell" data-jupyter="{&quot;outputs_hidden&quot;:true}" data-tags="[]">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#https://takoscribe.com/2020/12/29/kmeans-clustering-with-pytorch/</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> functools</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradientKMeans(nn.Module):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_centroids, n_epochs, batch_size, lr<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_centroids <span class="op">=</span> num_centroids</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_epochs <span class="op">=</span> n_epochs</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _initialize(<span class="va">self</span>, x):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        assignment <span class="op">=</span> [i <span class="op">%</span> <span class="va">self</span>.num_centroids <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(x.size(<span class="dv">0</span>))]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        random_indices <span class="op">=</span> torch.randperm(<span class="bu">len</span>(assignment))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        random_assignment <span class="op">=</span> torch.LongTensor(assignment)[random_indices]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_centroids):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.centroids.data[i] <span class="op">=</span> x[random_assignment <span class="op">==</span> i].mean(<span class="dv">0</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _assign(<span class="va">self</span>, x):</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> ((x[:,<span class="va">None</span>] <span class="op">-</span> <span class="va">self</span>.centroids) <span class="op">**</span> <span class="dv">2</span>).mean(<span class="dv">2</span>).argmin(<span class="dv">1</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> indices</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._assign(x)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.centroids <span class="op">=</span> nn.Parameter(torch.zeros(<span class="va">self</span>.num_centroids, X.shape[<span class="dv">1</span>]))</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> torch.optim.Adam(<span class="va">self</span>.parameters(), lr<span class="op">=</span><span class="va">self</span>.lr)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.loss <span class="op">=</span> nn.MSELoss()</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        centroids_init <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        cost_window <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        costs <span class="op">=</span> []</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        X_t <span class="op">=</span> torch.utils.data.TensorDataset(torch.Tensor(X), torch.zeros((X.shape[<span class="dv">0</span>], )))</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        iterator <span class="op">=</span> torch.utils.data.DataLoader(X_t, batch_size<span class="op">=</span><span class="va">self</span>.batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_epochs):</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> tqdm.tqdm(total<span class="op">=</span><span class="bu">len</span>(X) <span class="op">//</span> <span class="va">self</span>.batch_size) <span class="im">as</span> progress_bar:</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> x, _ <span class="kw">in</span> iterator:</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="kw">not</span> centroids_init: </span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>._initialize(x)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>                        centroids_init <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>                    assignment <span class="op">=</span> <span class="va">self</span>._assign(x)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.optimizer.zero_grad()</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>                    means <span class="op">=</span> <span class="va">self</span>.centroids[assignment]</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>                    cur_cost <span class="op">=</span> <span class="va">self</span>.loss(x, means)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>                    cur_cost.backward()</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.optimizer.step()</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>                    costs.append(cur_cost.item())</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>                    progress_bar.set_postfix({</span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'KMeans'</span>: <span class="bu">float</span>(functools.<span class="bu">reduce</span>(<span class="kw">lambda</span> x, y: x <span class="op">+</span> y, costs[<span class="op">-</span>cost_window:])) <span class="op">/</span>  <span class="bu">len</span>(costs[<span class="op">-</span>cost_window:])</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>                    })</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>                    progress_bar.update(<span class="dv">1</span>) <span class="co"># 1 step</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> <span class="va">self</span>(torch.Tensor(X))</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> Y</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_centroids(<span class="va">self</span>):</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> <span class="va">self</span>.centroids.cpu().detach().numpy()</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> centroids</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="bu">len</span>(centers)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">6000</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> make_blobs(n_samples<span class="op">=</span>n_samples, centers<span class="op">=</span>centers, cluster_std<span class="op">=</span><span class="fl">0.7</span>, random_state<span class="op">=</span><span class="dv">40</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>gradient_kmeans <span class="op">=</span> GradientKMeans(<span class="dv">2</span>, <span class="dv">10</span>, <span class="dv">64</span>)</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>gradient_kmeans.fit(X)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> gradient_kmeans.predict(X)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>centroids <span class="op">=</span> gradient_kmeans.get_centroids()</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>plt.figure(<span class="dv">1</span>)</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, col <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"r"</span>, <span class="st">"b"</span>, <span class="st">"g"</span>, <span class="st">"m"</span>, <span class="st">"y"</span>, <span class="st">"c"</span>]):</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    cluster_data <span class="op">=</span> y_true <span class="op">==</span> k</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[cluster_data, <span class="dv">0</span>], X[cluster_data, <span class="dv">1</span>], c<span class="op">=</span>col, marker<span class="op">=</span><span class="st">"."</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>plt.scatter(centroids[:, <span class="dv">0</span>], centroids[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"w"</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>94it [00:00, 591.45it/s, KMeans=1.01]                                                                                                                                                                                                                                                   
94it [00:00, 1217.76it/s, KMeans=0.943]                                                                                                                                                                                                                                                 
94it [00:00, 1212.54it/s, KMeans=0.921]                                                                                                                                                                                                                                                 
94it [00:00, 1192.24it/s, KMeans=0.941]                                                                                                                                                                                                                                                 
94it [00:00, 1196.55it/s, KMeans=0.928]                                                                                                                                                                                                                                                 
94it [00:00, 1179.02it/s, KMeans=0.89]                                                                                                                                                                                                                                                  
94it [00:00, 1181.19it/s, KMeans=0.922]                                                                                                                                                                                                                                                 
94it [00:00, 1178.50it/s, KMeans=0.932]                                                                                                                                                                                                                                                 
94it [00:00, 1096.57it/s, KMeans=0.932]                                                                                                                                                                                                                                                 
94it [00:00, 1188.89it/s, KMeans=0.909]                                                                                                                                                                                                                                                 </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="K-means_files/figure-html/cell-2-output-2.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>