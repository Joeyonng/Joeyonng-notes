<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Joeyonng - ML Q &amp; A</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Joeyonng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Knowledge</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Notes/index.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://joeyonng.github.io/joeyonng-backyard/" rel="" target=""><i class="bi bi-House" role="img">
</i> 
 <span class="menu-text">Backyard</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#machine-learning-interview-questions" id="toc-machine-learning-interview-questions" class="nav-link active" data-scroll-target="#machine-learning-interview-questions">Machine Learning Interview Questions</a>
  <ul class="collapse">
  <li><a href="#ml-basics" id="toc-ml-basics" class="nav-link" data-scroll-target="#ml-basics">ML Basics</a></li>
  <li><a href="#neural-network" id="toc-neural-network" class="nav-link" data-scroll-target="#neural-network">Neural Network</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning">Unsupervised learning</a></li>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised Learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ML Q &amp; A</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="machine-learning-interview-questions" class="level1">
<h1>Machine Learning Interview Questions</h1>
<section id="ml-basics" class="level2">
<h2 class="anchored" data-anchor-id="ml-basics">ML Basics</h2>
<section id="what-is-the-trade-off-between-bias-and-variance" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-trade-off-between-bias-and-variance">What is the trade-off between bias and variance?</h4>
<ol type="1">
<li>We always want the model to have low bias and low variance at the same time, but it is difficult.</li>
<li>Bias is the average difference between the predictions and the label.</li>
<li>Variance refers to the sensitivity of our model to the fluctuations (noise) in the training set. OR variance describes how much a random variable of your model differs from its expected value.</li>
<li>Since we only have training set that is a sample from the overall distribution, training our model to reduce bias will generally increase variance at the same time because we don’t know what is the noise.</li>
<li>High bias and low variance lead to under-fitting, where your model is too simple to capture the regularities in your data. This will result in both low training accuracy and testing accuracy.</li>
<li>Low bias and high variance lead to over-fitting, where your model is too complex that it captures all the patterns in your training data including noise. It will result in high training accuracy, but low testing accuracy.</li>
</ol>
</section>
<section id="explain-over--and-under-fitting-and-how-to-combat-them" class="level4">
<h4 class="anchored" data-anchor-id="explain-over--and-under-fitting-and-how-to-combat-them">Explain over- and under-fitting and how to combat them?</h4>
<ol type="1">
<li>Under-fitting is when your model is too simple to fit the training set correctly. This will result in low training accuracy and low testing accuracy, assuming that our training set is a reasonable sample of the overall distribution.</li>
<li>To combat under-fitting, the first we can do is to choose the model with more complexity. If decision tree under-performs on a dataset, we can try random forest. Second we can increase the parameters that our model have. Like increasing number of trees or the maximum depth of the decision tree. Last, for some models that uses iterative learning procedure such as neural network, we can increase the training time.</li>
<li>Over-fitting is when your model is too complex such it can fit the training set very accurately. This means that it also fit the noise correctly and will result in high training accuracy and low testing accuracy.</li>
<li>To combat over-fitting, we first can reduce the number of parameters. Second, using ensemble of your current models is proven to reduce over-fitting. Third, without changing models, we can do data augmentation such as over-sampling more data to make the training better represent the actual distribution.</li>
</ol>
</section>
<section id="difference-between-supervised-and-unsupervised-learning" class="level4">
<h4 class="anchored" data-anchor-id="difference-between-supervised-and-unsupervised-learning">Difference between supervised and unsupervised learning?</h4>
<ol type="1">
<li>Supervised learning task is usually provided with labeled dataset and goal is to predict the correct label for a given instance.</li>
<li>Unsupervised learning only gives the instances themselves, without their labels. The goal is to learn patterns from those unlabeled data.</li>
<li>Supervised learning include classification and regression. Classification is to predict a categorical label while regression is to predict a continuous value.</li>
<li>Unsupervised learning include clustering and dimension reduction. Clustering is to group similar instances together and dimension reduction is to select the important features from all features.</li>
</ol>
</section>
<section id="what-is-the-curse-of-dimensionality-and-how-to-combat-it" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-curse-of-dimensionality-and-how-to-combat-it">What is the “Curse of Dimensionality” and how to combat it?</h4>
<ol type="1">
<li>Dimensionality refers to the number of features in your dataset.</li>
<li>It is harder for the models to search through a space as the number of features grows. The required number of training instances to achieve the same accuracy grows exponentially with the number of features. Since in practice the number of training instances are fixed, the performance of the models will typically decrease as the dimension increases.</li>
<li>We can use feature selection such as manual feature selection by human or feature extraction technique like PCA to reduce the dimensionality. The difference between selection and extraction is that selection selected subset of the original features and extraction extracts a set of new features from data.</li>
</ol>
</section>
<section id="what-is-a-confusion-matrix" class="level4">
<h4 class="anchored" data-anchor-id="what-is-a-confusion-matrix">What is a confusion matrix?</h4>
<ol type="1">
<li>The confusion matrix is used to evaluate the performance of an supervised classifier on a dataset. It has <span class="math inline">N</span> rows and <span class="math inline">N</span> columns where <span class="math inline">N</span> is the number of classes in the dataset.</li>
<li>Each row of the matrix gives the number of predictions from the classifier for a specific label and each column of the matrix gives the number of actual labels. If the classifier is for a binary classification dataset, the first row gives the number of true positives and the number of false positives and the second row will give false negatives and true negatives.</li>
<li>Other performance evaluation methods such as accuracy or F1-score will be misleading on unbalanced dataset. For example, if we have a dataset that has 95 dogs and 5 cats and a classifier than always predict dog. We have 95% accuracy and 97.5% F1-score, which tells that our classifier is very good. Confusion matrix will give a whole picture of metrics that include true positive, true negative, false positive and false negative. If we add up true positive and false positive in the matrix, we can see that our classifier only predict positive labels.</li>
</ol>
<p>Notes: 1. For a binary classification problem, we can have several terms: 1. True positive: classifier gives positive class and the prediction is correct. 2. False positive: classifier gives positive class and the prediction is incorrect. 3. True negative: classifier gives negative class and the prediction is correct. 4. False negative: classifier gives negative class and the prediction is incorrect. 2. For a binary classification problem, we have several performance measures: 1. <strong>Sensitivity (true positive rate)</strong>: number of positive predictions correctly labeled / number of instances with positive labels. <span class="math display"> \mathrm{\frac{TP}{TP + FN}} </span> False positive rate: number of positive predictions incorrectly labeled / number of instances with negative labels. <span class="math display"> \mathrm{\frac{FP}{FP + TN}} </span> 2. <strong>Specificity (true negative rate)</strong>: number of negative predictions correctly labeled / number of instances with negative labels. <span class="math display"> \mathrm{\frac{TN}{TN + FP}} </span> 3. <strong>Precision</strong>: number of positive instances correctly predicted / number of positive predictions. <span class="math display"> \mathrm{\frac{TP}{TP + FP}} </span> 4. <strong>Recall</strong>: same as sensitivity. <span class="math display"> \mathrm{\frac{TP}{TP + FN}} </span> 5. <strong>F1-score</strong>: harmonic mean of the precision and recall. <span class="math display"> \mathrm{2 \cdot \frac{precision \cdot recall}{precision + recall} = \frac{TP}{TP + \frac{1}{2}(FP + FN)}}</span></p>
</section>
<section id="what-is-roc-curve-and-aoc" class="level4">
<h4 class="anchored" data-anchor-id="what-is-roc-curve-and-aoc">What is ROC curve and AOC?</h4>
<p>https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc 1. An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The <span class="math inline">x</span> axis is false positive rate and <span class="math inline">y</span> axis is true positive rate. 2. An ROC curve plots TPR vs.&nbsp;FPR at different classification thresholds. For all models that first produce a score and then thresholded to give the classification, different thresholds mean different number of positive and negative predictions. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives and the curve will always go higher. 3. A perfect model will have a straight horizontal line at y = 1 while a perfectly wrong model will have a horizontal line at y = 0. A random guessing model will have a diagonal line, which means that the model has no class separation capacity. 4. AOC stands for area under the curve and measures the area under the ROC curve. It is used to measure a model’s performance across all possible classification thresholds.</p>
</section>
<section id="what-is-exponential-moving-average-exponential-weighted-moving-average" class="level4">
<h4 class="anchored" data-anchor-id="what-is-exponential-moving-average-exponential-weighted-moving-average">What is exponential moving average (exponential weighted moving average)?</h4>
<ol type="1">
<li>Calculate moving average of a set of data points is creating a series of averages of different subsets of the dataset. The dataset to be analyzed usually contains time series data (data that is indexed by timestamps). Exponential moving average (EMA) is one type of moving average algorithm.</li>
<li>The EMA value at timestamp <span class="math inline">t</span> calculates the average of the data from the beginning to timestamp <span class="math inline">t</span>, which is calculated by adding up the weighted value of the data at timestamp <span class="math inline">t</span> and the weighted EMA value on the previous timestamp <span class="math inline">t-1</span>.</li>
<li>The weighted part is called the smoothing factor and is a hyper-parameter that is between 0 and 1. Higher smoothing factor means the current data value is weighted more and the previous EMA value is weighted less in the calculation of the new EMA value.</li>
<li>This techniques is usually used by gradient optimizer to calculate new learning rate.</li>
</ol>
<p>Notes: 1. <strong>[EMA equation]</strong>: If the current timestamp is <span class="math inline">t</span>, then the equation for EMA is: <span class="math display"> y_{t} = \alpha x_{t} + (1 - \alpha)y_{t-1} </span> where <span class="math inline">y_{t}</span> is the moving average at timestamp <span class="math inline">t</span>, <span class="math inline">x_{t}</span> is the data point at timestamp <span class="math inline">t</span>, and <span class="math inline">\alpha</span> is the smoothing factor, which is the range <span class="math inline">[0, 1]</span>.</p>
</section>
<section id="what-are-cross-valiation-and-nested-cross-validation" class="level4">
<h4 class="anchored" data-anchor-id="what-are-cross-valiation-and-nested-cross-validation">What are cross-valiation and nested cross-validation?</h4>
<p>TODO</p>
</section>
</section>
<section id="neural-network" class="level2">
<h2 class="anchored" data-anchor-id="neural-network">Neural Network</h2>
<section id="what-is-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="what-is-gradient-descent">What is gradient descent?</h4>
<ol type="1">
<li>Gradient descent is an optimization algorithm that can iteratively minimize the target function to find its local minimum. If the target function is convex, then gradient descent can also find its global minimum.</li>
<li>First we calculate the derivatives of the target function with respect to the parameters of the model. This is the gradient.</li>
<li>Second we update the parameters to the opposite direction of the gradients to minimize the value of the target function. This is the descent.</li>
<li>Gradient descent will minimize the target function, while gradient ascent, which updates the parameters to the same direction of the gradients, will maximize the target function.</li>
</ol>
</section>
<section id="what-is-back-propagation" class="level4">
<h4 class="anchored" data-anchor-id="what-is-back-propagation">What is back-propagation?</h4>
<ol type="1">
<li>TODO</li>
</ol>
</section>
<section id="differences-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent." class="level4">
<h4 class="anchored" data-anchor-id="differences-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent.">Differences between gradient descent, stochastic gradient descent and mini-batch gradient descent.</h4>
<p>https://ruder.io/optimizing-gradient-descent/index.html#fn4 https://cs231n.github.io/neural-networks-3/#update 1. Normal gradient descent is batch gradient descent. One batch means one complete run of the training set. Thus, we need to evaluate all instances of the training set before one gradient update. Gradient descent is more slow, but guaranteed to converge to the local minimum. 2. Stochastic gradient descent means that one gradient update is performed for each instance evaluated. This approach converges faster and can be used on the fly as the new instance comes in, but can cause target function to fluctuate. 3. Mini-batch is the combination of the two above. Instead of whole batch or single instance, we take subset of training set as the mini-batch and evaluate them to get gradient for a single gradient update. This combines the benefits of two method above.</p>
<p>Notes: 1. <strong>[Different gradient descent optimization algorithms]</strong> 1. SGD <span class="math display"> \theta_{t+1} = \theta_{t} - \lambda \partial_{t}(\theta) </span> where <span class="math inline">\lambda</span> is the learning rate and <span class="math inline">\partial_{t}(\theta)</span> is the gradient of the loss function w.r.t the parameter <span class="math inline">\theta</span> at time <span class="math inline">t</span>. 2. SGD with Momentum: It will help the convergence speed of SGD because it reduces the oscillations of the SGD near the local minimas, which is done by building up the velocity in the correct direction that has consistent gradients <span class="math display"> v_{t} = \mu v_{t-1} - \lambda\cdot\partial_{t}(\theta)  </span> <span class="math display"> \theta_{t+1} = \theta_{t} + v_{t} </span> where <span class="math inline">\mu</span> is the momentum parameter that is typically 0.9 and <span class="math inline">v_{t}</span> is the correct accumulated gradient direction at time <span class="math inline">t</span>. 3. Adagrad: It will make the learning rates of the weights that receive high gradients reduced, and the learning rates of the weights that receive small or infrequent updates increased. Thus we don’t have to manually tune the learning rates in the training progress. However, since there is no way to reduce the accumulated squared gradients in the denominator of the learning rate, the monotonically decreasing learning rate in the training process will eventually stop the training. <span class="math display"> g_{t} = g_{t-1} + \partial_{t}^{2}(\theta) </span> <span class="math display"> \theta_{t+1} = \theta_{t} - \frac{\lambda}{\sqrt{g_{t} + \epsilon}} \partial_{t}(\theta) </span> where <span class="math inline">g_{t}</span> is the accumulation of the squared gradient for <span class="math inline">\theta</span> until time <span class="math inline">t</span> and <span class="math inline">\epsilon</span> is a small value used to prevent the division by 0. 4. RMSprop: RMSprop improves on Adagrad by replacing the accumulation of the past squared gradients with the exponential moving average of the past squared gradients. This can solve the issue of Adagrad that the learning rates are monotonically decreasing. <span class="math display"> e_{t} = \beta e_{t-1} + (1-\beta)\partial_{t}^{2}(\theta) </span> <span class="math display"> \theta_{t+1} = \theta_{t} - \frac{\lambda}{\sqrt{e_{t} + \epsilon}} \partial_{t}(\theta)</span> where <span class="math inline">e_{t}</span> is the exponential moving average of the squared gradient for <span class="math inline">\theta</span> until time <span class="math inline">t</span> and <span class="math inline">\beta</span> is like momentum that controls degree of weighting decay and is usually set to 0.9. 5. Adam: Adam improves on RMSprop by replacing the raw gradient with the exponential moving average of the past gradients in the update step. It thus combines the benefits of RMSprop and SGD with momentum. <span class="math display"> v_{t} = \beta_{1} v_{t-1} + (1-\beta_{1})\partial_{t}(\theta) </span> <span class="math display"> e_{t} = \beta_{2} e_{t-1} + (1-\beta_{2})\partial_{t}^{2}(\theta) </span> <span class="math display"> \hat{v}_{t} = \frac{v_{t}}{1-\beta_{1}^{t}} </span> <span class="math display"> \hat{e}_{t} = \frac{e_{t}}{1-\beta_{2}^{t}} </span> <span class="math display"> \theta_{t+1} = \theta_{t} - \frac{\lambda \hat{v}_{t}}{\sqrt{\hat{e}_{t} + \epsilon}} </span> where <span class="math inline">\hat{v}_{t}</span> and <span class="math inline">\hat{e}_{t}</span> are the bias corrected versions of <span class="math inline">v_{t}</span> and <span class="math inline">e_{t}</span> and <span class="math inline">\beta^{t}</span> means the <span class="math inline">\beta</span> to the power of <span class="math inline">t</span>.</p>
</section>
<section id="what-is-vanishing-gradient" class="level4">
<h4 class="anchored" data-anchor-id="what-is-vanishing-gradient">What is vanishing gradient?</h4>
<ol type="1">
<li>Vanishing gradient happens to the parameters in the earlier layers of a deep neural network where the gradients are so small in the back-propagation process that the weights are not really changed.</li>
<li>The primary reason for this problem is the choice of activation functions such as sigmoid or hyperbolic tangent function, whose gradients are very small and are always much less than 1. If multiple layers with such activations are stacked together, the gradients to the earlier layers of the networks are multiplied lots of times with the loss gradient in the back-propagation process. Each layer reduce the original loss gradient by a fraction and in the end the gradient to the earlier layers are very small. Therefore, the large number of layers are also an important reason for vanishing gradient problem.</li>
<li>One effective solution to the problem is to use other activation functions such as ReLU.</li>
</ol>
</section>
<section id="why-is-relu-better-and-more-often-used-than-sigmoid-in-neural-networks" class="level4">
<h4 class="anchored" data-anchor-id="why-is-relu-better-and-more-often-used-than-sigmoid-in-neural-networks">Why is ReLU better and more often used than sigmoid in Neural Networks?</h4>
<ol type="1">
<li>It can help solve the vanishing gradient problem. The gradient of the ReLU for inputs larger than 0 is 1 and thus the gradient won’t be reduced in the back-propagation process.</li>
<li>ReLU is computationally more efficient because it only needs to cut the negative input to 0.</li>
<li>Historically speaking, ReLU is just good enough for neural network to be trained stably.</li>
</ol>
</section>
<section id="what-is-the-difference-between-l_1-and-l_2-regularization" class="level4">
<h4 class="anchored" data-anchor-id="what-is-the-difference-between-l_1-and-l_2-regularization">What is the difference between <span class="math inline">L_1</span> and <span class="math inline">L_2</span> regularization?</h4>
<ol type="1">
<li><span class="math inline">L_1</span> regularization is also called Lasso regularization. It adds the sum of the absolute value of all weights in the neural network as a penalty term to the loss function.</li>
<li><span class="math inline">L_2</span> regularization is also called Ridge regularization. It adds the sum of the squared value of all weights in the neural network as a penalty term to the loss function.</li>
<li>They both are used to reduce over-fitting issue of the large neural network.</li>
<li>The key difference is the gradient of each penalty. The gradient of Lasso is a 1 or -1 depending on the sign of each weight, while the gradient of Ridge is 2 times the value of the parameter. The weights with Lasso can possibly shrink to exactly 0, while weights with Ridge can only shrink to a very small value instead of exact 0 because the gradients also decrease as the weights decrease. Thus Lasso can be used to train sparse neural network.</li>
</ol>
</section>
<section id="what-is-dropout-in-neural-network." class="level4">
<h4 class="anchored" data-anchor-id="what-is-dropout-in-neural-network.">What is dropout in neural network.</h4>
<ol type="1">
<li>Dropout is to randomly drop neurons of the network in the training process to avoid over-fitting. A neuron is dropped means that the data and the gradient don’t go through that neuron in both forward or backward process.</li>
<li>Typically dropout is applied per layer and we can set a probability <span class="math inline">p</span> for each layer to indicate what percentage of neurons we want to drop in that layer. <span class="math inline">p</span> is usually selected for 0.5 for hidden layer, but a less value for input layer like 0.1 because randomly dropping an entire column of input data is very risky.</li>
<li>The dropout neurons are changed every instance or mini-batch depending on what type of gradient descent we are using. We only apply dropout in the training process and we will use all neurons for testing to have consistent output.</li>
</ol>
</section>
<section id="what-is-batch-normalization" class="level4">
<h4 class="anchored" data-anchor-id="what-is-batch-normalization">What is batch normalization?</h4>
<ol type="1">
<li>Batch normalization is to normalize the inputs to each layer for each mini-batch. Batch normalization is proved to help neural network training converges faster.</li>
<li>Batch normalization first normalize the inputs to each layer by subtracting the mini-batch mean from each value and then divide it by the mini-batch standard deviation. This process will make each input value to be in the range between 0 to 1. Then we need to scale and shift the normalized value into a desirable range. The coefficients for scaling and shifting are also two parameters that are needed to be learned in the backward propagation.</li>
<li>The challenge that batch normalization is trying to solve is called internal covariate shift. Since each layer’s output is fed into next layer’s input, the change of the weights in the first layer due to backward process after a mini-batch will cause the change of its output distribution. The internal covariate shift slows down the training process because the learning in the next iteration needs to accommodate for this change.</li>
</ol>
<p>Notes: 1. <strong>[Batch Normalization Layer]</strong>: Batch normalization layer can be appended before each layer and outputs the same dimension as the inputs 1. Get the mean <span class="math inline">\mu_{B}</span> and standard deviation <span class="math inline">\sigma_{B}</span> of the inputs in a mini-batch: <span class="math display"> \mu_{B}=\frac{1}{m}\sum_{i=1}^{m}x_i </span> <span class="math display"> \sigma_{B}=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_{B})^{2}} </span> 2. Normalize the input: <span class="math display"> \hat{x}_i=\frac{x_i-\mu_{B}}{\sigma_{B}} </span> 3. Scale and shift back, where <span class="math inline">\gamma</span> is the scaling parameter and <span class="math inline">\beta</span> is the shifting parameter: <span class="math display"> y_{i} = \gamma\hat{x}_{i}+\beta </span></p>
</section>
<section id="what-is-xavier-initialization" class="level4">
<h4 class="anchored" data-anchor-id="what-is-xavier-initialization">What is Xavier initialization?</h4>
<p>https://www.deeplearning.ai/ai-notes/initialization/index.html 1. Xavier initialization is a specific way of initialize the weights and bias in the neural network such that the variance of the activations are relatively the same across all layers. 2. If we use Xavier initialization method, the bias will be initialized to 0 and the weights are randomly sampled from a normal distribution that has mean of 0 and variance of 1 over the the number of neurons in the last layer. 3. If we initialize the weights to have the same value, all neurons will have the same activations. Same activations mean that all neurons will have the same gradients and will evolve the same throughout the training. 4. If we initialize the weights to be too small or too large, the activations of each layer in the first several iterations will also be very small or large. Since gradients of weights for each layer are calculated based on the activations, large weights will result in gradient exploding and small weights result in gradient vanishing, both of which prevent the neural from efficiently learning.</p>
</section>
</section>
<section id="unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised learning</h2>
<p>https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning#clustering</p>
<section id="k-means" class="level4">
<h4 class="anchored" data-anchor-id="k-means">K-means</h4>
<p>https://stanford.edu/~cpiech/cs221/handouts/kmeans.html 1. K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed. 2. We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change. 3. K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.</p>
<p>Notes 1. <strong>[K-means algorithm]</strong>: 1. Initialize cluster centroids <span class="math inline">\mu_{1}, ..., \mu_{k}</span> randomly. 2. Repeat until convergence: 1. Get the centroid for each instance <span class="math inline">x_{i}</span>: <span class="math display"> c_{i} = \underset{\mu_{i}}{\operatorname{argmin}} \lVert x_{i}-\mu_{i} \rVert^{2} </span> 2. Update the centroid based on the instances: <span class="math display"> \mu_{j} = \frac{\sum_{i}^{m}1_{\{c_i=j\}}x_{i}}{\sum_{i}^{m}1_{\{c_i=j\}}} </span></p>
</section>
<section id="pca-principle-component-analysis" class="level4">
<h4 class="anchored" data-anchor-id="pca-principle-component-analysis">PCA (Principle Component Analysis)</h4>
<p>https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c 1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. 2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.</p>
<p>Notes: 1. <strong>[Eigenvectors, Eigenvalues]</strong>: Given a matrix <span class="math inline">A\in\mathbb{R}^{n\times n}</span>, <span class="math inline">\lambda</span> is said to be an eigenvalue of <span class="math inline">A</span> if there exists a eigenvector <span class="math inline">z\in\mathbb{R}^n \neq 0</span>, such that: <span class="math display"> Az = \lambda z </span> 2. <strong>[Eigendecomposition (spectral decomposition]</strong>: Let <span class="math inline">M</span> be a real symmetric <span class="math inline">d \times d</span> matrix with eigenvalues <span class="math inline">\lambda_{1}, ... , \lambda_{d}</span> and corresponding orthonormal eigenvectors <span class="math inline">u_{1}, ..., u_{d}</span>. Then: <span class="math display"> M = Q \Lambda Q^T </span> where <span class="math inline">\Lambda</span> is a diagonal matrix with <span class="math inline">\lambda_{1}, ... , \lambda_{d}</span> in diagonal and 0 elsewhere and <span class="math inline">Q</span> matrix has <span class="math inline">u_{1}, ..., u{d}</span> vectors as columns.</p>
</section>
</section>
<section id="supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h2>
<section id="naive-bayes-classifier-nb" class="level4">
<h4 class="anchored" data-anchor-id="naive-bayes-classifier-nb">Naive Bayes Classifier (NB)</h4>
<ol type="1">
<li><strong>[Bayes’ theorem]</strong>: the conditional possibility of event <span class="math inline">A</span> given the event <span class="math inline">B</span> is true <span class="math inline">P(A|B)</span> can be computed as: <span class="math display"> P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} </span> which in the Bayesian term is written as: <span class="math display"> \mathrm{Posterior} = \frac{\mathrm{Likelihood} \cdot \mathrm{Prior}}{\mathrm{Evidence}} </span> If we think <span class="math inline">A</span> as a label and <span class="math inline">B</span> as a set of features:
<ul>
<li><span class="math inline">P(A|B)</span> is the <strong>posterior</strong> probability of a label given a set of features.</li>
<li><span class="math inline">P(B|A)</span> is the <strong>likelihood</strong> which is the probability of a set of features given a label.</li>
<li><span class="math inline">P(A)</span> is the <strong>prior</strong> probability of a label.</li>
<li><span class="math inline">P(B)</span> is the <strong>evidence</strong> probaility of a set of features.</li>
</ul></li>
<li><strong>[Naive Bayes]</strong>: Naive Bayes is a classifier that selects the label <span class="math inline">\hat{y}</span> from all possible labels <span class="math inline">y \in Y</span> that has maximum conditional possibility given the instance <span class="math inline">\mathbf{x}</span>. <span class="math display"> \hat{y} = \underset{y \in Y}{\operatorname{argmax}} P(y|\mathbf{x}) </span> Applying Bayes’ theorem, we have: <span class="math display"> P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y) \cdot P(y)}{P(\mathbf{x})} </span> Since <span class="math inline">P(\mathbf{x})</span> is a constant and is independent from <span class="math inline">P(y_{i})</span>, we can simply drop it: <span class="math display"> P(y|\mathbf{x}) \propto P(\mathbf{x}|y) \cdot P(y) </span> If we assume that each feature is independent from each other (naive conditional independence assumption), the possibility that the features values are all in <span class="math inline">\mathbf{x}</span> is the product of their possibilities: <span class="math display"> P(\mathbf{x}|y) = \prod_{i}^{m}P(x_{i}|y) </span> Put them together, we have: <span class="math display"> \hat{y} = \underset{y \in Y}{\operatorname{argmax}} \prod_{i}^{m}P(x_{i}|y) \cdot P(y) </span></li>
<li></li>
</ol>
</section>
<section id="linear-regression-and-logistic-regression-lr" class="level4">
<h4 class="anchored" data-anchor-id="linear-regression-and-logistic-regression-lr">Linear regression and logistic regression (LR)</h4>
<p>https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html 1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. 2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. 3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model’s output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction’s error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. 4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression. 5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels.</p>
<p>Notes: 1. <strong>[Mean squared Error (MSE)]</strong>:<br>
<span class="math display"> \mathrm{MSE} = \frac{1}{n}\sum_{i}^{n}(y_{i}-\hat{y}_{i})^2 </span> where <span class="math inline">y_{i}</span> is the actual label and <span class="math inline">\hat{y}_{i}</span> is the prediction given by the classifier. 1. <strong>[Binary cross entropy (BCE)]</strong>: only works if the labels <span class="math inline">y_{i}</span> are 0 or 1 and the values of predictions <span class="math inline">\hat{y}_{i}</span> are between 0 and 1. <span class="math display"> \mathrm{BCE} = -\frac{1}{n}\sum_{i}^{n}(y_{i}\log(\hat{y}_{i}) + (1-y_{i})\log(1-\hat{y}))) </span> which can be decomposed to two cases for each prediction and label pair: <span class="math display"> -\log(\hat{y}_{i}) \;\mathrm{if}\; y=1 </span> <span class="math display"> -\log(1 - \hat{y}_{i}) \;\mathrm{if}\; y=0 </span> 1. <strong>[Sigmoid (logistic) function and logic function]</strong>: <span class="math display"> \mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}} </span> <span class="math display"> \mathrm{logic}(x) = \log(\frac{x}{1 - x})</span> The inverse of sigmoid function is the logic function: <span class="math display">
    \begin{alignat}{2}
    x &amp;= \frac{1}{1 + e^{-y}} \\
    \frac{1}{x} &amp;= 1 + e^{-y} \\
    e^{-y} &amp;= \frac{1 - x}{x} \\
    e^{y} &amp;= \frac{x}{1 - x} \\
    y &amp;= \log(\frac{x}{1 - x}) \\
    \end{alignat}
    </span> 1. <strong>[How to solve the parameters for linear regression and logistic regression]</strong> 1. Linear regression can be solved mathematically by setting partial derivative of loss w.r.t each weight to 0: (bias is removed for simplification) <span class="math display"> \frac{\partial f}{\partial w_{k}} = \frac{2}{N} \sum_{i}^{N} x_{i,k} \bigg(\sum_{j}^{D}w_{j}x_{i,j} - \hat{y}_{i} \bigg) </span> <span class="math display"> = \frac{2}{N} \sum_{i}^{N} \bigg( x_{i, k} \sum_{j}^{D}w_{j}x_{i,j} - x_{i, k}\hat{y}_{i} \bigg) </span> <span class="math display"> = \frac{2}{N} \sum_{i}^{N} \bigg( x_{i, k} \sum_{j}^{D}w_{j}x_{i,j} \bigg) - \frac{2}{N} \sum_{i}^{N} x_{i,k}\hat{y}_{i} </span> <span class="math display"> = \frac{2}{N} \sum_{j}^{D} w_{j} \bigg( \sum_{i}^{N} x_{i,j}x_{i,k} \bigg) - \frac{2}{N} \sum_{i}^{N} x_{i,k}\hat{y}_{i} </span> 1. Gradient descent can be applied to solve both linear regression and logistic regression. Logistic regression doesn’t have a closed-form solution because of the non-linearity that the sigmoid function imposes.</p>
</section>
<section id="generalized-linear-models-glm-and-generalized-additive-models-gam" class="level4">
<h4 class="anchored" data-anchor-id="generalized-linear-models-glm-and-generalized-additive-models-gam">Generalized linear models (GLM) and generalized additive models (GAM)</h4>
<p>https://christophm.github.io/interpretable-ml-book/extend-lm.html 1. Generalized linear models build on linear regression models to predict a non-Gaussian distribution. It keeps the weighted sum of the features of the linear regression, but connect the weighted sum and the expected mean of the output distribution through a possibly nonlinear function. 2. For example, the logistic regression is a type of the generalized linear models and it assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logic function. 3. Generalized additive models further relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature. It allows to model the potentially non-linear relations between the features and the output.</p>
<p>Notes: 1. <strong>[Assumptions of linear regression]</strong>: 1. The input features are independent from each other (no interactions between the features). 2. The output distribution <span class="math inline">y</span> given the input features <span class="math inline">X</span> follows a Gaussian distribution. This follows the following theorem: &gt; Let <span class="math inline">X_1, ..., X_n</span> be <span class="math inline">n</span> mutually independent normal random variables, having means <span class="math inline">\mu_1, ..., \mu_n</span> and variances <span class="math inline">\sigma_1^2, ... \sigma_n^2</span>. If the random variable <span class="math inline">Y</span> is a linear combinations of the <span class="math inline">X</span> with <span class="math inline">w_1, ..., w_n</span> coefficients: <span class="math display">Y=\sum_{i=1}^{n}w_iX_i</span>, then <span class="math inline">Y</span> is a Gaussian distribution with the mean <span class="math inline">\mathrm{E}[Y] = \sum_{i=1}^{n}b_i\mu_i</span> and variance <span class="math inline">\mathrm{Var}[Y] = \sum_{i=1}^{n}b_i^2\sigma_i^2</span>. 3. The true relationship between each feature <span class="math inline">X_i</span> and <span class="math inline">y</span> is linear. 2. <strong>[Components of GLM]</strong> 1. Random component: the probability distribution of the output variable <span class="math inline">Y</span>. It’s expected value (mean value) is <span class="math inline">\mathrm{E}(Y)</span>. 2. Systematic component: the weighted sum <span class="math inline">\sum_{1}^{n}w_ix_i + w_0</span>. 3. Link function: the relation between the random component and the systematic component <span class="math inline">g</span>. <span class="math display"> g(\mathrm{E}(Y)) = \sum_{1}^{n}w_ix_i + w_0 </span> 3. <strong>[Equation of GAM]</strong> <span class="math display"> g(\mathrm{E}(Y)) = \sum_{1}^{n}f(x_i) </span> where <span class="math inline">f()</span> can be arbitrarily defined function.</p>
</section>
<section id="support-vector-machine-svm" class="level4">
<h4 class="anchored" data-anchor-id="support-vector-machine-svm">Support vector machine (SVM)</h4>
<p>https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/ https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf 1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters. 1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane. 1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. 1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.</p>
<p>Notes: 1. <strong>[SVM without slacks (hard margin SVM)]</strong>: Given a dataset with <span class="math inline">n</span> instances <span class="math inline">x_{i} \in R^{d}</span> and <span class="math inline">n</span> labels <span class="math inline">y_{i} \in \{-1, 1\}</span>, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights <span class="math inline">w \in R^{d}</span> and a bias <span class="math inline">b \in R</span>, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem: <span class="math display">
    \begin{alignat}{2}
    \min \quad &amp; \frac{1}{2} \lVert w \rVert^{2} \\
    \text{s.t. } \quad &amp; y_{i}(w x_{i} + b) \geq 1, \quad i = 1, \dots n \\
    \end{alignat}
    </span> 1. Solving the above optimization problem will give us two parallel hyperplanes (<span class="math inline">w x + b = 1</span> and <span class="math inline">w x + b = -1</span>) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between. 1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as <span class="math display"> \frac{\lvert b_{2} - b_{1} \rvert}{\lVert w \rVert} = \frac{\lvert (b + 1) - (b - 1) \rvert}{\lVert w \rVert} = \frac{2}{\lVert w \rVert} </span> 1. The constraints specify that the instances must be on the correct side of the two hyperplanes: <span class="math display"> w x_{i} + b \geq 1 \quad \mathrm{if} y_{i} = 1 </span> <span class="math display"> w x_{i} + b \leq -1 \quad \mathrm{if} y_{i} = -1 </span> and <span class="math inline">y_{i}(w x_{i} + b) \geq 1</span> summarizes the above two conditions. 1. <strong>[SVM with slacks (soft margin SVM)]</strong>: In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. <span class="math display">
    \begin{alignat}{2}
    \min \quad &amp; \frac{1}{2} \lVert w \rVert^{2} + C \sum_{i}^{n} \xi_{i} \\
    \text{s.t. } \quad &amp; y_{i}(w x_{i} + b) \geq 1 - \xi_{i}, \quad i = 1, \dots n \\
    \quad &amp; \xi_{i} \geq 0, \quad i = 1, \dots n \\
    \end{alignat}
    </span> where <span class="math inline">\xi_{i}</span> is the slack variable for the instance <span class="math inline">x_{i}</span> and <span class="math inline">C</span> is a hyperparameter that penalizes the misclassification of <span class="math inline">x_{i}</span>. 1. If <span class="math inline">\xi_{i}</span> is nonzero for <span class="math inline">x_{i}</span>, it means that <span class="math inline">x_{i}</span> is on the misclassified side of <span class="math inline">w x_{i} + b = 1</span> (or <span class="math inline">w x_{i} + b = -1</span>) and the distance is <span class="math inline">\xi_{i}</span>. 1. If <span class="math inline">C = 0</span>, <span class="math inline">\xi_{i}</span> can be arbitrary large for each <span class="math inline">x_{i}</span>. If <span class="math inline">C \to \inf</span>, it is the same as hard margin SVM because any misclassification can induce infinite loss.</p>
<ol type="1">
<li><strong>[Solving hard margin SVM]</strong>
<ol type="1">
<li>Rewrite the primal program for easier Lagrangian computation below: <span class="math display">
\begin{alignat}{2}
\min \quad &amp; \frac{1}{2} ww \\
\text{s.t. } \quad &amp; -(y_{i}(w x_{i} + b) - 1) \leq 0, \quad i = 1, \dots n \\
\end{alignat}
</span></li>
<li>We can derive the Lagrangian primal function from the primal program: <span class="math display">
\begin{alignat}{2}
L(w, b, \alpha) &amp; = f(w, b) + \sum_{i}^{n} \alpha h_{i}(w, b) \\
&amp; = \frac{1}{2} ww - \sum_{i}^{n} \alpha_{i}(y_{i}(w x_{i} + b) - 1) \\
\end{alignat}
</span> where <span class="math inline">\alpha</span> is a new variable called Lagrangian multiplier.</li>
<li>Then we can write and solve Lagrangian dual function: <span class="math display">
\begin{alignat}{2}
g(\alpha) &amp; = \min_{w, b} L(w, b, \alpha) \\
&amp; = \min_{w, b} \frac{1}{2} ww - \sum_{i}^{n} \alpha_{i}(y_{i}(w x_{i} + b) - 1) \\
\end{alignat}
</span> Taking the derivation of <span class="math inline">L(w, b, \alpha)</span> over <span class="math inline">w</span>: <span class="math display">
\begin{alignat}{2}
\frac{\partial L}{\partial w} &amp; = 0 \\
w - \sum_{i}{n} \alpha_{i}y_{i}x_{i} &amp; = 0 \\
w &amp; = \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \\
\end{alignat}
</span> Taking the derivation of <span class="math inline">L(w, b, \alpha)</span> over <span class="math inline">b</span>: <span class="math display">
\begin{alignat}{2}
\frac{\partial L}{\partial b} &amp; = 0 \\
\sum_{i}^{n} \alpha_{i}y_{i} &amp; = 0 \\
\end{alignat}
</span> Plug in <span class="math inline">w = \sum_{i}^{n} \alpha_{i}y_{i}x_{i}</span> back to <span class="math inline">g(\alpha)</span>: <span class="math display">
\begin{alignat}{2}
g(\alpha)
&amp; = \min_{w, b} \frac{1}{2} ww - \sum_{i}^{n} \alpha_{i}(y_{i}(w x_{i} + b) - 1) \\
&amp; = \min_{w, b} \frac{1}{2} \left( \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \right) \left( \sum_{j}^{n} \alpha_{j}y_{j}x_{j} \right)
     - \sum_{i}^{n} \alpha_{i} \left( y_{i} \left( \left( \sum_{j}^{n} \alpha_{j}y_{j}x_{j} \right) x_{i} + b \right) - 1 \right) \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     - \sum_{i}^{n} \alpha_{i}y_{i}\left( \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right) x_{i} + b \right) + \sum_{i}^{n}\alpha_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     - \sum_{i}^{n} \alpha_{i}y_{i} \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right) x_{i}
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     - \left( \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \right) \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right)
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     - \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} \\    
&amp; = \min_{w, b} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} \\  
\end{alignat}
</span> Since we know that <span class="math inline">\alpha_{i}y_{i} = 0</span>, then <span class="math inline">b\sum_{i}^{n} \alpha_{i}y_{i} = 0</span>, and thus the final Lagrange dual function is: <span class="math display"> g(\alpha) = \sum_{i}^{n}\alpha_{i} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) </span></li>
<li>The Lagrange dual problem is written as: <span class="math display">
\begin{alignat}{2}
\max \quad &amp; \sum_{i}^{n}\alpha_{i} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\
\text{s.t. } \quad &amp; \alpha_{i} \geq 0, \quad i = 1, \dots n \\
\quad &amp; \alpha_{i}y_{i} = 0 \\
\end{alignat}
</span> Notice that <span class="math inline">\alpha_{i}y_{i} = 0</span> is added as part of the constraint.</li>
<li>Since strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:
<ol type="1">
<li>The Lagrange dual problem only involves <span class="math inline">\alpha_{i}</span>, but primal problem has <span class="math inline">w</span> and <span class="math inline">b</span>, which are much more parameters.</li>
<li>The Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.</li>
</ol></li>
</ol></li>
<li><strong>[Solving soft margin SVM]</strong>
<ol type="1">
<li>Similar as hard margin SVM, we can write Lagrangian dual function as: <span class="math display">
\begin{alignat}{2}
g(\alpha, \beta) &amp; = \min_{w, b} \frac{1}{2} ww
- \sum_{i}^{n} \alpha_{i}\left( y_{i}(w x_{i} + b) - 1 + \xi_{i} \right) - \sum_{i}^{n}\beta_{i}\xi_{i} \\
\end{alignat}
</span> where a new Lagrange multiplier is introduced for the constraint <span class="math inline">\xi_{i} \geq 0</span>.</li>
<li>Similar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the <span class="math inline">w</span>, <span class="math inline">b</span>, and <span class="math inline">\xi_i</span>: <span class="math display">
\begin{alignat}{2}
\frac{\partial L}{\partial w} = 0 &amp; \Rightarrow w - \sum_{i}{n} \alpha_{i}y_{i}x_{i} = 0 \Rightarrow w = \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \\
\frac{\partial L}{\partial b} = 0 &amp; \Rightarrow \sum_{i}^{n} \alpha_{i}y_{i} = 0 \\
\frac{\partial L}{\partial \xi_{i}} = 0 &amp; \Rightarrow C - \alpha_{i} - \beta_{i} = 0 \Rightarrow C = \alpha_{i} + \beta_{i} \\
\end{alignat}
</span> and plug the <span class="math inline">w = \sum_{i}^{n} \alpha_{i}y_{i}x_{i}</span> and <span class="math inline">C = \alpha_{i} + \beta_{i}</span> back in <span class="math inline">g(\alpha, \beta)</span>. <span class="math display">
\begin{alignat}{2}
g(\alpha, \beta)
&amp; = \min_{w, b} \frac{1}{2} ww + C\sum_{i}^{n}\xi_{i} - \sum_{i}^{n} \alpha_{i}\left( y_{i}(w x_{i} + b) - 1 + \xi_{i} \right) - \sum_{i}^{n}\beta_{i}\xi_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \left( \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \right) \left( \sum_{j}^{n} \alpha_{j}y_{j}x_{j} \right) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     - \sum_{i}^{n} \alpha_{i} \left( y_{i} \left( \left( \sum_{j}^{n} \alpha_{j}y_{j}x_{j} \right) x_{i} + b \right) - 1 + \xi_{i} \right) - \sum_{i}^{n} \beta_{i} \xi_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     - \sum_{i}^{n} \alpha_{i}y_{i}\left( \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right) x_{i} + b \right)
     + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     - \sum_{i}^{n} \alpha_{i}y_{i} \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right) x_{i}
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     - \left( \sum_{i}^{n} \alpha_{i}y_{i}x_{i} \right) \left( \sum_{j}^{n} \alpha_{j} y_{j} x_{j} \right)
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\
&amp; = \min_{w, b} \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     - \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j})
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\    
&amp; = \min_{w, b} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n}(\alpha_{i} + \beta_{i})\xi_{i}
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\  
&amp; = \min_{w, b} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \sum_{i}^{n} \alpha_{i}\xi_{i} + \sum_{i}^{n} \beta_{i}\xi_{i}
     + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} - \sum_{i}^{n} \alpha_{i} \xi_{i} - \sum_{i}^{n} \beta_{i} \xi_{i} \\  
&amp; = \min_{w, b} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\sum_{i}^{n} \alpha_{i}y_{i} + \sum_{i}^{n}\alpha_{i} \\
&amp; = \min_{w, b} \sum_{i}^{n}\alpha_{i}  - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\
\end{alignat}
</span> which has exactly the same form as Lagrangian dual function of hard margin SVM.</li>
<li>The Lagrange dual problem is written as: <span class="math display">
\begin{alignat}{2}
\max \quad &amp; \sum_{i}^{n}\alpha_{i} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\
\text{s.t. } \quad &amp; \alpha_{i} \geq 0, \quad i = 1, \dots n \\
\quad &amp; \beta_{i} \geq 0, \quad i = 1, \dots n \\
\quad &amp; \alpha_{i}y_{i} = 0 \\
\end{alignat}
</span> Since we know <span class="math inline">C = \alpha_{i} + \beta_{i} \Rightarrow \alpha_{i} = C - \beta_{i}</span>, the constraint <span class="math inline">\beta_{i} \geq 0</span> can be removed by merging into <span class="math inline">\alpha_{i} \geq 0</span>: <span class="math display">
\begin{alignat}{2}
\max \quad &amp; \sum_{i}^{n}\alpha_{i} - \frac{1}{2} \sum_{i}^{n}\sum_{j}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\
\text{s.t. } \quad &amp; C \geq \alpha_{i} \geq 0, \quad i = 1, \dots n \\
\quad &amp; \alpha_{i}y_{i} = 0 \\
\end{alignat}
</span> The only difference with Lagrange dual problem of hard margin SVM is the addition of <span class="math inline">C \geq \alpha_{i}</span>.</li>
</ol></li>
<li><strong>[Kernel trick]</strong>
<ol type="1">
<li>Kernel trick</li>
</ol></li>
<li><strong>[Duality and KKT conditions]</strong>
<ol type="1">
<li>The Lagrangian dual problem:
<ol type="1">
<li>Given a minimization primal problem: <span class="math display">
\begin{alignat}{2}
\min_{x} \quad &amp; f(x) \\
\text{s.t. } \quad &amp; h_{i}(x) \leq 0, \quad i = 1, \dots, n \\
\quad &amp; l_{j}(x) = 0, \quad j = 1, \dots, m \\
\end{alignat}
</span></li>
<li>The Lagrangian is defined as: <span class="math display"> L(x, u, v) = f(x) + \sum_{i}^{n} u_{i}h_{i}(x) + \sum_{j}^{m} v_{j}l_{j}(x) </span> where <span class="math inline">u_{i}</span> and <span class="math inline">v_{j}</span> are new variables called Lagrangian multipliers.</li>
<li>The Lagrange dual function is: <span class="math display"> g(u, v) = \min_{x} L(x, u, v) </span></li>
<li>The Lagrange dual problem is: <span class="math display">
\begin{alignat}{2}
\max_{u, v} \quad &amp; g(u, v) \\
\text{s.t. } \quad &amp; u \geq 0 \\
\end{alignat}
</span></li>
<li>The properties of dual problem:
<ol type="1">
<li>The dual problem is always convex even if the primal problem is not convex.</li>
<li>For any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).</li>
</ol></li>
</ol></li>
<li>Karush-Kuhn-Tucker (KKT) conditions
<ol type="1">
<li>Given the Lagrange dual problem stated above, the KKT conditions are:
<ol type="1">
<li>Stationarity condition: <span class="math display"> 0 \in \partial \left( f(x) + \sum_{i=1}^{n} u_{i} h_{i}(x) + \sum_{j=1}^{m} v_{j}l_{j}(x) \right) </span></li>
<li>Complementary Slackness: <span class="math display"> u_{i}h_{i}(x) = 0, \quad i = 1, \dots, n </span></li>
<li>Primal feasibility: <span class="math display"> h_{i}(x) \leq 0, \quad i = 1, \dots, n </span> <span class="math display"> l_{j}(x) = 0,  \quad j = 1, \dots, m </span></li>
<li>Dual feasibility: <span class="math display"> u_{i} \geq 0, \quad i = 1, \dots, n </span></li>
</ol></li>
<li>If a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the <span class="math inline">x^{*}</span> and <span class="math inline">u^{*}, v^{*}</span> are primal and dual solutions if and only if <span class="math inline">x^{*}</span> and <span class="math inline">u^{*}, v^{*}</span> satisfy the KKT conditions.</li>
</ol></li>
</ol></li>
</ol>
</section>
<section id="decision-tree-dt" class="level4">
<h4 class="anchored" data-anchor-id="decision-tree-dt">Decision Tree (DT)</h4>
<p>https://victorzhou.com/blog/intro-to-random-forests/ 1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. 2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. 3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.</p>
<p>Notes 1. <strong>[Gini Index and Information Entropy]</strong>: Both applies to a dataset (instances with labels) to measure its uncertainty. They both become 0 when there is only one class in the set.<br>
Gini Index (Gini impurity) measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. <span class="math display"> G(\mathcal{D}) = \sum_{c=1}^{C} \textrm{P}(c)(1-\textrm{P}(c)) = 1 - \sum_{c=1}^{C} \textrm{P}(c)^2 </span> Information Entropy can be roughly thought as the dataset’s variance. <span class="math display"> E(\mathcal{D}) = \sum_{c=1}^{C} \textrm{P}(c)\log_2\textrm{P}(c) </span> In both cases, <span class="math inline">\mathcal{D}</span> is the dataset to be evaluated, <span class="math inline">C</span> is the total number of classes in <span class="math inline">\mathcal{D}</span> and <span class="math inline">\textrm{P}(c)</span> is the probability of picking an instance with the class <span class="math inline">c</span> (fraction of instances with class <span class="math inline">c</span> in <span class="math inline">\mathcal{D}</span>). 2. <strong>[Gini Gain and Information Gain]</strong>: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset. <span class="math display"> G(\mathcal{D}, S) = M(\mathcal{D}) - \sum_{s\in S}\frac{\lvert s \rvert}{\lvert D \rvert} M(s)</span> where <span class="math inline">\mathcal{D}</span> is the dataset before splitting, <span class="math inline">S</span> are subsets of <span class="math inline">\mathcal{D}</span> created from all possible splitting of <span class="math inline">\mathcal{d}</span>, <span class="math inline">M</span> is Gini Index (<span class="math inline">G</span>) or Information Entropy (<span class="math inline">E</span>), and <span class="math inline">\lvert \cdot \rvert</span> gives the number of items in a set. 3. <strong>[Decision tree training algorithm]</strong>: We consider binary classification decision tree. Given a dataset <span class="math inline">\mathcal{D}</span>, 1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. 2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. 3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset. 4. The splitting stops when no further splitting can be made (the dataset contains only one class).</p>
</section>
<section id="random-forest-rf" class="level4">
<h4 class="anchored" data-anchor-id="random-forest-rf">Random Forest (RF)</h4>
<p>https://victorzhou.com/blog/intro-to-random-forests/ 1. Random Forest contains many decision trees and combine all their outputs to give a final decision. 2. A particular goal in training a random forest is to make each tree in the forest different from each other. First is to use bootstrapping, which means that each decision tree is trained on different dataset that is randomly sampled with replacement from the original dataset. Then to further inject randomness, random subset of the features instead of all features are considered in each split of the decision tree. Then the final output of random forest is to take the majority vote or average each output. 3. The goal of randomize the decision trees and taking the aggregation result is to reduce the variance and thus prevent over-fitting of the single decision tree. By taking an average of the random predictions, some errors can cancel out. Using multiple trees in the prediction make random forest a black box and the explanation for a prediction is hard to be understood by the users.</p>
<p>Notes: 1. <strong>[Bagging]</strong>: Bagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output.</p>
</section>
<section id="adaboost-ada" class="level4">
<h4 class="anchored" data-anchor-id="adaboost-ada">Adaboost (Ada)</h4>
<p>https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html<br>
https://arxiv.org/pdf/1403.1452.pdf 1. Adaboost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (&gt;50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. 2. Adaboost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. 3. Adaboost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner.</p>
<p>Notes: 1. <strong>[Adaboost algorithm]</strong> Here we show the adaboost algorithm for binary classification problems (<span class="math inline">y \in \{-1, 1\}</span>). 1. For the dataset with <span class="math inline">N</span> instances, initialize the observation weights for each instance <span class="math inline">w_i=\frac{1}{N}, i=1,2, ... ,N</span>. 2. For <span class="math inline">m = 1 ... M</span>, 1. Fit a classifier <span class="math inline">G_m(x)</span> to the training instances with weights <span class="math inline">w_i</span>. 2. Compute <span class="math display"> E_m=\frac{\sum_{i=1}^{N} w_i \mathcal{1}(y_i\neq G_m(x_i))}{\sum_{i=1}^{N}w_i} </span> 3. Compute <span class="math display"> \alpha_m = \log(\frac{1-E_m}{E_m}) </span> 4. Set <span class="math display"> w_i \gets w_i \cdot e^{\alpha_m y_i G_m(x_i)} </span> 3. Final output of Adaboost: <span class="math display"> G(x) = \textrm{sign} (\sum_{m=1}^M \alpha_m G_m(x)) </span></p>
</section>
<section id="gradient-boosting-gb" class="level4">
<h4 class="anchored" data-anchor-id="gradient-boosting-gb">Gradient Boosting (GB)</h4>
<ol type="1">
<li><p>Gradient boosting can be seen as the generalized version of boosting, i.e.&nbsp;Adaboost is one special case of gradient boosting. GB can be seen as</p></li>
<li><p>GB is a generalized additive model of n weak learners. <span class="math display"> G(x) = g_{1}(x) + \dots + g_{n}(x) </span> where <span class="math inline">G(x)</span> is the final gradient boosting model and <span class="math inline">g(x)</span> is one type of weak learners.</p></li>
<li><p>The weak learner <span class="math inline">g(x)</span> can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting.</p></li>
<li><p>$g_{1}(x) g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets.</p></li>
</ol>
<p>Given a loss function <span class="math inline">L(\cdot)</span>, a training set <span class="math inline">X = \{\mathbf{x_{i}}\}</span>, <span class="math inline">\mathbf{y} = \{y_{i}\}</span>, a learning rate <span class="math inline">\alpha</span>, and a number of iterations <span class="math inline">M</span>, the algorithm to train a GBRT is as follows: 1. Intiailize <span class="math inline">G(x)</span> by fitting CART on <span class="math inline">D</span> 1. For <span class="math inline">m = 1 \dots M</span>, 1. Evaluate the loss over the current <span class="math inline">G(x)</span> 1. Calculate the gradient of the loss w.r.t the labels to get the <strong>residuals</strong> <span class="math inline">\tilde{\mathbf{y}}</span>: <span class="math display"> \tilde{\mathbf{y}} = \frac{\partial L(G(X), \mathbf{y})}{\partial \mathbf{y}}</span> Note <span class="math inline">\tilde{\mathbf{y}}</span> has the same shape as <span class="math inline">\mathbf{y}</span>. 1. Use <span class="math inline">X</span> and <strong>residuals</strong> <span class="math inline">\tilde{\mathbf{y}}</span> as the new training set to train a CART <span class="math inline">g(x)</span>. 1. Add the new weak leaner into the current model: <span class="math display"> G(x) = G(x) + \alpha g(x) </span></p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>