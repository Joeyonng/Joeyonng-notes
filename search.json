[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joeyonng’s Notebook",
    "section": "",
    "text": "Symbol\nName\nDescription\n\n\n\n\nx\nScalar\nVariables are scalars (numbers).\n\n\n\\mathbf{x}\nVector\nBold non-capitalized variables are vectors.\n\n\n\\hat{\\mathbf{x}}\nUnit vector\nVectors that have a hat are unit vectors.\n\n\n\\mathbf{X}\nMatrix\nBold capitalized variables are matrices.\n\n\nX\nRandom variable\nCapitalized variables are random variables.\n\n\n\\mathcal{X}\nSet\nCalligraphic variables are sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{a} \\cdot \\mathbf{b}\nDot product\nDot product between vector \\mathbf{a} and \\mathbf{b} (same as \\mathbf{a}^{T} \\mathbf{b}).\n\n\n\\mathbf{A}\\mathbf{b}\nMatrix vector product\nMatrix product between matrix \\mathbf{A} and vector \\mathbf{b} (column matrix).\n\n\n\\mathbf{a}^{T}\nVector transpose\nThe transposed vector is a matrix of size 1 \\times d\n\n\n\\mathbf{A}^{T}\nMatrix transpose\nTranspose a matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{A}_{i, j}\nMatrix element selection\nSelect the scalar at row i and column j of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{i, *}\nMatrix row selection\nSelect the vector at row i of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{*, j}\nMatrix column selection\nSelect the vector at column j of matrix \\mathbf{A}.\n\n\n\\mathbf{a}_{i}\nVector element selection\nSelect the scalar at index i of vector \\mathbf{a}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbb{1}_{cond}\nConditional operator\nEvaluates to 1 if cond is true, 0 otherwise.\n\n\n\n\n\n\n\n\n\n\nProduct\n \\ln(xy) = \\ln(x) + \\ln(y) \nQuotient\n \\ln \\left( \\frac{x}{y} \\right) = \\ln(x) - \\ln(y) \nLog of power\n \\ln(x^{y}) = y \\ln(x) \nLog reciprocal\n \\ln \\left( \\frac{1}{x} \\right) = \\ln(1) - \\ln(x) = 0 - \\ln(x) = -\\ln(x) \n\n\n\n\n\nThe squared norm of vector \\mathbf{x}\n \\lVert \\mathbf{x} \\rVert^{2} = \\mathbf{x} \\cdot \\mathbf{x} = \\mathbf{x}^{T} \\mathbf{x}\nThe squared norm of a vector difference between \\mathbf{a} and \\mathbf{b}\n \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^{2} = (\\mathbf{a} - \\mathbf{b})^{T} (\\mathbf{a} - \\mathbf{b}) = \\mathbf{a}^{T}\\mathbf{a} - 2 \\mathbf{a}^T\\mathbf{b} + \\mathbf{b}^{T}\\mathbf{b} \nThe matrix form of the dot product between two vectors \\mathbf{a} and \\mathbf{b} with a coefficient \\lambda\n \\lambda(\\mathbf{a} \\cdot \\mathbf{b}) = \\mathbf{a}^{T} \\mathbf{\\Lambda} \\mathbf{b} \nwhere \\mathbf{\\Lambda} is a diagonal matrix with value \\lambda.\nThe transpose of the product of two matrices \\mathbf{A} and \\mathbf{B}\n (\\mathbf{A}\\mathbf{B})^{T} = \\mathbf{B}^{T}\\mathbf{A}^{T} \n\n\n\n\n\nThe derivative of the sigmoid function \\sigma is \\sigma (1 - \\sigma)\n\n\\begin{align}\n\\frac{\\partial \\sigma}{\\partial x} & = \\frac{\\partial}{\\partial x} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{\\partial}{\\partial x} (1 + e^{-x})^{-1} \\\\\n& = -(1 + e^{-x})^{-2} \\times -e^{-x} \\\\\n& = \\frac{e^{-x}}{(1 + e^{-x})^{2}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( \\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( 1 - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\sigma (1 - \\sigma) \\\\\n\\end{align}"
  },
  {
    "objectID": "index.html#notations",
    "href": "index.html#notations",
    "title": "Joeyonng’s Notebook",
    "section": "",
    "text": "Symbol\nName\nDescription\n\n\n\n\nx\nScalar\nVariables are scalars (numbers).\n\n\n\\mathbf{x}\nVector\nBold non-capitalized variables are vectors.\n\n\n\\hat{\\mathbf{x}}\nUnit vector\nVectors that have a hat are unit vectors.\n\n\n\\mathbf{X}\nMatrix\nBold capitalized variables are matrices.\n\n\nX\nRandom variable\nCapitalized variables are random variables.\n\n\n\\mathcal{X}\nSet\nCalligraphic variables are sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{a} \\cdot \\mathbf{b}\nDot product\nDot product between vector \\mathbf{a} and \\mathbf{b} (same as \\mathbf{a}^{T} \\mathbf{b}).\n\n\n\\mathbf{A}\\mathbf{b}\nMatrix vector product\nMatrix product between matrix \\mathbf{A} and vector \\mathbf{b} (column matrix).\n\n\n\\mathbf{a}^{T}\nVector transpose\nThe transposed vector is a matrix of size 1 \\times d\n\n\n\\mathbf{A}^{T}\nMatrix transpose\nTranspose a matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbf{A}_{i, j}\nMatrix element selection\nSelect the scalar at row i and column j of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{i, *}\nMatrix row selection\nSelect the vector at row i of matrix \\mathbf{A}.\n\n\n\\mathbf{A}_{*, j}\nMatrix column selection\nSelect the vector at column j of matrix \\mathbf{A}.\n\n\n\\mathbf{a}_{i}\nVector element selection\nSelect the scalar at index i of vector \\mathbf{a}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nName\nDescription\n\n\n\n\n\\mathbb{1}_{cond}\nConditional operator\nEvaluates to 1 if cond is true, 0 otherwise."
  },
  {
    "objectID": "index.html#cheat-sheet",
    "href": "index.html#cheat-sheet",
    "title": "Joeyonng’s Notebook",
    "section": "",
    "text": "Product\n \\ln(xy) = \\ln(x) + \\ln(y) \nQuotient\n \\ln \\left( \\frac{x}{y} \\right) = \\ln(x) - \\ln(y) \nLog of power\n \\ln(x^{y}) = y \\ln(x) \nLog reciprocal\n \\ln \\left( \\frac{1}{x} \\right) = \\ln(1) - \\ln(x) = 0 - \\ln(x) = -\\ln(x) \n\n\n\n\n\nThe squared norm of vector \\mathbf{x}\n \\lVert \\mathbf{x} \\rVert^{2} = \\mathbf{x} \\cdot \\mathbf{x} = \\mathbf{x}^{T} \\mathbf{x}\nThe squared norm of a vector difference between \\mathbf{a} and \\mathbf{b}\n \\lVert \\mathbf{a} - \\mathbf{b} \\rVert^{2} = (\\mathbf{a} - \\mathbf{b})^{T} (\\mathbf{a} - \\mathbf{b}) = \\mathbf{a}^{T}\\mathbf{a} - 2 \\mathbf{a}^T\\mathbf{b} + \\mathbf{b}^{T}\\mathbf{b} \nThe matrix form of the dot product between two vectors \\mathbf{a} and \\mathbf{b} with a coefficient \\lambda\n \\lambda(\\mathbf{a} \\cdot \\mathbf{b}) = \\mathbf{a}^{T} \\mathbf{\\Lambda} \\mathbf{b} \nwhere \\mathbf{\\Lambda} is a diagonal matrix with value \\lambda.\nThe transpose of the product of two matrices \\mathbf{A} and \\mathbf{B}\n (\\mathbf{A}\\mathbf{B})^{T} = \\mathbf{B}^{T}\\mathbf{A}^{T} \n\n\n\n\n\nThe derivative of the sigmoid function \\sigma is \\sigma (1 - \\sigma)\n\n\\begin{align}\n\\frac{\\partial \\sigma}{\\partial x} & = \\frac{\\partial}{\\partial x} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{\\partial}{\\partial x} (1 + e^{-x})^{-1} \\\\\n& = -(1 + e^{-x})^{-2} \\times -e^{-x} \\\\\n& = \\frac{e^{-x}}{(1 + e^{-x})^{2}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\frac{1}{1 + e^{-x}} \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( \\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\frac{e^{-x}}{1 + e^{-x}} \\left( 1 - \\frac{e^{-x}}{1 + e^{-x}} \\right) \\\\\n& = \\sigma (1 - \\sigma) \\\\\n\\end{align}"
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html",
    "href": "Knowledge/Calculus/Derivative.html",
    "title": "Functions",
    "section": "",
    "text": "TODO\nNeed to mention that the following notes are always assuming the function is continuous at all points."
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html#scalar-valued-and-vector-valued-functions",
    "href": "Knowledge/Calculus/Derivative.html#scalar-valued-and-vector-valued-functions",
    "title": "Functions",
    "section": "Scalar-valued and vector-valued functions",
    "text": "Scalar-valued and vector-valued functions\nA scalar-valued function f outputs a scalar, while a vector-valued function \\mathbf{f} outputs a vector.\nA vector-valued function can be interpreted as a vector of scalar-valued functions\n\n\\mathbf{f} =\n\\begin{bmatrix}\nf_{1} \\\\\n\\vdots \\\\\nf_{m} \\\\\n\\end{bmatrix}."
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html#univariate-and-multivariate-functions",
    "href": "Knowledge/Calculus/Derivative.html#univariate-and-multivariate-functions",
    "title": "Functions",
    "section": "Univariate and multivariate functions",
    "text": "Univariate and multivariate functions\nA univariate function takes a scalar x as the input, while a multivariate function takes a vector \\mathbf{x} as the input"
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html#ordinary-univariate-derivatives",
    "href": "Knowledge/Calculus/Derivative.html#ordinary-univariate-derivatives",
    "title": "Functions",
    "section": "Ordinary (univariate) Derivatives",
    "text": "Ordinary (univariate) Derivatives\nThe derivative function of a univariate scalar-valued function f, denoted \\frac{d f}{d x} or f', is another univariate scalar-valued function that takes x as the input and outputs the derivative of the function at point x:\n\nf' (x) = \\frac{d f}{d x} (x) = \\lim_{h \\rightarrow 0} \\frac{f (x + h) - f (x)}{h}.\n\nThe derivative of f at point x is the slope of the tangent line to the function f at the point x, which also represents the rate of the change of f at x in the positive (increasing) direction.\n:::{prf:proof} Derivative as the rate of change :class: dropdown\nA line has the form of\n\ny = k x + b\n\nwhere k is the slope of the line and b represents the interception of the line on the y axis.\nWe can calculate the slope of the line if we know the two points (x_{1}, y_{1}) and (x_{2}, y_{2}) that are on the line\n\n\\begin{aligned}\ny_{2} - y_{1}\n& = k x_{2} + b - k x_{1} + b\n\\\\\n& = k (x_{2} - x_{1})\n\\\\\nk\n& = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}}.\n\\end{aligned}\n\nThus, if we set x' = x + h, the equation\n\n\\frac{f (x + h) - f (x)}{h} = \\frac{f (x') - f(x)}{x' - x}\n\nrepresents the the slope of the line connecting the points (x', f (x')) and (x, f(x)).\nAs h getting smaller, the line connecting the points (x', f (x')) and (x, f(x)) is becoming the tangent line of f(x) at point x. :::"
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html#multivariate-derivatives",
    "href": "Knowledge/Calculus/Derivative.html#multivariate-derivatives",
    "title": "Functions",
    "section": "Multivariate Derivatives",
    "text": "Multivariate Derivatives\n\nDirectional derivatives\nGiven a non-zero vector \\mathbf{u} \\in \\mathbb{R}^{n}, the directional derivative function of a scalar-valued multivariate function f, denoted D_{\\mathbf{u}} f, is another scalar-valued multivariate function that takes \\mathbf{x} \\in \\mathbb{R}^{n} as the input and outputs the directional derivative of the function with respect to \\mathbf{u} at point \\mathbf{x}:\n\nD_{\\mathbf{u}} f (\\mathbf{x}) = \\lim_{h \\rightarrow 0} \\frac{f (\\mathbf{x} + h \\mathbf{u}) - f (\\mathbf{x})}{h \\lVert \\mathbf{u} \\rVert}.\n\nIf the given vector \\mathbf{u} is a unit vector (\\lVert \\mathbf{u} \\rVert = 1), the directional derivative function is simplified to\n\nD_{\\mathbf{u}} f (\\mathbf{x}) = \\lim_{h \\rightarrow 0} \\frac{f (\\mathbf{x} + h \\mathbf{u}) - f (\\mathbf{x})}{h}.\n\nThe directional derivative of f with respect to the vector \\mathbf{u} at the point \\mathbf{x} represents the rate of change of f at \\mathbf{x} in the positive direction of \\mathbf{u}.\n\nReparameterization of directional derivatives\nGiven the vectors \\mathbf{x} and \\mathbf{u}, the univariate function\n\ng (\\epsilon) = f (\\mathbf{x} + \\epsilon \\mathbf{u})\n\ndescribes the values of f along a particular line that starts from the point \\mathbf{x} and goes in the direction of the vector \\mathbf{u}.\nThe derivative of function g at point \\epsilon = 0 gives the rate of change of g at point 0 and also the rate of change of f at \\mathbf{x} in the direction of \\mathbf{u}, which is exactly the directional derivative:\n\nD_{u} f (\\mathbf{x}) \\lVert \\mathbf{u} \\rVert = \\frac{d}{d \\epsilon} g (\\epsilon) \\Big|_{\\epsilon = 0} = \\frac{d}{d \\epsilon} f (\\mathbf{x} + \\epsilon \\mathbf{u}) \\Big|_{\\epsilon = 0}.\n\nThis observation provides an alternative way to compute a directional derivative that only requires computing an ordinary derivative.\n:::{prf:proof} Reparameterization of directional derivative :class: dropdown\n\n\\begin{aligned}\n\\frac{d g}{d \\epsilon} (\\epsilon) \\Big|_{\\epsilon = 0}\n& = \\lim_{h \\rightarrow 0} \\frac{c (\\epsilon + h) - c (\\epsilon)}{h} \\Big|_{\\epsilon = 0}\n\\\\\n& = \\lim_{h \\rightarrow 0} \\frac{f (\\mathbf{x} + (\\epsilon + h) \\mathbf{u}) - f (\\mathbf{x} + \\epsilon \\mathbf{u})}{h} \\Big|_{\\epsilon = 0}\n\\\\\n& = \\lim_{h \\rightarrow 0} \\frac{f (\\mathbf{x} + h \\mathbf{u}) - f (\\mathbf{x})}{h}\n\\\\\n& = D_{\\mathbf{u}} f (\\mathbf{x}) \\lVert \\mathbf{u} \\rVert.\n\\end{aligned}\n\n:::\n\n\n\nPartial derivatives\nThe partial derivative function of a multivariate function f with respect to a dimension i, denoted as \\frac{\\partial f}{\\partial x_{i}}, is a special case of the directional derivative function, where the given vector \\mathbf{u} must be a standard basis vector \\mathbf{e}_{i} of the dimension i:\n\n\\frac{\\partial f}{\\partial x_{i}} (\\mathbf{x}) = D_{\\mathbf{e}_{i}} f (\\mathbf{x}) = \\lim_{h \\rightarrow 0} \\frac{f (\\mathbf{x} + h \\mathbf{e}_{i}) - f (\\mathbf{x})}{h}.\n\nThe partial derivative of f with respect to \\mathbf{e}_{i} at \\mathbf{x} gives the rate of the change of the function at \\mathbf{x} along the direction of the ith dimension of the vector space.\nThe partial derivative \\frac{\\partial f}{\\partial x_{i}} (\\mathbf{x}) can be calculated using the orindary derivative\n\n\\frac{\\partial f}{\\partial x_{i}} (\\mathbf{x}) = \\frac{d f^{\\mathbf{x}}}{d x_{i}} (x_{i})\n\nwhere f^{\\mathbf{x}} is a univariate function obtained by setting all variables except x_{i} in f as constants\n\nf^{\\mathbf{x}} (x_{i}) = f (\\mathbf{x})\n\n\n\nGradients\nThe gradient function of a multivariate function f, denoted as \\nabla f, is a vector-valued function, where each element in the output vector is the partial derivative of f with respect to all standard basis vectors\n\n\\nabla f (\\mathbf{x}) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_{1}} (\\mathbf{x}) \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_{n}} (\\mathbf{x})\n\\end{bmatrix}.\n\nThe directional derivative with respect to any non-zero vector \\mathbf{u} of function f at point x can be readily calculated using its gradient as follows:\n\nD_{\\mathbf{u}} f (\\mathbf{x}) = \\nabla f (\\mathbf{x}) \\cdot \\frac{\\mathbf{u}}{\\lVert \\mathbf{u} \\rVert}.\n\n:::{prf:proof} Directional derivative calculated using gradient :class: dropdown\nUsing the Reparameterization of directional derivative, we can write the directional derivative as\n\nD_{\\mathbf{u}} f (\\mathbf{x}) \\lVert \\mathbf{u} \\rVert = \\frac{d f}{d \\epsilon} (\\mathbf{x} + \\epsilon \\mathbf{u}) \\Big|_{\\epsilon = 0}.\n\nBy using the chain rule and writing \\mathbf{g} (\\epsilon) = \\mathbf{x} + \\epsilon \\mathbf{u}, we can also see that the derivative of the univariate function f (\\mathbf{x} + \\epsilon \\mathbf{u}) at point \\epsilon = 0 can be written as\n\n\\begin{aligned}\n\\frac{d f}{d \\epsilon} (\\mathbf{x} + \\epsilon \\mathbf{u}) \\Big|_{\\epsilon = 0}\n& = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial g_{i} (\\epsilon)} (\\mathbf{g} (\\epsilon)) \\frac{d g_{i}}{d \\epsilon} (\\epsilon) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial (x_{i} + \\epsilon u_{i})} (\\mathbf{x} + \\epsilon \\mathbf{u}) \\frac{d (x_{i} + \\epsilon u_{i})}{d \\epsilon} (\\epsilon) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial (x_{i} + \\epsilon u_{i})} (\\mathbf{x} + \\epsilon \\mathbf{u}) u_{i} \\Big|_{\\epsilon = 0}\n\\\\\n& = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial x_{i}} (\\mathbf{x}) u_{i}\n\\\\\n& = \\nabla f (\\mathbf{x}) \\cdot \\mathbf{u}.\n\\end{aligned}\n\nThus,\n\n\\begin{aligned}\nD_{\\mathbf{u}} f (\\mathbf{x}) = \\nabla f (\\mathbf{x}) \\cdot \\frac{\\mathbf{u}}{\\lVert \\mathbf{u} \\rVert}.\n\\end{aligned}\n\n:::\nIt turns out that the directional derivative of function f at point \\mathbf{x} with respect to the direction of the gradient vector is the largest. That is, the gradient of f at point \\mathbf{x} is the direction of steepest ascent along the function surface at the point \\mathbf{x}\n\n\\nabla f (\\mathbf{x}) = \\arg\\max_{\\mathbf{u}} D_{\\mathbf{u}} f(x).\n\n:::{prf:proof} Gradient as the steepest ascent direction :class: dropdown\nAccording to the definition of a dot product,\n\n\\begin{aligned}\nD_{\\mathbf{u}} f (\\mathbf{x})\n& = \\frac{\n\\nabla f (\\mathbf{x}) \\cdot \\mathbf{u}}{\\lVert \\mathbf{u} \\rVert}\n\\\\\n& = \\frac{\\lVert \\nabla f (\\mathbf{x}) \\rVert \\lVert \\mathbf{u} \\rVert \\cos (\\theta)}{\\lVert \\mathbf{u} \\rVert}\n\\\\\n& = \\lVert \\nabla f (\\mathbf{x}) \\rVert \\cos (\\theta)\n\\end{aligned}\n\nwhere \\theta is the angle between the vector \\nabla f(x) and \\mathbf{u}.\nSince \\lVert \\nabla f (\\mathbf{x}) \\rVert is fixed given \\mathbf{x},\n\n\\max_{\\mathbf{u}} D_{\\mathbf{u}} f (\\mathbf{x}) = \\max_{\\theta} \\cos (\\theta),\n\nwhich is maximized when \\mathbf{u} has the same direction as \\nabla f (\\mathbf{x}). :::\nThe directional derivative of the function f with respect to the direction of the gradient vector \\nabla f (\\mathbf{x}) at point \\mathbf{x} is the magnitude of the gradient vector\n\nD_{\\nabla f (\\mathbf{x})} f (\\mathbf{x}) = \\lVert \\nabla f (\\mathbf{x}) \\rVert.\n\n:::{prf:proof} The magnitude of the gradient vector gives the rate of the change along the steepest ascent direction :class: dropdown\nReplacing the \\mathbf{u} with \\nabla f (\\mathbf{x}) in the equation\n\nD_{\\mathbf{u}} f (\\mathbf{x}) = \\nabla f (\\mathbf{x}) \\cdot \\frac{\\mathbf{u}}{\\lVert \\mathbf{u} \\rVert}\n\nto get\n\n\\begin{aligned}\nD_{\\nabla f (\\mathbf{x})} f (\\mathbf{x})\n& = \\nabla f (\\mathbf{x}) \\cdot \\frac{\\nabla f (\\mathbf{x})}{\\lVert \\nabla f (\\mathbf{x}) \\rVert}\n\\\\\n& = \\frac{\\lVert \\nabla f (\\mathbf{x}) \\rVert^{2}}{\\lVert \\nabla f (\\mathbf{x}) \\rVert}\n\\\\\n& = \\lVert \\nabla f (\\mathbf{x}) \\rVert.\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Calculus/Derivative.html#function-of-functions",
    "href": "Knowledge/Calculus/Derivative.html#function-of-functions",
    "title": "Functions",
    "section": "Function of functions",
    "text": "Function of functions\nA function f itself can be viewed as an infinite-dimensional vector\n\nD_{u} F [f] = \\lim_{h \\rightarrow 0} \\frac{F [f + h u] - F [f]}{h}\n\nUsing a similar trick used in equation , we can see that\n\n\\begin{aligned}\nD_{u} F [f]\n& = D_{u} F [f + \\epsilon u] \\Big|_{\\epsilon = 0}\n\\\\\n& = \\lim_{h \\rightarrow 0} \\frac{F [f + \\epsilon u + h u] - F [f + \\epsilon u]}{h} \\Big|_{\\epsilon = 0}\n\\\\\n& = \\lim_{h \\rightarrow 0} \\frac{F [f + (\\epsilon + h) u] - F [f + \\epsilon u]}{h} \\Big|_{\\epsilon = 0}\n\\\\\n& = \\frac{d}{d \\epsilon} F [f + \\epsilon u] \\Big|_{\\epsilon = 0}\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html",
    "href": "Knowledge/Information Theory/ECE 255A notes.html",
    "title": "Lecture 1",
    "section": "",
    "text": "Alphabet x \\in \\mathcal{X}: collection of symbols\nSize of the alphabet \\lvert \\mathcal{X} \\rvert\n\n\n\nA string is a sequence of symbols \\bar{x}\nEmpty string \\Lambda"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#language",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#language",
    "title": "Lecture 1",
    "section": "",
    "text": "Alphabet x \\in \\mathcal{X}: collection of symbols\nSize of the alphabet \\lvert \\mathcal{X} \\rvert\n\n\n\nA string is a sequence of symbols \\bar{x}\nEmpty string \\Lambda"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#single-variable-zero-error-and-worst-case",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#single-variable-zero-error-and-worst-case",
    "title": "Lecture 1",
    "section": "Single variable, zero error and worst case:",
    "text": "Single variable, zero error and worst case:\n\n\\hat{L}_{c} (x) = \\max_{x \\in \\mathcal{X}} \\lvert C (\\mathcal{X}) \\rvert\n\n\n\\hat{L} (x) = \\min_{c} \\hat{L}_{c} (x)\n\n\n\\hat{L} (x) = \\lceil \\log_{2} \\lvert \\mathcal{X} \\rvert \\rceil\n\nProve by proving both \\geq and \\leq cases.\nThe number of binary sequences of length n is 2^{n}.\nthe largest number of prefix-free sequences of length \\leq n is 2^{n}"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#single-variable-zero-error-and-average-case",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#single-variable-zero-error-and-average-case",
    "title": "Lecture 1",
    "section": "Single variable, zero error and average case:",
    "text": "Single variable, zero error and average case:\n\n\\bar{L}_{c} (x) = \\mathbb{E} \\lvert C (x) \\rvert = \\sum_{x \\in \\mathcal{X}} p (x) \\lvert C (x) \\rvert\n\n\n\\bar{L} (x) = \\min_{\\text{valid} c} \\bar{L}_{c} (x)\n\nThe length to encode a x should be related how often appears in the alphabats.\nThe length of encoding x is related to the probability x\n\n\\lvert C (x) \\rvert = \\log_{2} \\frac{1}{p (x)}\n\nEntropy:\n\n\\bar{L} (x) = \\sum_{x \\in \\mathcal{X}} p (x) \\lvert C (x) \\rvert = \\sum_{x \\in \\mathcal{X}} p (x) \\log \\frac{1}{p (x)} = H (x)\n\n\nH (x) = \\bar{L} (x) \\leq \\hat{L} (x) = \\lceil \\log \\lvert \\mathcal{X} \\rvert \\rceil\n\n\nH (x) \\leq \\log \\lvert \\mathcal{X} \\rvert\n\nHow to prove this?"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#jensens-inequality",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#jensens-inequality",
    "title": "Lecture 1",
    "section": "Jensen’s inequality",
    "text": "Jensen’s inequality\nIf f is a concave function, then\n\n\\mathbb{E} (f (x)) \\leq f (\\mathbb{E} (x))\n\n\nProof: H (x) \\leq \\log \\lvert \\mathcal{X} \\rvert\nIf f'' (x) \\leq 0 over I, then f is concave over I\nIf f = \\log x, since f' = \\frac{1}{x} and f'' = \\frac{1}{x^{2}}, f is concave on (0, \\infty)\n\n\\begin{aligned}\nH (x)\n& = \\sum_{i} p_{i} \\log \\frac{1}{p_{i}}\n\\\\\n& \\leq \\log \\sum_{i}^{n} p_{i} \\frac{1}{p_{i}}\n\\\\\n& \\leq \\log n\n\\end{aligned}\n\nH (x) = \\log n if and only if p_{i} = p = \\frac{1}{n}.\nGiven l_{1}, l_{2}, \\dots, l_{n}, are there prefix-free sequences of these lengths?\nWe know\n\nl_{i} = \\log \\frac{1}{p_{i}} = - \\log p_{i} \\\\\n\\log p_{i} = -l_{i} \\\\\np_{i} = 2^{- l_{i}}\n\n\n\nKraft’s inequality\nThere exists prefix-free code words of lengths l_{1}, \\dots l_{n} if and only if\n\n\\sum p_{i} = \\sum 2^{-l_{i}} \\leq 1\n\nSuppose l_{1}, \\dots, l_{n} are lengths of prefix-free code words. Show that \\sum 2^{- l_{i}} \\leq 1.\nUsing binary tree: define the L = \\max_{i} l_{i} and project the node of l_{i} to level L as S_{i}\nBecause we assume prefix free, S_{i} are disjoint free,\n\n\\bigcup S_{i} \\subseteq \\{0, 1\\}^{L} \\\\\n\\lvert \\bigcup S_{i} \\rvert \\leq 2^{L} \\\\\n\\sum \\lvert S_{i} \\rvert = \\sum 2^{L - l{i}} \\\\\n\\sum 2^{- l_{i}} \\leq 1\n\n\n\nShow \\bar{L} (x) &lt; H (x) + 1\nGiven p_{1}, \\dots, p_{n}, l_{i} = \\lceil \\log \\frac{1}{p_{i}} \\rceil\n\n\\sum 2^{- l_{i}} = \\sum 2^{-\\lceil \\log \\frac{1}{p_{i}} \\rceil} \\leq \\sum 2^{\\log \\frac{1}{p_{i}}} = \\sum 2^{\\log p_{i}} = \\sum p_{i} = 1\n\nTherefore there exists codewods of lengths \\lceil \\log \\frac{1}{p_{i}} \\rceil.\nCall such codes Shannon codes\n\n\\bar{L}_{c} (x) = \\sum p_{i} \\lceil \\log \\frac{1}{p_{i}} \\rceil &lt; \\sum p_{i} (\\log \\frac{1}{p_{i}} + 1) = \\sum p_{i} \\log \\frac{1}{p_{i}} + 1 = H (x) + 1\n\n\n\nShow for all valid codes c: L_{c} (x) \\geq H (x)\nLet c be a valid code, let l_{i} (x) be the number of bits for encoding x\n\nH (x) - \\hat{L}_{c} (x) = \\sum_{x} p(x) \\log \\frac{1}{p (x)} - \\sum_{x} p (x) l (x) = \\sum_{x} p (x) (\\log \\frac{1}{p (x)} + \\log 2^{-l (x)}) \\leq \\log \\sum p (x) \\frac{2^{-l (x)}}{p(x)} = \\log \\sum 2^{- l(x)} \\leq \\log 1\n\nTherefore,\n\nH (x) \\leq \\bar{L} (x) &lt; H (x)  + 1\n\n\n\nMore about entropy\nEntropy for a binary distribution with p\nEntropy: the expected surprise"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#entropy-of-functions",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#entropy-of-functions",
    "title": "Lecture 1",
    "section": "Entropy of functions",
    "text": "Entropy of functions\nA function f defeind over \\mathcal{X}. What is the relation between H (x) and H (f (x))?\nIf and only if f is a one-to-one function,\n\nH (x) \\geq H (f (x))"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#grouping-properties",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#grouping-properties",
    "title": "Lecture 1",
    "section": "Grouping properties",
    "text": "Grouping properties\n\nH (p_{1}, \\dots, p_{k}, q_{1}, \\dots, q_{n}) = H (p, q) + p H (\\frac{p_{1}}{p}, \\dots, \\frac{p_{k}}{p}) + q H (\\frac{q_{1}}{q}, \\dots, \\frac{q_{k}}{q})\n\nwhere p = \\sum_{i = 1}^{k} = p_{i} and q = \\sum_{i = 1}^{n} = q_{i}"
  },
  {
    "objectID": "Knowledge/Information Theory/ECE 255A notes.html#encoding-block-encodes",
    "href": "Knowledge/Information Theory/ECE 255A notes.html#encoding-block-encodes",
    "title": "Lecture 1",
    "section": "Encoding block encodes",
    "text": "Encoding block encodes\n\nWorst case\nEncoding C_{n}: \\mathcal{X}^{n} \\to \\{0, 1\\}^{*}.\nDecoding D_{n}: \\{0, 1\\}^{*} \\to \\mathcal{X}^{n}.\n\n\\hat{L}_{n, C_{n}} (x) = \\max \\{\\lvert C_{n} (\\mathcal{x}^{n}) \\rvert: x^{n} \\in \\mathcal{X}^{n} \\}\n\n\n\\hat{L}_{n} (x) = \\min_{C_{n}} \\hat{L}_{n, C_{n}}\n\n\n\\hat{L}^{*} (x) = \\lim_{n \\to \\infty} \\frac{1}{n} \\hat{L}_{n} (x)\n\n\n\\hat{L}_{n} (x) = \\lceil \\log \\lvert x^{n} \\rvert \\rceil = \\lceil \\log \\lvert x \\rvert^{n} \\rceil \\rceil = \\lceil n \\log \\lvert x \\rvert \\rceil\n\n\n\\hat{L}^{*} (x) = \\lim_{n \\to \\infty} \\frac{\\lceil n \\log \\lvert x \\rvert \\rceil}{n} = \\log \\lvert x \\rvert\n\n\n\\bar{L}_{n, C_{n}} (x) = \\sum_{x^{n}} p (x^{n}) \\lvert C_{n} (x^{n}) \\rvert\n\n\n\\bar{L}_{n} (x) = \\min_{C_{n}} \\bar{L}_{n, C_{n}} (x)\n\n\n\\bar{L}^{*} (x) = \\lim_{n \\to \\infty} \\frac{1}{n} \\bar{L}_{n} (x)\n\n\nH (x^{n}) \\leq \\bar{L}_{n} (x) &lt; H (x^{n}) + 1\n\n\nn H (x) \\leq \\bar{L}_{n} (x) &lt; n H (x^{n}) + 1\n\n\n\\lim n H (x) \\leq \\bar{L}^{*} (x) &lt; n H (x^{n}) + 1\n\n\n\nEntropy of random vectors\n\nH (X, Y) = \\sum_{x, y} p (x, y) \\log \\frac{1}{p (x, y)}\n\nIf X and Y are independent,\n\nH (x, y) = \\sum_{x, y} p (x, y) \\log \\frac{1}{p (x, y)} = \\sum_{x, y} p (x, y) \\log \\frac{1}{p (x) p (y)} = \\sum_{x, y} p (x, y) \\log \\frac{1}{p (x)} + \\sum_{x, y} p (x, y) \\log \\frac{1}{p (y)} = \\sum_{x} p (x) \\log \\frac{1}{p (x)} + \\sum_{y} p (y) \\log \\frac{1}{p (y)} = H (x) + H (y)"
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html",
    "title": "Vectors",
    "section": "",
    "text": "Let \\mathbb{F} be a field. The vector space \\mathbb{F}^{n} is defined as the set of all tuples (ordered lists) that have n field elements of \\mathbb{F}\n\\mathbb{F}^{n} =\n\\left\\{\n\\begin{bmatrix}\n    x_{1} \\\\\n    \\vdots \\\\\n    x_{n} \\\\\n\\end{bmatrix},\nx_{i} \\in \\mathbb{F}\n\\right\\},\nwhere each element member is called a n dimensional column vector."
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#matrix-vector-multiplication",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#matrix-vector-multiplication",
    "title": "Vectors",
    "section": "Matrix-vector multiplication",
    "text": "Matrix-vector multiplication\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}, the matrix-vector multiplication is a function that maps from vector space \\mathbf{x} \\in \\mathbb{F}^{n} to \\mathbf{y} \\in \\mathbb{F}^{m}\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x} =\n\\begin{bmatrix}\n    y_{1} = a_{1, 1} \\cdot x_{1} + \\dots a_{1, n} \\cdot x_{n} \\\\\n    \\vdots \\\\\n    y_{m} = a_{m, 1} \\cdot x_{1} + \\dots a_{m, n} \\cdot x_{n} \\\\\n\\end{bmatrix}.\n\nIf we view \\mathbf{A} as n columns of \\mathbb{F}^{m} vectors,\n\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n} \\\\\n\\end{bmatrix}\n\nthe vector \\mathbf{y} can be interpreted as the linear combinations of columns of \\mathbf{A} with elements of \\mathbf{x} as coefficients\n\n\\mathbf{y} = \\sum_{i=1}^{n} x_{i} \\mathbf{a}_{i}.\n\n(matrix-multiplication)="
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#matrix-multiplication",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#matrix-multiplication",
    "title": "Vectors",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nGiven two matrices \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and \\mathbf{B} \\in \\mathbb{F}^{n \\times r}, the matrix multiplication is a function that applies vector matrix multiplication on the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n} and the vector \\mathbf{b}_{i} \\in \\mathbb{F}^{n} (ith column of \\mathbf{B})\n\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} =\n\\begin{bmatrix}\n\\mathbf{c}_{1} = \\mathbf{A} \\mathbf{b}_{1} & \\dots & \\mathbf{c}_{r} = \\mathbf{A} \\mathbf{b}_{r} \\\\\n\\end{bmatrix},\n\nwhich results in a vector \\mathbf{c}_{i} \\in \\mathbb{F}^{m} as the ith column of the \\mathbf{C}.\n\nThe column i of \\mathbf{C} is a linear combination of columns of \\mathbf{A} using the elements of the column i of \\mathbf{B} as coefficients.\nThe row i of \\mathbf{C} is a linear combination of rows of \\mathbf{B} using the elements of row i of \\mathbf{A} as coefficients."
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#null-space",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#null-space",
    "title": "Vectors",
    "section": "Null space",
    "text": "Null space\nThe null space of the matrix \\mathbf{A} is the set\n\nN (\\mathbf{A}) = \\left\\{\n    \\mathbf{x} \\in \\mathbb{F}^{n} \\mid \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\in \\mathbb{F}^{m}\n\\right\\},\n\nwhich is the set of the vectors in \\mathbb{F}^{n} that is mapped to 0 \\in \\mathbb{F}^{m} by matrix \\mathbf{A}.\n:::{prf:proof} N (\\mathbf{A}) is a subspace of \\mathbb{F}^{n} :class: dropdown\nSince \\mathbf{A} 0 = 0 \\in \\mathbb{F}^{n}, \\mathbf{x} = 0 \\in N (A). Thus N (A) is not empty.\nConsider \\mathbf{x}_{1}, \\mathbf{x}_{2} \\in N (A). According to the definition of the N (A), \\mathbf{A} \\mathbf{x}_{1} = 0 and \\mathbf{A} \\mathbf{x}_{2} = 0.\nThen,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n& = 0\n\\\\\n\\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in N (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n& = \\alpha \\cdot 0\n\\\\\n\\mathbf{A} (\\alpha \\cdot \\mathbf{x}_{1})\n& = 0.\n\\end{aligned}\n\nThus N (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in N (\\mathbf{A}).\n\nThus, N (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#range-image-space",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#range-image-space",
    "title": "Vectors",
    "section": "Range (image) space",
    "text": "Range (image) space\nThe range (image) space of the matrix \\mathbf{A} is the set\n\nR (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\},\n\nwhich is the set of vectors in \\mathbb{F}^{m} that can be mapped from \\mathbb{F}^{n} by matrix \\mathbf{A}.\n:::{prf:proof} R (\\mathbf{A}) is a subspace of \\mathbb{F}^{m} :class: dropdown\nSince \\mathbf{x} = 0 \\in \\mathbb{F}^{n}, \\mathbf{y} = 0 \\in R (A). Thus R (A) is not empty.\nConsider \\mathbf{y}_{1}, \\mathbf{y}_{2} \\in R (A). According to the definition of the R (A), there exists an \\mathbf{x}_{1} \\in \\mathbb{F}^{m} for \\mathbf{y}_{1} and an \\mathbf{x}_{2} \\in \\mathbb{F}^{m} for \\mathbf{y}_{2}.\nThen,\n\n\\begin{aligned}\n\\mathbf{y}_{1} + \\mathbf{y}_{2}\n& = \\mathbf{A} \\mathbf{x}_{1} + \\mathbf{A} \\mathbf{x}_{2}\n\\\\\n& = \\mathbf{A} (\\mathbf{x}_{1} + \\mathbf{x}_{2})\n\\end{aligned}\n\nSince by the closure under addition property,\n\n\\mathbf{x}_{1} + \\mathbf{x}_{2} \\in \\mathbb{F}^{m}\n\nand thus R (\\mathbf{A}) is closed under vector addition\n\n\\mathbf{y}_{1} + \\mathbf{y}_{2} \\in R (\\mathbf{A}).\n\nAlso, for all \\alpha \\in \\mathbb{F},\n\n\\begin{aligned}\n\\alpha \\cdot \\mathbf{y}_{1}\n& = \\alpha \\cdot \\mathbf{A} \\mathbf{x}_{1}\n\\\\\n& = \\mathbf{A} (\\alpha \\cdot \\mathbf{y}_{1})\n\\end{aligned}\n\nAgain, since by the closure under scalar multiplication property,\n\n\\alpha \\cdot \\mathbf{x}_{1} \\in \\mathbb{F}^{m}, \\forall \\alpha \\in \\mathbb{F},\n\nand thus R (\\mathbf{A}) is closed under scalar multiplication\n\n\\alpha \\cdot \\mathbf{y}_{1} \\in R (\\mathbf{A}).\n\nThus, R (\\mathbf{A}) is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#column-and-row-space",
    "href": "Knowledge/Linear Algebra/02_Vectors_and_Matrices.html#column-and-row-space",
    "title": "Vectors",
    "section": "Column and row space",
    "text": "Column and row space\nThe column space of the matrix \\mathbf{A} is the set of linear combinations of columns of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nand the row space of the matrix \\mathbf{A} is the set of linear combinations of rows of \\mathbf{A}\n\nC (\\mathbf{A}^{T}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{i, *}^{T}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\},\n\nwhich is the same as the column space of \\mathbf{A}^{T}.\nBy the definition of matrix-vector multiplication, the column space of \\mathbf{A} is the same as the range space of \\mathbf{A}\n\nC (\\mathbf{A}) = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\sum_{i=1}^{n} \\alpha_{i} \\cdot \\mathbf{a}_{*, i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\} = \\left\\{\n    \\mathbf{y} \\in \\mathbb{F}^{m} \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{x}, \\forall \\mathbf{x} \\in \\mathbb{F}^{n}\n\\right\\} = R (\\mathbf{A})."
  },
  {
    "objectID": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html",
    "href": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html",
    "title": "Similarity",
    "section": "",
    "text": "Two matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{C}^{n \\times n} are similar if there exists a non-singular matrix \\mathbf{P} such that\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{B},\n\\mathbf{A} = \\mathbf{P} \\mathbf{B} \\mathbf{P}^{-1}.\nSimilar matrices have the same eigenvalues.\n:::{prf:proof} similar matrices have the same eigenvalues. :class: dropdown\nSuppose \\lambda_{1}, \\dots, \\lambda_{n} are eigenvalues of \\mathbf{B}.\n\\begin{aligned}\n\\text{det} (\\mathbf{B} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} - \\lambda \\mathbf{P}^{-1} \\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1} (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1}) \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}) \\text{det} (\\mathbf{P})\n& = 0\n\\\\\n\\text{det} (\\mathbf{P}^{-1}) \\text{det} (\\mathbf{P}) \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I})\n& = 0\n\\\\\n\\text{det} (\\mathbf{A} - \\lambda \\mathbf{I})\n& = 0\n& [\\text{det} (\\mathbf{P}^{-1}) = \\frac{1}{\\text{det} (\\mathbf{P})}]\n\\\\\n\\end{aligned}\n :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#unitarily-similar",
    "href": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#unitarily-similar",
    "title": "Similarity",
    "section": "Unitarily similar",
    "text": "Unitarily similar\nTwo matrices \\mathbf{A}, \\mathbf{B} \\in \\mathbb{C}^{n \\times n} are unitarily similar if there exists an unitary matrix \\mathbf{U} such that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n\nwhich, according to the property of unitary matrix, can also be written as\n\n\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U} = \\mathbf{B}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalization-and-eigensystems",
    "href": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalization-and-eigensystems",
    "title": "Similarity",
    "section": "Diagonalization and eigensystems",
    "text": "Diagonalization and eigensystems\n\\mathbf{A} is diagonalizable if and only if \\mathbf{A} has n linearly independent eigenvectors. That is,\n\n\\mathbf{A} = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\nor\n\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P} = \\mathbf{\\Lambda},\n\nwhere\n\nthe columns of \\mathbf{P} \\in \\mathbb{C}^{n \\times n} are n linearly independent eigenvectors,\nthe diagonal values of \\mathbf{\\Lambda} are corresponding eigenvalues.\n\n:::{prf:proof} \\mathbf{A} is diagonalizable if and only if \\mathbf{A} has n linearly independent eigenvectors. :class: dropdown\nWe first prove that If \\mathbf{A} is diagonalizable, then \\mathbf{A} has n linearly independent eigenvectors.\nFirst note that\n\n\\begin{aligned}\n\\mathbf{A}\n& = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{-1}\n\\\\\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{\\Lambda} is a diagonal matrix with \\lambda_{1}, \\dots, \\lambda_{n} on its diagonal,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\mathbf{P}_{*, i} \\lambda_{i} = \\lambda_{i} \\mathbf{P}_{*, i}.\n\nThus, (\\lambda, \\mathbf{P}_{*, i}) is an eigenpair of \\mathbf{A}, and \\mathbf{A} has n such eigenpairs.\nSince \\mathbf{P} is non-singular (full-rank), \\mathbf{P} has independent columns. Thus, \\mathbf{A} has n independent eigenvectors.\nThen we prove that if \\mathbf{A} has n linearly independent eigenvectors, then \\mathbf{A} is diagonalizable.\nAssume the columns of \\mathbf{P} \\in \\mathbf{C}^{n \\times n} are the eigenvectors of \\mathbf{A}, and \\lambda_{1}, \\dots, \\lambda_{n} are corresponding eigenvalues,\n\n\\mathbf{A} \\mathbf{P}_{*, i} = \\lambda_{i} \\mathbf{P}_{*, i} \\quad \\forall i = 1, \\dots, n.\n\nBy rewriting \\lambda_{1}, \\dots, \\lambda_{n} as diagonals for the diagonal matrix \\mathbf{\\Lambda},\n\n\\mathbf{A} \\mathbf{P} = \\mathbf{P} \\mathbf{\\Lambda}.\n\nSince the columns of the \\mathbf{P} are linearly independent, \\mathbf{P} has full rank and thus there exists the inverse of \\mathbf{P}\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{P}\n& = \\mathbf{P} \\mathbf{\\Lambda}\n\\\\\n\\mathbf{P}^{-1} \\mathbf{A} \\mathbf{P}\n& = \\mathbf{\\Lambda}\n\\end{aligned}\n\nwhich shows that \\mathbf{A} is similar to \\mathbf{\\Lambda} and thus is diagonalizable. :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalizability-and-multiplicities",
    "href": "Knowledge/Linear Algebra/14_Similarity_and_Diagonalization.html#diagonalizability-and-multiplicities",
    "title": "Similarity",
    "section": "Diagonalizability and multiplicities",
    "text": "Diagonalizability and multiplicities\nTODO"
  },
  {
    "objectID": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html",
    "href": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html",
    "title": "Orthogonality",
    "section": "",
    "text": "A set of non-zero vectors v_{1}, \\dots, v_{k} are orthogonal if\n\\langle v_{i}, v_{j} \\rangle = 0, \\quad \\forall i \\neq j."
  },
  {
    "objectID": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-orthogonality",
    "href": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-orthogonality",
    "title": "Orthogonality",
    "section": "Properties of orthogonality",
    "text": "Properties of orthogonality\nIf v_{1}, \\dots, v_{k} are orthogonal vectors,\n(orthogonality-property-1)=\n\nv_{1}, \\dots, v_{k} are also linearly independent.\n:::{prf:proof} v_{1}, \\dots, v_{k} are also linearly independent :class: dropdown\nTODO\n:::\n\n(orthogonality-property-2)=\n\nSuppose \\mathcal{S} is a subspace with \\text{dim} (S) = n and v_{1}, \\dots, v_{k} \\in \\mathcal{S}, then the set \\{ v_{1}, \\dots, v_{k} \\} forms a basis of \\mathcal{S}. Then, \\{ v_{1}, \\dots, v_{k} \\} is an orthogonal basis of \\mathcal{S}.\n:::{prf:proof} Orthogonal basis :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#representing-vectors-using-orthogonal-basis",
    "href": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#representing-vectors-using-orthogonal-basis",
    "title": "Orthogonality",
    "section": "Representing vectors using orthogonal basis",
    "text": "Representing vectors using orthogonal basis\nSuppose \\mathcal{S} is a subspace and \\{ v_{1}, \\dots, v_{n} \\} is an orthogonal basis of \\mathcal{S}, any vector v \\in \\mathcal{S} can be represented using \\{ v_{1}, \\dots, v_{n} \\}:\n\nv = \\sum_{i=1}^{n} \\alpha_{i} v_{i},\n\nwhere\n\n\\alpha_{i} =\n\\frac{\n    \\langle v, v_{i} \\rangle\n}{\n    \\lVert v_{i} \\rVert_{ip}^{2}\n}\n\n:::{prf:proof} coefficients of orthogonal representation :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#orthonormal-vectors",
    "href": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#orthonormal-vectors",
    "title": "Orthogonality",
    "section": "Orthonormal vectors",
    "text": "Orthonormal vectors\nA set of vectors v_{1}, \\dots, v_{k} are orthonormal if all vectors in the set are orthogonal to each other, and each vector has the inner product norm of 1.\nIf \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{k} \\in \\mathbb{C}^{n} and are the columns of the matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times k}, then \\mathbf{A} has orthonormal columns and thus\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{I}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-unitary-matrix",
    "href": "Knowledge/Linear Algebra/07_Orthogonality_and_Unitary_Matrix.html#properties-of-unitary-matrix",
    "title": "Orthogonality",
    "section": "Properties of unitary matrix",
    "text": "Properties of unitary matrix\n(unitary-matrix-property-1)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n:::{prf:proof} \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} = \\mathbf{U}^{-1} :class: dropdown\nBy definition, \\mathbf{U} has orthogonal columns and thus linearly independent columns. By the rank property, \\mathbf{U} has a unique inverse matrix \\mathbf{U}^{-1} such that\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{I}_{n \\times n}.\n  \nSince by definition we know\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}_{n \\times n},\n  \nthen it must follow that\n\n  \\mathbf{U}^{H} = \\mathbf{U}^{-1}.\n  \nThe reverse can be proved backward following the procedure above.\n:::\n\n(unitary-matrix-property-2)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}_{n \\times n}.\n:::{prf:proof} \\mathbf{U} is unitary if and only if \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}_{n \\times n}. :class: dropdown\nFollowing the unitary matrix property, the inverse \\mathbf{U}^{-1} can be both left and right inverse\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nReplacing \\mathbf{U}^{-1} with \\mathbf{U}^{H}, we have the results:\n\n  \\mathbf{U}^{H} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{H} = \\mathbf{I}.\n  \n:::\n\n(unitary-matrix-property-3)=\n\nThe matrix \\mathbf{U} is unitary if and only if \\mathbf{U} \\mathbf{x} doesn’t change the length of \\mathbf{x}:\n\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert = \\lVert \\mathbf{x} \\rVert.\n  \n:::{prf:proof} \\mathbf{U} is unitary if and only if \\lVert \\mathbf{U} \\mathbf{x} \\rVert = \\lVert \\mathbf{x} \\rVert, \\forall \\mathbf{x} :class: dropdown\n\n  \\begin{aligned}\n  \\lVert \\mathbf{U} \\mathbf{x} \\rVert\n  & = \\sqrt{\\lVert \\mathbf{U} \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{U} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{I} \\mathbf{x}}\n  & [\\mathbf{U}^{H} \\mathbf{U} = \\mathbf{I}]\n  \\\\\n  & = \\sqrt{\\mathbf{x}^{H} \\mathbf{x}}\n  \\\\\n  & = \\sqrt{\\lVert \\mathbf{x} \\rVert^{2}}\n  \\\\\n  & = \\lVert \\mathbf{x} \\rVert\n  \\end{aligned}\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html",
    "href": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html",
    "title": "Complementary Subspaces",
    "section": "",
    "text": "Subspaces \\mathcal{X}, \\mathcal{Y} of a vector space \\mathcal{V} are complementary if\n\\mathcal{V} = \\mathcal{X} + \\mathcal{Y},\nand\n\\mathcal{X} \\cap \\mathcal{Y} = 0,\nin which case \\mathcal{V} is the direct sum of \\mathcal{X} and \\mathcal{Y} and is denoted as\n\\mathcal{V} = \\mathcal{X} \\oplus \\mathcal{Y}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-complementary-subspaces",
    "href": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-complementary-subspaces",
    "title": "Complementary Subspaces",
    "section": "Properties of complementary subspaces",
    "text": "Properties of complementary subspaces\n(complementary-subspaces-property-1)=\n\n\\mathcal{X} and \\mathcal{Y} are complementary if and only if there exist unique vectors x \\in \\mathcal{X} and y \\in \\mathcal{Y} such that\n\n  v = x + y,\n  \nfor each v \\in \\mathcal{V}.\n:::{prf:proof} :class: dropdown\nSuppose there are two pairs of x_{1}, x_{2} \\in \\mathcal{X} and y_{1}, y_{2} \\in \\mathcal{Y} such that\n\n  v = x_{1} + y_{1} = x_{2} + y_{2}.\n  \nThen\n\n  \\begin{aligned}\n  x_{1} + y_{1}\n  & = x_{2} + y_{2}\n  \\\\\n  x_{1} - x_{2}\n  & = y_{2} - y_{1}\n  \\\\\n  \\end{aligned}\n  \nAccording to the definition of the subspace\n\n  \\begin{aligned}\n  x_{1}, x_{2} \\in \\mathcal{X}\n  & \\Rightarrow x_{1} - x_{2} \\in \\mathcal{X}\n  \\\\\n  y_{1}, y_{2} \\in \\mathcal{X}\n  & \\Rightarrow y_{1} - y_{2} \\in \\mathcal{Y}.\n  \\end{aligned}\n  \nwhich means\n\n  x_{1} - x_{2} = y_{2} - y_{1} \\Rightarrow x_{1} - x_{2} \\in \\mathcal{Y}.\n  \nThus,\n\n  x_{1} - x_{2} \\in \\mathcal{X} \\cap \\mathcal{Y}.\n  \nHowever, since by definition \\mathcal{X} \\cap \\mathcal{Y} = 0,\n\n  \\begin{aligned}\n  x_{1} - x_{2}\n  & = 0.\n  \\\\\n  x_{1}\n  & =  x_{2}.\n  \\end{aligned}\n  \nSimilar argument can be made for y_{1} and y_{2}.\n:::\n\n(complementary-subspaces-property-2)=\n\nSuppose \\mathcal{X} has a basis \\mathcal{B}_{\\mathcal{X}} and \\mathcal{Y} has a basis \\mathcal{B}_{\\mathcal{Y}}. Then \\mathcal{X} and \\mathcal{Y} are complementary if and only if\n\n  \\mathcal{B}_{\\mathcal{X}} \\cap \\mathcal{B}_{\\mathcal{Y}} = \\emptyset\n  \nand\n\n  \\mathcal{B}_{\\mathcal{X}} \\cup \\mathcal{B}_{\\mathcal{Y}}\n  \nis a basis for \\mathcal{V}.\n:::{prf:proof} :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-projection-matrix",
    "href": "Knowledge/Linear Algebra/08_Complementary_Subspaces_and_Projection.html#properties-of-projection-matrix",
    "title": "Complementary Subspaces",
    "section": "Properties of projection matrix",
    "text": "Properties of projection matrix\n(projection-matrix-property-1)=\n\n\\mathbf{I} - \\mathbf{P} is the complementary projection matrix of \\mathbf{v} onto \\mathcal{Y} along \\mathcal{X}.\n:::{prf:proof} (\\mathbf{I} - \\mathbf{P}) v = y :class: dropdown\nAccording to the definition of projection matrix,\n\n  \\begin{aligned}\n  v\n  & = x + y\n  \\\\\n  & = \\mathbf{P} v + y.\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  y\n  & = v - \\mathbf{P} v\n  \\\\\n  & = (\\mathbf{I} - \\mathbf{P})v.\n  \\end{aligned}\n  \n:::\n\n(projection-matrix-property-2)=\n\nR (\\mathbf{P}) = N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{X} and N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y}.\n:::{prf:proof} R (\\mathbf{P}) = N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{X}, N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y} :class: dropdown\nSince v \\in \\mathcal{V} and \\mathcal{X},\n\n  \\mathbf{P} v = x \\Rightarrow R (\\mathbf{P}) = \\mathcal{X}.\n  \nAlso, for all v \\in N (\\mathbf{I} - \\mathbf{P}),\n\n  \\begin{aligned}\n  (\\mathbf{I} - \\mathbf{P}) v\n  & = 0\n  \\\\\n  v - \\mathbf{P} v\n  & = 0\n  \\\\\n  v - x\n  & = 0\n  \\\\\n  v\n  & = x\n  \\end{aligned}\n  \nwhich means v \\in \\mathcal{X} \\Rightarrow \\mathcal{V} \\subseteq N (\\mathbf{I} - \\mathbf{P}).\nSince N (\\mathbf{I} - \\mathbf{P}) \\subseteq \\mathcal{V},\n\n  N (\\mathbf{I} - \\mathbf{P}) = \\mathcal{V}.\n  \nThe proof for N (\\mathbf{P}) = R (\\mathbf{I} - \\mathbf{P}) = \\mathcal{Y} is the same.\n:::\n\n(projection-matrix-property-3)=\n\nFor x \\in \\mathcal{X}, y \\in \\mathcal{Y},\n\n  \\mathbf{P} x = x,\n  \nand\n\n  \\mathbf{P} y = 0.\n  \n:::{prf:proof} \\mathbf{P} x = x, \\mathbf{P} y = 0, for x \\in \\mathcal{X} and y \\in \\mathcal{Y} :class: dropdown\nTODO\n:::\n\n(projection-matrix-property-4)=\n\nA linear operator \\mathbf{P} on \\mathcal{V} is a projection matrix if and only if \\mathbf{P} is idempotent (\\mathbf{P} = \\mathbf{P}^{2}).\n:::{prf:proof} \\mathbf{P} is a projector if and only if \\mathbf{P} = \\mathbf{P}^{2} :class: dropdown\nAccording to the definition of the projector \n  \\mathbf{P}^{2} v = \\mathbf{P} \\mathbf{P} v = \\mathbf{P} x.\n  \nAccording to the property of projection matrix\n\n  \\mathbf{P} x = x = \\mathbf{P} v.\n  \nThus, \n  \\mathbf{P} v = \\mathbf{P}^{2} v \\Rightarrow \\mathbf{P} = \\mathbf{P}^{2}.\n  \n:::\n\n(projection-matrix-property-5)=\n\nIf \\mathcal{V} = \\mathbb{R}^{n} or \\mathbb{C}^{n}, then \\mathbf{P} is given by\n\n  \\mathbf{P} =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  =\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{I} & \\mathbf{0} \\\\\n      \\mathbf{0} & \\mathbf{0}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\mathbf{X} & \\mathbf{Y}\n  \\end{bmatrix}^{-1}\n  \nwhere the columns of \\mathbf{X} and \\mathbf{Y} are respective bases for \\mathcal{X} and \\mathcal{Y}.\n:::{prf:proof} :class: dropdown\nTODO\n:::\n\n(projection-matrix-property-6)=\n\n\\mathbf{P} is unique for a given \\mathcal{X} and \\mathcal{Y}.\n:::{prf:proof} \\mathbf{P} is unique :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html",
    "href": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html",
    "title": "Linear Map",
    "section": "",
    "text": "Let \\mathcal{U} and \\mathcal{V} be the vector spaces over the same filed \\mathbb{F}. A map T: \\mathcal{U} \\to \\mathcal{V} is linear if"
  },
  {
    "objectID": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-linear-map",
    "href": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-linear-map",
    "title": "Linear Map",
    "section": "Properties of linear map",
    "text": "Properties of linear map\nLet \\mathcal{U} and \\mathcal{V} be two vector spaces over the same filed \\mathbb{F}, and suppose T is a linear map T: \\mathcal{U} \\to \\mathcal{V}.\n\nA linear map must satisfy T (0) = 0.\n:::{prf:proof} T (0) = 0 :class: dropdown\nWe prove by contradiction. Suppose there exists a linear map such that\n\n  T (0) \\neq 0.\n  \nSuppose v = 0 and according to the definition of the linear map\n\n  \\begin{aligned}\n  T (\\alpha \\cdot v)\n  & = \\alpha \\cdot T (v), \\forall \\alpha \\in \\mathbb{F}\n  \\\\\n  T (0)\n  & = \\alpha \\cdot T (0), \\forall \\alpha \\in \\mathbb{F}.\n  \\end{aligned}\n  \nSince T (0) \\neq 0, we can divide both sides by T (0)\n\n  \\alpha = 1, \\forall \\alpha \\in \\mathbb{F},\n  \nwhich raises a contradiction. :::\nThere exists a matrix representation of T.\n:::{prf:proof} a matrix representation exists for any linear map :class: dropdown\nSuppose \\text{dim} (\\mathcal{U}) = n and \\text{dim} (\\mathcal{V}) = m.\nLet \\{ u_{1}, \\dots, u_{n} \\} be a basis of \\mathcal{U}, and \\{ v_{1}, \\dots, v_{m} \\} be a basis of \\mathcal{V}.\nSuppose any vector x \\in \\mathcal{U} satisfies that\n\n  x = \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i},\n  \nand the mapped vector T (x) \\in \\mathcal{V} satisfies that\n\n  T (x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}.\n  \nSince T (u_{i}) \\in \\mathcal{V}, we have\n\n  T (u_{i}) = \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}, \\forall i = 1, \\dots, n.\n  \nBy the definition of the linear map,\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} \\cdot u_{i}\n  \\right)\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\cdot T (u_{i}).\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} \\sum_{j=1}^{m} c_{i, j} \\cdot v_{j}\n  \\\\\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}.\n  \\\\\n  \\end{aligned}\n  \nBecause of the unique representation property,\n\n  \\begin{aligned}\n  T(x) = \\sum_{j=1}^{m} \\beta_{j} \\cdot v_{j}\n  & = \\sum_{j=1}^{m} \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} c_{i, j}\n  \\right) \\cdot v_{j}\n  \\\\\n  \\beta_{j}\n  & = \\sum_{i=1}^{n} \\alpha_{i} c_{i, j} \\quad \\forall j = 1, \\dots, m,\n  \\end{aligned}\n  \nwhich can be represented in the matrix form\n\n  \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1}  \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, m} & \\dots & c_{n, m} \\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix}.\n  \nHence, given any vector x \\in \\mathcal{U} that has basis coefficients in a given basis \\{ u_{1}, \\dots, u_{n} \\}\n\n  \\mathbf{a} = \\begin{bmatrix}\n  \\alpha_{1} \\\\\n  \\vdots \\\\\n  \\alpha_{n} \\\\\n  \\end{bmatrix},\n  \nthere is a mapped vector T (x) \\in \\mathcal{V} with basis coefficients in a given basis \\{ v_{1}, \\dots, v_{n} \\}\n\n  \\mathbf{b} = \\begin{bmatrix}\n  \\beta_{1} \\\\\n  \\vdots \\\\\n  \\beta_{m} \\\\\n  \\end{bmatrix},\n  \nwhere\n\n  \\mathbf{b} = \\mathbf{C} \\mathbf{a}.\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "href": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#generalization-of-null-space-and-range-space",
    "title": "Linear Map",
    "section": "Generalization of null space and range space",
    "text": "Generalization of null space and range space\nSince every linear map has a matrix representation, the null space and range space defined using matrix can be redefined using linear map.\nGiven a map T: \\mathcal{U} \\to \\mathcal{V}, the null space (kernel) is\n\nN (T) = \\left\\{\n    x \\in \\mathcal{U} \\mid T (x) = 0\n\\right\\},\n\nand the range space (column space) is\n\nR (T) = \\left\\{\n    y \\in \\mathcal{V} \\mid y \\in T (x), \\forall x \\in \\mathcal{U}\n\\right\\}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-rank",
    "href": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#properties-of-rank",
    "title": "Linear Map",
    "section": "Properties of rank",
    "text": "Properties of rank\nGiven a matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n},\n(rank-property-1)=\n\nrank-nullity theorem: \\text{rank} (\\mathbf{A}) + \\text{dim} (N (\\mathbf{A})) = n\n:::{prf:proof} the rank-nullity theorem :class: dropdown\nSince \\mathbf{A} represents a linear transform T: \\mathcal{U} \\to \\mathcal{V} with \\mathrm{dim} (\\mathcal{U}) = m, \\mathrm{dim} (\\mathcal{V}) = n the rank-nullity theorem can also be stated as\n\n  \\mathrm{dim} (R (T)) + \\mathrm{dim} (N (T)) = \\mathrm{dim} (\\mathcal{U}).\n  \nBy assuming that \\text{dim} (N (T)) = k, there exists a basis \\{ b_{1}, \\dots, b_{k} \\} of N (T).\nSince by definition of the null space, N (T) is a subspace of \\mathcal{U}, then according to the property of the basis, there exists a set of vectors \\{ b_{k + 1}, \\dots, b_{n} \\} in \\mathcal{U} such that\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nis a basis of \\mathcal{U}.\nConsider any x \\in \\mathcal{U},\n\n  \\begin{aligned}\n  T (x)\n  & = T \\left(\n      \\sum_{i=1}^{n} \\alpha_{i} b_{i}\n  \\right)\n  & [\\{ b_{1}, \\dots, b_{i} \\} \\text{ is a basis of } \\mathcal{U}]\n  \\\\\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  & [\\text{T is linear}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T), they are also in N (T). According to the definition of the null space\n\n  T (b_{i}) = 0 \\quad i = 1, \\dots, k.\n  \nThen we can show that every T (x) is a linear combination of T (b_{k+1}) \\dots, T (b_{n}):\n\n  \\begin{aligned}\n  T (x)\n  & = \\sum_{i=1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=1}^{k} \\alpha_{i} T (b_{i}) + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = 0 + \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\\\\n  & = \\sum_{i=k+1}^{n} \\alpha_{i} T (b_{i})\n  \\end{aligned}.\n  \nSince the definition of the range space is\n\n  R (T) = \\{ y \\mid y = T(x), \\forall x \\in \\mathcal{U} \\},\n  \nall vectors in R (T) are linear combinations of T (b_{k+1}) \\dots, T (b_{n}).\nAlso, since b_{k+1}, \\dots, b_{n} \\in \\mathcal{U}, we have\n\n  T (b_{k+1}), \\dots, T (b_{n}) \\in R (T).\n  \nby the definition of the range space.\nThus, by the property of span\n\n  R (T) = \\text{span} (T (b_{k+1}), \\dots, T (b_{n})).\n  \nThen we will show that T (b_{k+1}), \\dots, T (b_{n}) are linearly independent by supposing a set of \\beta_{k+1}, \\dots, \\beta_{n} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i})\n  & = 0\n  \\\\\n  T \\left(\n      \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  \\right)\n  & = 0\n  & [\\text{T is linear}]\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & \\in N (T)\n  & [\\text{def of null space}]\n  \\\\\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{k} \\} is also a basis of the N (T), there exists a set of \\gamma_{1}, \\dots, \\gamma_{k} such that\n\n  \\begin{aligned}\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i}\n  & = \\sum_{i=j}^{k} \\gamma_{j} b_{j}\n  \\\\\n  \\sum_{i=k+1}^{n} \\beta_{i} b_{i} + \\sum_{j=1}^{k} - \\gamma_{j} b_{j}\n  & = 0.\n  \\end{aligned}\n  \nSince \\{ b_{1}, \\dots, b_{n} \\} are linearly independent, it must hold that\n\n  \\beta_{i} = \\gamma_{j} = 0 \\quad i = k + 1, \\dots, n, j = 1, \\dots, k.\n  \nThus, we have shown that\n\n  \\sum_{i=k+1}^{n} \\beta_{i} T (b_{i}) = 0 \\iff \\beta_{i} = 0 \\quad i = k + 1, \\dots, n,\n  \nwhich means the vectors T (b_{k + 1}), \\dots, T (b_{n}) are linearly independent and therefore is a basis of R (T).\nFinally, since\n\n\\{ b_{1}, \\dots, b_{k} \\} is a basis of N (T),\n\\{ b_{k + 1}, \\dots, b_{n} \\} is a basis of R (T), and\n\\{ b_{1}, \\dots, b_{n} \\} is a basis of \\mathcal{U},\n\nwe have the rank-nullity theorem\n\n  \\text{dim} (N (T)) + \\text{dim} (R (T)) = \\text{dim} (\\mathcal{U}).\n  \n:::\n\n(rank-property-2)=\n\n\\text{rank} (\\mathbf{A}) \\leq \\min (m, n)\n:::{prf:proof} \\text{rank} (\\mathbf{A}) \\leq \\min (m, n) :class: dropdown\nAccording to the rank property\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) = n - \\text{rank} (\\mathbf{A}).\n  \nSince by definition, the number of vectors in a basis is non-negative\n\n  \\text{dim} (N (\\mathbf{\\mathbf{A}})) \\geq 0,\n  \nwe have\n\n  \\text{rank} (\\mathbf{A}) \\leq n.\n  \nAlso, since R (\\mathbf{A}) is a subspace of \\mathbb{F}^{m} and the property\n\n  \\text{rank} (\\mathbf{A}) = \\text{dim} (R (\\mathbf{A})) \\leq m.\n  \nThus,\n\n  \\text{rank} (\\mathbf{A}) \\leq \\min(m, n).\n   :::\n\nIf \\text{rank} (\\mathbf{A}) = \\min (m, n), matrix is called a full (row or column) rank matrix.\nIf \\text{rank} (\\mathbf{A}) = n, \\mathbf{a}_{*, 1}, \\dots, \\mathbf{a}_{*, n} are linearly independent.\n\n\n(rank-property-3)=\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T})\n:::{prf:proof} \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T}) :class: dropdown\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Let \\{ \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in \\mathbb{F}^{m} \\} be the columns of \\mathbf{A}\n\n  \\mathbf{A} =\n  \\begin{bmatrix}\n  \\mathbf{a}_{1} & \\dots & \\mathbf{a}_{n}\n  \\end{bmatrix}.\n  \nLet \\text{rank} (\\mathbf{A}) = r.\nBy the definition of the range space, the columns of \\mathbf{A} are in its range space\n\n  \\mathbf{a}_{1}, \\dots, \\mathbf{a}_{n} \\in R (\\mathbf{A})\n  \nand thus the columns of \\mathbf{A} are linear combinations of any basis of R (\\mathbf{A}).\nSuppose there exists a basis of R (\\mathbf{A}) as columns of \\mathbf{B}\n\n  \\mathbf{B} =\n  \\begin{bmatrix}\n  \\mathbf{b}_{1} & \\dots & \\mathbf{b}_{r}\n  \\end{bmatrix}.\n  \nand a matrix \\mathbf{C} \\in \\mathbb{F}^{r \\times n}\n\n  \\mathbf{C} =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{n, 1} \\\\\n  \\vdots & \\dots & \\vdots \\\\\n  c_{1, r} & \\dots & c_{n, r} \\\\\n  \\end{bmatrix}\n  \nsuch that the columns of \\mathbf{A} are linear combinations of the basis \\mathbf{B} using the columns of \\mathbf{C} as cofficients\n\n  \\mathbf{A} = \\mathbf{B} \\mathbf{C}.\n  \nBy the defintion of matrix multiplication, the rows of \\mathbf{A} are also linear combinations of rows of \\mathbf{C} using the rows of \\mathbf{B} as coefficients. Thus, by the spanning set property,\n\n  R (\\mathbf{A}^{T}) \\subseteq R (\\mathbf{C}^{T}).\n  \nThen, by the dimension property,\n\n  \\begin{aligned}\n  \\text{dim} (R (\\mathbf{A}^{T}))\n  & \\leq \\text{dim} (R (\\mathbf{C}^{T}))\n  \\\\\n  \\text{rank} (\\mathbf{A}^{T})\n  & \\leq \\text{rank} (\\mathbf{C}^{T}).\n  \\end{aligned}\n  \nSince \\mathbf{C}^{T} \\in \\mathbb{F}^{n \\times n}, according to the rank property\n \\text{rank} (\\mathbf{C}^{T}) \\leq r = \\text{rank} (\\mathbf{A}). \nThus,\n\n  \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}).\n  \nFinally, we can use the same argument to prove\n\n  \\text{rank} (\\mathbf{A}) \\leq \\text{rank} (\\mathbf{A}^{T}).\n  \nby substituting \\mathbf{A} as \\mathbf{A}^{T}.\nSince \\text{rank} (\\mathbf{A}^{T}) \\leq \\text{rank} (\\mathbf{A}) and $ () (^{T})$ at the same time, we can prove\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T}).\n   :::\n\n(rank-property-6)=\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}).\n  \n:::{prf:proof} \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}), \\mathbf{A} \\in \\mathbb{R}^{m \\times n} :class: dropdown\nTODO\n:::\n\n(rank-property-4)=\n\nIf \\mathbf{A} \\in \\mathbb{R}^{m \\times n} and \\mathbf{B} \\in \\mathbb{R}^{m \\times m} and \\mathbf{C} \\in \\mathbb{R}^{n \\times n} are full rank matrices, then\n\n  \\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{B} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{C}).\n  \n:::{prf:proof} Multiplication by a full-rank square matrix preserves rank :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "href": "Knowledge/Linear Algebra/05_Linear_Map_and_Rank.html#rank-and-matrix-inverse",
    "title": "Linear Map",
    "section": "Rank and matrix inverse",
    "text": "Rank and matrix inverse\n(rank-property-4)=\n\nSuppose \\mathbf{A} \\in \\mathbb{F}^{m \\times n}.\nThere exists a left inverse matrix \\mathbf{B} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n  \\mathbf{B} \\mathbf{A} = \\mathbf{I}_{n \\times n},\n  \nif and only if \\text{rank} (\\mathbf{A}) = n.\nThere exists a right inverse matrix \\mathbf{C} \\in \\mathbb{F}^{n \\times m} of \\mathbf{A} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m}.\n  \nif and only if \\text{rank} (\\mathbf{A}) = m.\n:::{prf:proof} \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\text{rank} (\\mathbf{A}) = m :class: dropdown\nTo prove this, we first prove that there exists a matrix \\mathbf{C} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A).\n  \nFirst notice that\n\n  \\mathbf{I}_{m \\times m} =\n  \\begin{bmatrix}\n  \\mathbf{e}_{1} & \\dots & \\mathbf{e}_{m}\n  \\end{bmatrix}\n  \nwhere \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in \\mathbb{F}^{m}.\nThus, \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a linear combination of the columns of \\mathbf{A}. By the definition of the range space, we can see\n\n  \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n  \nConversely, since \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} \\in R (\\mathbf{A}), each \\mathbf{e}_{i}, i = 1, \\dots, m is a linear combination of the columns in \\mathbf{A}.\nThus, there exists a set of coefficients c_{j, i}, j = 1, \\dots, n for the linear combination of \\mathbf{e}_{i}, and the matrix of these coefficients is\n\n  \\mathbf{C} =\n  \\begin{bmatrix}\n  c_{1, 1} & \\dots & c_{1, m} \\\\\n  \\vdots & c_{j, i} & \\vdots \\\\\n  c_{n, 1} & \\dots & c_{n, m} \\\\\n  \\end{bmatrix}.\n  \nThen we prove that\n\n  \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}) \\iff \\text{rank} (\\mathbf{A}) = m.\n  \nAgain notice that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} is the standard basis of \\mathbb{F}^{m}.\nSince R (\\mathbf{A}) \\subseteq \\mathbb{F}^{m}, all vectors in R (\\mathbf{A}) are linear combinations of \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m}.\nSince we know that \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}), according to the spanning set property,\n\n  R (\\mathbf{A}) = \\text{span} (\\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m}).\n  \nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} are linearly independent, the set \\mathbf{e}_{1}, \\dots, \\mathbf{e}_{m} is a basis for R (\\mathbf{A}).\nAccording to the cardinality of basis,\n\n  \\text{rank} (\\mathbf{A}) = m.\n  \nConversely, since we know \\text{rank} (\\mathbf{A}) = m = \\text{dim} (\\mathbb{F}^{m} and $ R () ^{m}$,\n\n  R (\\mathbf{A}) = \\mathbb{F}^{m}.\n  \nSince by definition \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in \\mathbb{F}^{m},\n\n  \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (\\mathbf{A}).\n  \nIn conclusion, we have proven that there exists a matrix \\mathbf{C} such that\n\n  \\mathbf{A} \\mathbf{C} = \\mathbf{I}_{m \\times m} \\iff \\mathbf{e}_{1}, \\dots \\mathbf{e}_{m} \\in R (A) \\iff \\text{rank} (\\mathbf{A}) = m.\n  \n:::\n:::{prf:proof} \\mathbf{B} \\mathbf{A} = \\mathbf{I}_{n \\times n} \\iff \\text{rank} (\\mathbf{A}) = n. :class: dropdown\nWe first notice that\n\n  \\begin{aligned}\n  (\\mathbf{B} \\mathbf{A})^{T}\n  & = \\mathbf{I}_{n \\times n}^{T}\n  \\\\\n  \\mathbf{A}^{T} \\mathbf{B}^{T}\n  & = \\mathbf{I}_{n \\times n}.\n  \\end{aligned}\n  \nBy applying the proof above, we can prove that\n\n  \\text{rank} (\\mathbf{A}^{T}) = n.\n  \nAccording to the rank property,\n\n  \\text{rank} (\\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = n.\n  \n:::\n\n(rank-property-5)=\n\nIf \\mathbf{A} \\in \\mathbb{F}^{n \\times n} is a square matrix and has full rank (\\text{rank}(\\mathbf{A}) = n), then \\mathbf{A} has a unique inverse matrix \\mathbf{A}^{-1} such that\n\n  \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n  \n:::{prf:proof} \\text{rank} (\\mathbf{A}) = n \\iff \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n} :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html",
    "href": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html",
    "title": "Span",
    "section": "",
    "text": "Given a vector space \\mathcal{V} over a field \\mathbb{F}, the span of a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is the set of all possible linear combinations of v_{1}, \\dots, v_{m}\n\\text{span} (v_{1}, \\dots, v_{n}) = \\left\\{\n    \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}, \\forall \\alpha_{i} \\in \\mathbb{F}\n\\right\\}.\nIf \\mathcal{V} is \\mathbb{F}^{m}, the set vectors v_{1}, \\dots, v_{n} can be viewed as the columns of the matrix \\mathbf{A} \\in \\mathbb{F}^{m \\times n}. Then\n\\text{span} (v_{1}, \\dots, v_{n}) = R (\\mathbf{A}).\nSince we have proved R (\\mathbf{A}) is a subspace of \\mathcal{V}, the span of a set of vectors is a subspace of \\mathcal{V}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "href": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#spanning-set",
    "title": "Span",
    "section": "Spanning set",
    "text": "Spanning set\nFor a set of vectors \\mathcal{S} = v_{1}, \\dots, v_{n}, if the subspace \\mathcal{W} is the span of \\mathcal{S}\n\n\\mathcal{W} = \\text{span} (\\mathcal{S}),\n\nthen the set of vectors \\mathcal{S} is the spanning set of the subspace \\mathcal{W}.S."
  },
  {
    "objectID": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-the-spanning-set",
    "href": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-the-spanning-set",
    "title": "Span",
    "section": "Properties of the spanning set",
    "text": "Properties of the spanning set\nConsider a set of vectors $ = { v_{1}, , v_{n} }$ and a subspace \\mathcal{W}.\n(spanning-set-property-1)=\n\nIf \\mathcal{A} is a spanning set of \\mathcal{W}, the new set\n\n  \\mathcal{A} \\cup \\left\\{\n      u_{1}, \\dots, u_{n}\n  \\right\\}\n  \nis still a spanning set of \\mathcal{W} for arbitrary vectors u_{1}, \\dots, u_{n} \\in \\mathcal{W}.\n\n(spanning-set-property-2)=\n\n\\mathcal{A} is a spanning set of \\mathcal{W} if and only if\n\nall vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n} and\nv_{1}, \\dots, v_{n} \\in \\mathcal{W}.\n\n:::{prf:proof} necessary and sufficient conditions for a spanning set :class: dropdown\nSince this statement has “if and only if”, we first prove in the forward direction.\nIf v_{1}, \\dots, v_{n} is a spanning set of \\mathcal{W}, then by the definition of spanning set, the set of all linear combinations of v_{1}, \\dots, v_{n} is the subspace \\mathcal{W}.\nThus, all vectors in the subspace \\mathcal{W} are the linear combinations of v_{1}, \\dots, v_{n}.\nSince every v_{i}, i = 1, \\dots, n is a linear combination of itself, then v_{1}, \\dots, v_{n} are also in \\mathcal{W}.\nThen we prove in the backward direction.\nSince all vectors in \\mathcal{W} are linear combinations of v_{1}, \\dots, v_{n}, by the definition of span,\n\n  \\mathcal{W} \\subseteq \\text{span} (v_{1}, \\dots, v_{n}).\n  \nSince v_{1}, \\dots, v_{n} \\in \\mathcal{W} and the closure property of the subspace, all linear combinations of v_{1}, \\dots, v_{n} are also in \\mathcal{W}, which means that\n\n  \\text{span} (v_{1}, \\dots, v_{n}) \\subseteq \\mathcal{W}.\n  \nThus,\n\n  \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}).\n  \n:::\n\n(spanning-set-property-3)=\n\nIf vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, then\n\n  \\text{span} (\\mathcal{B}) \\subseteq \\text{span} (\\mathcal{A}).\n  \n:::{prf:proof} \\text{span} (\\mathcal{B}) \\subseteq \\text{span} (\\mathcal{A}) if \\mathcal{B} is a linear combination of \\mathcal{A}. :class: dropdown\nWe prove by contradiction. Suppose that the vectors in set \\mathcal{B} are linear combinations of the vectors in \\mathcal{A}, but there exists an vector v \\in \\text{span} (\\mathcal{B}) but v \\notin \\text{span} (\\mathcal{A}).\nSuppose \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}. Since v \\in \\mathcal{B} is a linear combinations of vectors in \\mathcal{A}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that\n\n  v = \\sum_{i = 1}^{n} \\alpha_{i} a_{i}.\n  \nwhich by definition of span means v \\in \\text{span} (\\mathbf{A}).\nHowever, we assume that v \\notin \\text{span} (\\mathbf{A}), which raises a contradiction. :::\n\n(spanning-set-property-4)=\n\nIf \\text{span} (\\mathcal{A}) = \\mathcal{W} and \\text{span} (\\mathcal{B}) = \\mathcal{U}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\mathcal{W} + \\mathcal{U},\n  \nwhere\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}.\n  \n:::{prf:proof} span of the union of spanning sets is sum of the subspaces. :class: dropdown\nConsider \\mathcal{A} = \\{ a_{1}, \\dots, a_{n} \\}, \\mathcal{B} = \\{ b_{1}, \\dots, b_{m} \\}, then\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      \\sum_{i=1}^{n} \\alpha_{i} a_{i} + \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\alpha_{i}, \\beta_{i} \\in \\mathbb{F}\n  \\right\\}.\n  \nNote that\n\n  w \\in \\mathcal{W} = \\sum_{i=1}^{n} \\alpha_{i} a_{i}, \\forall \\alpha_{i} \\in \\mathbb{F},\n  \n\n  u \\in \\mathcal{U} = \\sum_{i=1}^{n} \\beta_{i} b_{i}, \\forall \\beta_{i} \\in \\mathbb{F}.\n  \nThus,\n\n  \\text{span} (\\mathcal{A} \\cup \\mathcal{B}) = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\} = \\mathcal{W} + \\mathcal{U}.\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-linear-independence",
    "href": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#properties-of-linear-independence",
    "title": "Span",
    "section": "Properties of linear independence",
    "text": "Properties of linear independence\n\nGiven a matrix \\mathbf{A} whose columns are vectors \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\in \\mathbb{F}^{m} the set of vectors \\mathcal{S} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\} is linearly independent when the null space of \\mathbf{A} only contains \\mathbf{0} \\in \\mathbb{F}^{n}.\n\n  N (\\mathbf{A}) = \\left\\{\n      \\mathbf{0}\n  \\right\\}\n  \n:::{prf:proof} :class: dropdown\nBy definition, N (\\mathbf{A}) = \\{ \\mathbf{0} \\} says that the only set of \\alpha_{1}, \\dots, \\alpha_{n} satisfying\n\n  \\sum_{i = 1}^{n} \\alpha_{i} \\mathbf{A}_{*, i} = 0\n  \nis \\alpha_{1}, \\dots, \\alpha_{n} = 0, which is equivalent to saying the columns of \\mathbf{A} are a linearly independent set.\n:::\nIf vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} are linearly independent and the subspace \\mathcal{W} = \\text{span} (v_{1}, \\dots, v_{n}), the set of field elements \\{ \\alpha_{1}, \\dots, \\alpha_{n} \\} that is paired with the set of vectors \\{ v_{1}, \\dots, v_{n} \\} to represent any vector u \\in \\mathcal{W}: u = \\sum_{i=1}^{n} \\alpha_{i} v_{i} is unique.\n:::{prf:proof} unique representation property :class: dropdown\nProof by contradiction. Suppose there is another set of field elements that can be paired with v_{1}, \\dots, v_{n} to represent u\n\n  \\left\\{\n      \\beta_{1}, \\dots, \\beta_{n} \\mid u = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\right\\}.\n  \nThen,\n\n  \\begin{aligned}\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i}\n  & = \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  \\\\\n  \\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} - \\sum_{i=1}^{n} \\beta_{i} \\cdot v_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{n} (\\alpha_{i} - \\beta_{i}) \\cdot v_{i}\n  & = 0\n  \\\\\n  \\alpha_{i} - \\beta_{i}\n  & = 0\n  \\\\\n  \\alpha_{i}\n  & = \\beta_{i},\n  \\end{aligned}\n  \nwhich contradicts to the fact \\alpha_{i} and \\beta_{i} should be different.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "href": "Knowledge/Linear Algebra/03_Span_and_Linear_Independence.html#linear-dependence",
    "title": "Span",
    "section": "Linear dependence",
    "text": "Linear dependence\nGiven a vector space \\mathcal{V} over a field \\mathbb{F}, a set of non-zero vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} is linear dependent when there exists an \\alpha_{i} \\neq 0 such that\n\n\\sum_{i=1}^{n} \\alpha_{i} \\cdot v_{i} = 0.\n\n(existence-of-linear-combination)=\nIf a set of vectors \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an index j (1 \\leq j \\leq n) such that v_{j} is a linear combination of the rest of the vectors:\n\nv_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} \\cdot v_{i}.\n\n:::{prf:proof} existence of linear combination :class: dropdown\nSince the set \\{ v_{1}, \\dots, v_{n} \\} is linearly dependent, there exists an \\beta_{j} \\neq 0 (1 \\leq j \\leq n) such that\n\n\\beta_{1} \\cdot v_{1} + \\dots \\beta_{j} \\cdot v_{j} + \\dots \\beta_{n} \\cdot v_{n} = 0.\n\nThus,\n\n\\begin{aligned}\n\\beta_{j} \\cdot v_{j}\n& = - \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = - \\beta_{j}^{-1} \\cdot \\sum_{i=1, i \\neq j} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} - \\beta_{j}^{-1} \\beta_{i} \\cdot v_{i}\n\\\\\nv_{j}\n& = \\sum_{i=1, i \\neq j} \\alpha_{i} \\cdot v_{i}\n& [\\text{rewrite } - \\beta_{j}^{-1} \\beta_{i} = \\alpha_{i}]\n\\\\\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html",
    "href": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html",
    "title": "Inner Product",
    "section": "",
    "text": "A vector space \\mathcal{V} over field \\mathbb{R} or \\mathbb{C} is a inner product space if there exists a function called inner product, denoted\n\\langle \\cdot, \\cdot \\rangle: \\mathcal{V} \\times \\mathcal{V} \\to \\mathbb{R}\nwith the following properties:"
  },
  {
    "objectID": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#properties-of-inner-product",
    "href": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#properties-of-inner-product",
    "title": "Inner Product",
    "section": "Properties of inner product",
    "text": "Properties of inner product\n(inner-product-property-1)=\n\n\\langle x, \\alpha y + \\alpha z \\rangle = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n:::{prf:proof} :class: dropdown\n\n  \\begin{aligned}\n  \\langle x, \\alpha y + \\alpha z \\rangle\n  & = \\overline{\\langle \\alpha y + \\alpha z, x \\rangle}\n  & [\\text{Conjugate sym}]\n  \\\\\n  & = \\overline{\\alpha \\langle y, x \\rangle + \\alpha \\langle z, x \\rangle}\n  & [\\text{Linearity of 1st}]\n  \\\\\n  & = \\overline{\\alpha} \\overline{\\langle y, x \\rangle} + \\overline{\\alpha} \\overline{\\langle z, x \\rangle}\n  \\\\\n  & = \\overline{\\alpha} \\langle x, y \\rangle + \\overline{\\alpha} \\langle x, z \\rangle\n  \\end{aligned}\n   :::\n\n(inner-product-property-2)=\n\nIf $x, y = 0 $ for all x \\in \\mathcal{V}, then y = 0.\n:::{prf:proof} :class: dropdown\nTODO :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#lp-norm",
    "href": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#lp-norm",
    "title": "Inner Product",
    "section": "Lp-norm",
    "text": "Lp-norm\nGiven \\mathcal{V} = \\mathbb{R}^{n}, define \\lVert \\cdot \\rVert_{p}: \\mathcal{V} \\to \\mathbb{R}^{+} as\n\n\\lVert \\mathbf{v} \\rVert_{p} = \\left(\n    \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{p}\n\\right)^{\\frac{1}{p}}, \\quad \\mathbf{v} \\in \\mathcal{V},\n\nfor p \\geq 1.\n\nL_{1} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{1} = \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert\n  \nL_{2} norm:\n\n  \\lVert \\mathbf{v} \\rVert_{2} = \\left( \\sum_{i = 1}^{n} \\lvert v_{i} \\rvert^{2} \\right)^{\\frac{1}{2}}"
  },
  {
    "objectID": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "href": "Knowledge/Linear Algebra/06_Inner_Product_and_Norm.html#norm-induced-by-inner-product",
    "title": "Inner Product",
    "section": "Norm induced by inner product",
    "text": "Norm induced by inner product\nGiven an inner product vector space \\mathcal{V}, a norm \\lVert \\cdot \\rVert_{ip}: \\mathcal{V} \\to R^{+} induced by its inner product is\n\n\\lVert v \\rVert_{ip} = \\sqrt{\\langle v, v \\rangle}, \\quad v \\in \\mathcal{V}.\n\n\nFor the vector space \\mathbb{C}^{n}, the norm induced by inner product is the same as L_{2} norm\n\n  \\lVert \\mathbf{v} \\rVert_{ip} = \\sqrt{ \\sum_{i=1}^{n} v_{i} v_{i} } = \\sqrt{ \\sum_{i=1}^{n} \\lvert v_{i} \\rvert^{2} } = \\lVert \\mathbf{v} \\lVert_{2}\n  \nCauchy-Schwarz Inequality: let \\mathcal{V} be an inner product space. Then, for any u, v \\in \\mathcal{V}, we have\n\n  \\lvert \\langle u, v \\rangle \\rvert \\leq \\lVert u \\rVert_{ip} \\lVert v \\rVert_{ip}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html",
    "title": "Orthogonal projection",
    "section": "",
    "text": "Suppose \\mathcal{M} is a subspace in the vector space \\mathcal{V}. Since \\mathcal{M}^{\\perp} is a orthogonal complementary subspace of \\mathcal{M}, we have \\mathcal{M} \\oplus \\mathcal{M}^{\\perp} = \\mathcal{V}.\nFor v \\in \\mathcal{V}, let v = m + n, where m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}. We define a linear operator \\mathbf{P}_{\\mathcal{M}} \\in \\mathbb{C}^{n \\times n} such that\n\\mathbf{P}_{\\mathcal{M}} v = m.\nwhere"
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#properties-of-orthogonal-projection",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#properties-of-orthogonal-projection",
    "title": "Orthogonal projection",
    "section": "Properties of orthogonal projection",
    "text": "Properties of orthogonal projection\nSuppose m is the orthogonal projection of v on the subspace \\mathcal{M}\n\nm = \\mathbf{P}_{\\mathcal{M}} v.\n\n(orthogonal-projection-property-1)=\n\nm always exists and is unique.\n:::{prf:proof} Existence and uniqueness of m :class: dropdown\nTODO\n:::\n\n(orthogonal-projection-property-2)=\n\nOrthogonality of the difference.\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}\n  \n:::{prf:proof} \\langle v - m, x \\rangle = 0, \\forall x \\in \\mathcal{M} :class: dropdown\nSince n \\in \\mathcal{M}^{\\perp}, by definition of the orthogonal complement of subspaces,\n\n  \\langle n, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \nSince by definition v = m + n,\n\n  \\langle v - m, x \\rangle = 0, \\quad \\forall x \\in \\mathcal{M}.\n  \n:::\n\n(orthogonal-projection-property-3)=\n\nClosest point theorem: m is the closest point in \\mathcal{M} to v in terms of the ip norm:\n\n  m = \\min_{x \\in \\mathcal{M}} \\lVert v - x \\rVert.\n  \n:::{prf:proof} m minimizes \\lVert v - x \\rVert :class: dropdown\nAccording to the property of orthogonal projection, the projection m always exists.\nThus, we can add and subtract m,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m + m - x \\rVert^{2} \\quad \\forall x \\in \\mathcal{M}\n  \\\\\n  & = \\langle v - m + m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\langle v - m, v - m + m - x \\rangle + \\langle m - x, v - m + m - x \\rangle\n  \\\\\n  & = \\lVert v - m \\rVert^{2} + \\langle v - m, m - x \\rangle + \\langle m - x, v - m \\rangle + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\end{aligned}\n  \nSince m - x \\in \\mathcal{M}, according to the property of orthogonal projection,\n\n  \\langle v - m, m - x \\rangle = \\langle m - x, v - m \\rangle = 0.\n  \nThus,\n\n  \\begin{aligned}\n  \\lVert v - x \\rVert^{2}\n  & = \\lVert v - m \\rVert^{2} + \\lVert m - x \\rVert^{2}\n  \\\\\n  \\lVert v - x \\rVert^{2}\n  & \\geq \\lVert v - m \\rVert^{2}\n  & [\\lVert m - x \\rVert^{2} \\geq 0]\n  \\end{aligned}\n  \nwhich shows that\n\n  \\lVert v - m \\rVert^{2}\n  \nminimizes the distances between v to all vectors x in the subspace \\mathcal{M}.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#orthogonal-projection-matrix",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#orthogonal-projection-matrix",
    "title": "Orthogonal projection",
    "section": "Orthogonal projection matrix",
    "text": "Orthogonal projection matrix\nIf \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of v onto the subspace \\mathcal{M}, and the columns of \\mathbf{M} \\in \\mathbb{C}^{n \\times r} are r bases for \\mathcal{M}, then\n\n\\mathbf{P}_{\\mathcal{M}} = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{M})^{-1} \\mathbf{M}^{H}.\n\n:::{prf:proof} \\mathbf{P}_{\\mathcal{M}} = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{M})^{-1} \\mathbf{M}^{H} :class: dropdown\nSince \\text{rank} (\\mathbf{M}^{H} \\mathbf{H}) = \\text{rank} (\\mathcal{M}) = r and \\mathbf{M}^{H} \\mathbf{H} \\in \\mathbb{C}^{r \\times r}, according to the property of rank, (\\mathbf{M}^{H} \\mathbf{H})^{-1} exists.\nSuppose the columns of \\mathbf{N} \\in \\mathbb{C}^{n \\times s} are orthonormal bases for \\mathcal{M}^{\\perp}, where s = n - r according to the property of complementary subspaces.\nThus, we can construct the below matrix\n\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\nAccording to the definition of complementary subspaces, \\mathbf{N}^{T} \\mathbf{M} = 0 and \\mathbf{M}^{T} \\mathbf{N} = 0.\nThus,\n\n\\begin{aligned}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}\n& =\n\\begin{bmatrix}\n    \\mathbf{I}_{r \\times r} & \\mathbf{0} \\\\\n    \\mathbf{0} & \\mathbf{I}_{s \\times s} \\\\\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N} \\\\\n\\end{bmatrix}^{-1}\n& =\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}.\n\\end{aligned}\n\nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, by the definition of projector,\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{N}\n\\end{bmatrix}^{-1} \\\\\n& =\n\\begin{bmatrix}\n    \\mathbf{M} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n    (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H} \\\\\n    \\mathbf{N}^{H} \\\\\n\\end{bmatrix}\n\\\\\n& = \\mathbf{M} (\\mathbf{M}^{H} \\mathbf{H})^{-1} \\mathbf{M}^{H}.\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#properties-of-orthogonal-projection-matrix",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#properties-of-orthogonal-projection-matrix",
    "title": "Orthogonal projection",
    "section": "Properties of orthogonal projection matrix",
    "text": "Properties of orthogonal projection matrix\n(orthogonal-projection-matrix-property-1)=\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  R (\\mathbf{P}) = N (\\mathbf{P})^{\\perp}.\n  \n:::{prf:proof} R (\\mathbf{P}) = N (\\mathbf{P})^{\\perp} :class: dropdown\nTODO\n:::\n\n(orthogonal-projection-matrix-property-2)=\n\n\\mathbf{P} is a orthogonal projection matrix if and only if\n\n  \\mathbf{P}^{H} = \\mathbf{P}.\n  \n:::{prf:proof} \\mathbf{P}^{H} = \\mathbf{P} :class: dropdown\nAccording to the orthogonal decomposition theorem,\n\n  \\begin{aligned}\n  R (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})\n  \\\\\n  R (\\mathbf{P})\n  & = N (\\mathbf{P}^{H})^{\\perp}.\n  \\\\\n  \\end{aligned}\n  \nAccording to the property of the orthogonal projection matrix, \n  \\begin{aligned}\n  N (\\mathbf{P})^{\\perp}\n  & = N (\\mathbf{P}^{H})^{\\perp}\n  \\\\\n  \\mathbf{P}\n  & = \\mathbf{P}^{H}.\n  \\\\\n  \\end{aligned}\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#application-least-square-problem",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#application-least-square-problem",
    "title": "Orthogonal projection",
    "section": "Application: least square problem",
    "text": "Application: least square problem\nConsider the problem of solving a system of linear equation for \\mathbf{x} \\in \\mathbb{C}^{n} given \\mathbf{y} \\in \\mathbb{C}^{m} and \\mathbf{A} \\in \\mathbb{C}^{m \\times n},\n\n\\mathbf{y} = \\mathbf{A} \\mathbf{x}.\n\nThis problem has a solution only when \\mathbf{y} \\in R (\\mathbf{A}). When it has no solution, the objective is changed to solve the least square problem\n\n\\mathbf{x}^{*} = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\nso that \\mathbf{A} \\mathbf{x} can be as close to \\mathbf{y} as possible.\nSolving the least square problem is the same as solving an orthogonal projection problem,\n\n\\begin{aligned}\n\\mathbf{x}^{*}\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}^{2}\n\\\\\n& = \\min_{\\mathbf{x} \\in \\mathbb{C}^{n}} \\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2}\n& [\\lVert \\mathbf{y} - \\mathbf{A} \\mathbf{x} \\rVert_{2} \\geq 0]\n\\\\\n\\mathbf{z}^{*}\n& = \\min_{\\mathbf{z} \\in R (\\mathbf{A})} \\lVert \\mathbf{y} - \\mathbf{z} \\rVert_{2}\n& [\\mathbf{z} = \\mathbf{A} \\mathbf{x}, \\mathbf{z}^{*} = \\mathbf{A} \\mathbf{x}^{*}].\n\\end{aligned}\n\nwhich is the problem of finding the closest point of \\mathbf{y} on R (\\mathbf{A}),\n\n\\begin{aligned}\n\\mathbf{z}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\end{aligned}\n\nWe can then deduce the system of normal equations:\n\n\\mathbf{A} \\mathbf{x}^{*} = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y} \\iff \\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*} = \\mathbf{A}^{H} \\mathbf{y}.\n\n:::{prf:proof} Normal equations from orthogonal projection :class: dropdown\nFirst multiplying both ends by \\mathbf{P}_{R (\\mathbf{A})} and use the projector property,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})}^{2} \\mathbf{y}\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n& [\\mathbf{P}_{R (\\mathbf{A})}^{2} = \\mathbf{P}_{R (\\mathbf{A})}]\n\\\\\n\\end{aligned}\n\nThen,\n\n\\begin{aligned}\n\\mathbf{P}_{R (\\mathbf{A})} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{P}_{R (\\mathbf{A})} \\mathbf{y}.\n\\\\\n\\mathbf{P}_{R (\\mathbf{A})} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\end{aligned}\n\nwhich shows that\n\n\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y} \\in N (\\mathbf{P}_{R (\\mathbf{A})}).\n\nAccording to the property of the orthogonal projection matrix,\n\nN (\\mathbf{P}_{R (\\mathbf{A})}) = R (\\mathbf{A})^{\\perp} = N (\\mathbf{A}^{H}),\n\nwhich means\n\n\\begin{aligned}\n\\mathbf{A}^{H} (\\mathbf{A} \\mathbf{x}^{*} - \\mathbf{y})\n& = 0\n\\\\\n\\mathbf{A}^{H} \\mathbf{A} \\mathbf{x}^{*}\n& = \\mathbf{A}^{H} \\mathbf{y}.\n\\end{aligned}\n\n:::\n(affine-projection)="
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#affine-space",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#affine-space",
    "title": "Orthogonal projection",
    "section": "Affine space",
    "text": "Affine space\nA set of vectors in the vector space \\mathcal{V} forms an affine space \\mathcal{A} if they are the sums of the vectors in a subspace \\mathcal{M} \\subset \\mathcal{V} and a non-zero vector v \\in \\mathcal{V},\n\n\\mathcal{A} = v + \\mathcal{M}.\n\nThat is, all vectors in \\mathcal{A} are sums of vectors in \\mathcal{M} and v.\n\n\\mathcal{A} is a subspace as it does NOT necessarily contain 0 vector.\n\\mathcal{A} can visualized as the subspace \\mathcal{M} translated away from origin through v."
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#affine-projection-1",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#affine-projection-1",
    "title": "Orthogonal projection",
    "section": "Affine projection",
    "text": "Affine projection\nAlthough affine spaces are not a subspaces, the concept of orthogonal projection can also be applied to affine spaces.\nGiven a vector b \\in \\mathcal{V} and an affine space \\mathcal{A}, the affine projection a \\in \\mathcal{A} is the orthogonal projection of b onto the affine space \\mathcal{A} and can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v),\n\nwhere \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of \\mathcal{M}.\n:::{prf:proof} Affine projection :class: dropdown\nSince the affine space \\mathcal{A} is the set of vectors translated from \\mathcal{M} through the vector v, the affine projection a of b onto \\mathcal{A} is the translated version of orthogonal projection of the translated version of b onto the translated version of \\mathcal{A}.\nThus, the affine projection onto \\mathcal{A} is a orthogonal projection onto \\mathcal{M} with a translated input and output.\nBy the definition of the orthogonal projection,\n\na - v = \\mathbf{P}_{\\mathcal{M}} (b - v)\n\nwhere a - v is the translated version of a, b - v the translated version of b, and \\mathbf{P}_{\\mathcal{M}} is the orthogonal projection matrix of subspace \\mathcal{M}.\nThus, the affine projection a of b onto \\mathcal{A} can be expressed as\n\na = v + \\mathbf{P}_{\\mathcal{M}} (b - v).\n\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#hyperplanes",
    "href": "Knowledge/Linear Algebra/12_Orthogonal_Projection_and_Affine_Projection.html#hyperplanes",
    "title": "Orthogonal projection",
    "section": "Hyperplanes",
    "text": "Hyperplanes\nAn affine space \\mathcal{H} = \\mathbf{v} + \\mathcal{M} \\subseteq \\mathbb{R}^{n} for which \\text{dim} (\\mathcal{M}) = n − 1 is called a hyperplane, and is usually expressed as the set\n\n\\mathcal{H} = \\{ \\mathbf{x} | \\mathbf{w}^{T} \\mathbf{x} = \\beta \\}\n\nwhere \\beta is a scalar and \\mathbf{w} is a non-zero vector.\nIn this case, the hyperplane can be viewed as the subspace\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}\n\ntranslated by the vector\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}.\n\n:::{prf:proof} \\mathcal{H} = \\{ \\mathbf{x} | \\mathbf{w}^{T} \\mathbf{x} = \\beta \\} represents an affine space in \\mathbb{R}^{n} :class: dropdown\nThe set \\mathcal{H} contains the general solutions of the linear system \\mathbf{w}^{T} \\mathbf{x} = \\beta.\nA particular solution to the linear system is\n\n\\begin{aligned}\n\\mathbf{x}\n& = (\\mathbf{w}^{T})^{-1} \\beta\n\\\\\n& = (\\mathbf{w}^{T})^{T} (\\mathbf{w}^{T} (\\mathbf{w}^{T})^{T})^{-1} \\beta\n\\\\\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\beta\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n& [\\beta, \\mathbf{w}^{T} \\mathbf{w} \\text{ are scalars}].\n\\end{aligned}\n\nAccording to the orthogonal decomposition theorem, the general solution to the linear system can be expressed as\n\n\\begin{aligned}\nN (\\mathbf{w}^{T})\n& = R (\\mathbf{w})^{\\perp}\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0, \\forall \\mathbf{y} \\in R (\\mathbf{w}) \\}\n& [\\text{def of perp}]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\alpha \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } R (\\mathbf{w})]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\alpha \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0, \\forall \\alpha \\in \\mathbb{R} \\}\n& [\\text{def of } \\langle \\cdot, \\cdot \\rangle]\n\\\\\n& = \\{ \\mathbf{x} \\mid \\langle \\mathbf{x}, \\mathbf{w} \\rangle = 0 \\}\n\\\\\n& = \\mathbf{w}^{\\perp}.\n\\end{aligned}\n\nSince the general solution of any linear system is the sum of a particular solution and the general solution of the associated homogeneous equation, the set of general solutions is expressed as\n\n\\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{w}^{\\perp}.\n\nThus,\n\n\\mathcal{H} = \\mathbf{v} + \\mathcal{M},\n\nwhere\n\n\\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w},\n\n\n\\mathcal{M} = \\mathbf{w}^{\\perp}.\n\nTODO: explain the general solution of any linear system.\n:::\nThe orthogonal projection \\mathbf{a} of a point \\mathbf{b} \\in \\mathbb{R}^{n} onto the hyperplane \\mathcal{H} is given by\n\n\\mathbf{a} = \\mathbf{b} - \\left(\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{b} - \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\mathbf{w}.\n\n:::{prf:proof} Orthogonal projection onto a hyperplane is $ = - ( {^{T}} ) $ :class: dropdown\nSince we know that \\mathcal{H} is an affine space with \\mathcal{W} = \\mathbf{w}^{\\perp} and \\mathbf{v} = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}, the affine projection can be expressed as\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\mathbf{v} + \\mathbf{P}_{\\mathcal{M}} (\\mathbf{b} - \\mathbf{v}),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\end{aligned}\n\nAccording to the property of projection matrix,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{w}}.\n\nDenoted the basis of subspace \\mathcal{M} as the columns of matrix \\mathbf{M}, the orthogonal projection matrix onto \\mathcal{M} is\n\n\\begin{aligned}\n\\mathbf{P}_{\\mathcal{M}}\n& = \\mathbf{M} (\\mathbf{M}^{T} \\mathbf{M})^{-1} \\mathbf{M}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\mathbf{w} (\\mathbf{w}^{T} \\mathbf{w})^{-1} \\mathbf{w}^{T}\n\\\\\n\\mathbf{P}_{\\mathbf{w}}\n& = \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\\end{aligned}\n\nThus,\n\n\\mathbf{P}_{\\mathbf{w}^{\\perp}} = \\mathbf{I} - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}}.\n\nPlugging it back into the top equation\n\n\\begin{aligned}\n\\mathbf{a}\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\mathbf{P}_{\\mathbf{w}^{\\perp}} \\left(\n    \\mathbf{b} - \\frac{\n        \\beta\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    } \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\n    \\beta\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} + \\left(\n    \\mathbf{I} - \\frac{\n        \\mathbf{w} \\mathbf{w}^{T}\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right) \\left(\n    \\mathbf{b} - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right),\n\\\\\n& = \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w} + \\left(\n    \\mathbf{b}\n    - \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n    - \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n    + \\frac{\\mathbf{w}^{T} \\mathbf{w} \\beta}{\\mathbf{w}^{T} \\mathbf{w} \\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\right)\n\\\\\n& = \\mathbf{b}\n- \\frac{\\mathbf{w} \\mathbf{w}^{T}}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{b}\n+ \\frac{\\beta}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#preliminary",
    "href": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#preliminary",
    "title": "Generalized Additive (Linear) Models (GAM)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nGaussian (normal) distribution\n\nTODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#linear-regression-in-a-probablitiy-view",
    "href": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#linear-regression-in-a-probablitiy-view",
    "title": "Generalized Additive (Linear) Models (GAM)",
    "section": "## Linear regression in a probablitiy view",
    "text": "## Linear regression in a probablitiy view\n\nLinear combination of Gaussian distributions\nGaussian distribution is closed under linear transformation, which means:\n\nLet X \\sim \\mathcal{N}(\\mu, \\sigma^{2}). Then the random variable Y = aX + b also follows Gaussian distribution\n Y \\sim \\mathcal{N}(a\\mu + b, a\\sigma^{2}) \nLet X \\sim \\mathcal{N}(\\mu_{X}, \\sigma_{X}^{2}) and Y \\sim \\mathcal{N}(\\mu_{Y}, \\sigma_{Y}^{2}). Then, if X and Y are independent, the random variable Z = X + Y also follows Gaussian distribution:\n Z \\sim \\mathcal{N}(\\mu_{X} + \\mu_{Y}, \\sigma_{X}^{2} + \\sigma_{Y}^{2}) \nLet X_1, \\dots, X_n be n mutually independent Gaussian random variables with means \\mu_{1}, \\dots, \\mu_{n} and variances \\sigma_{1}^{2}, \\dots \\sigma_{n}^{2}. If the random variable Y is a linear combinations of the X with w_{1}, \\dots, w_{n} coefficients and b_{1}, \\dots, b_{n} biases:\n Y = \\sum_{i=1}^{n} w_{i}X_{i} + b_{i}\nthen Y also follows Gaussian distribution:\n Y \\sim \\mathcal{N}(\\sum_{i=1}^{n} w_{i}\\mu_{i} + b_{i}, \\sum_{i=1}^{n} w_{i}^2\\sigma_{i}^2) \n\n\n\nAssumptions of linear regression\nGiven a dataset \\mathbf{X} \\in \\mathbb{R}^{n \\times d} with each feature treated as a random variable (X_{1}, X_{2}, \\dots, X_{d}), then linear regression of the form\n y = \\mathbf{w}\\mathbf{x} + b \nwill have the following assumptions to work properly: 1. The input features are independent from each other. 1. The output distribution given the input features follows a Gaussian distribution. 1. The true relationship between each feature and the output is linear.\nHowever, in most of the real world applications, the assumptions above can hardly be satisfied and can be remedied by following methods accordingly: 1. Use PCA to extract a set of independent features from all features that are not natually mutually independent. 1. Use GLM to model non-Gaussian output distribution. 1. Use GAM to model non-linear relation between features and the output."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#generalized-linear-models-glm",
    "href": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#generalized-linear-models-glm",
    "title": "Generalized Additive (Linear) Models (GAM)",
    "section": "Generalized Linear Models (GLM)",
    "text": "Generalized Linear Models (GLM)\n\nGeneralized linear models (GLM) extends the linear regression model by using a non-linear function that connects the weighted sum of the Gaussian input features with the expected mean of the non-Gaussian output distribution.\nGLM can be expressed as:\n g(\\mathbb{E}(Y)) = \\sum_{i=1}^{n} w_{i}X_{i} + b \nwhere X_{1}, X_{2}, \\dots X_{n} are the input random variables and Y is the output random variable.\nThe components of GLM can be formalized as: 1. Random component: the probability distribution of the output variable Y. It’s expected value (mean value) is \\mathbb{E}(Y). 1. Systematic component: the weighted sum \\sum_{i=1}^{n} w_{i}X_{i} + b. 1. Link function: the relation (often non-linear) g(\\cdot) between the random component and the systematic component.\n\nLogistic regression as a GLM for Bernoulli distribution\n\nTODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#generalized-addictive-models-gam",
    "href": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#generalized-addictive-models-gam",
    "title": "Generalized Additive (Linear) Models (GAM)",
    "section": "## Generalized Addictive Models (GAM)",
    "text": "## Generalized Addictive Models (GAM)\n g(\\mathrm{E}(Y)) = \\sum_{1}^{n}f(x_i)  where f() can be arbitrarily defined function.\n\nTODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#references",
    "href": "Knowledge/Supervised Learning/Generalized Additive (Linear) Models (GAM).html#references",
    "title": "Generalized Additive (Linear) Models (GAM)",
    "section": "References",
    "text": "References\n\n\nhttps://christophm.github.io/interpretable-ml-book/extend-lm.html\nhttp://www.stat.ucla.edu/~nchristo/introstatistics/introstats_normal_linear_combinations.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#preliminary",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#preliminary",
    "title": "Logistic Regression (LR)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nOptimization\n\nConvex function\n\nTODO\n\n\n\nGradient descent\n\nTODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#linear-regression-for-regression-problems",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#linear-regression-for-regression-problems",
    "title": "Logistic Regression (LR)",
    "section": "## Linear regression for regression problems",
    "text": "## Linear regression for regression problems\n\nLinear function\nIf a function with single input and output y = f(x) is a linear function, it represents a straight line on the coordinate plane. Thus it has the form:\n y = wx + b \nwhere w is the slope and b is the y-intercept of the line.\nA linear function can also take multiple inputs and we can represent n inputs with a vector \\mathbf{x} \\in \\mathbb{R}^{d}. Then the equation changes to:\n y = \\mathbf{w} \\cdot \\mathbf{x} + b \nwhere \\mathbf{w} \\in \\mathbb{R}^{d} is called weights or the weight vector and b \\in \\mathbb{R} is called bias.\nLinear regression is a supervised regression model that simply fits a linear function between the inputs \\mathbf{x} and output y.\n\n\nLoss function\nLoss function is a function that measures the error between the labels predicted by your current model and the true labels for a set of instances (training set).\nLinear regression minimizes the a particular loss function called mean squared error (MSE) function. Given a set of training labels \\mathbf{y} = y_{1}, y_{2}, \\dots, y_{n} for n training instances and the predicted labels \\hat{\\mathbf{y}} = \\hat{y}_{1}, \\hat{y}_{2}, \\dots, \\hat{y}_{n} for the same instances, MSE is defined as:\n \\operatorname{MSE}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i}^{n}(\\hat{y}_{i} - y_{i})^{2} \nThus, fitting a linear regression model for a training set \\mathbf{X} \\in \\mathbb{R}^{n \\times d} with labels \\mathbf{y} \\in \\mathbb{R}^{n} is a convex optimization problem:\n\n\\begin{align}\n\\min & \\quad \\lVert (\\mathbf{X}\\mathbf{w} + b) - \\mathbf{y} \\rVert_{2}^{2} \\\\\n= \\min & \\quad \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} \\\\\n\\end{align}\n\n\n\nRegularization\nRegularization is a technique used to avoid over-fitting of the machine learning models. Regularization works by adding an additional penalty term in the loss function to penalize aspects of the model other than the prediction error.\n L_{\\text{new}} = L_{\\text{old}} + \\lambda L_{\\text{reg}} \nwhere \\lambda is a hyperparamter that adjust the trade-off between the primary objective and the regularization.\nFor linear regression and other many models, usually L_{2} regularization (Ridge) and L_{1} regularization (Lasso) are used. - Ridge penalizes the L_{2} norm of the weights\n\n\\begin{align}\nL_{\\text{ridge}} & = \\lVert \\mathbf{w} \\rVert_{2}^{2} \\\\\n& = \\sum_{i=1}^{\\lvert \\mathbf{w} \\rvert} w_{i}^{2} \\\\\n\\end{align}\n\n\nLasso penalizes the L_{1} norm of the weights\n\n\n\\begin{align}\nL_{\\text{lasso}} & = \\lVert \\mathbf{w} \\rVert_{1} \\\\\n& = \\sum_{i=1}^{\\lvert \\mathbf{w} \\rvert} \\lvert w_{i} \\rvert \\\\\n\\end{align}\n\nBoth Ridge and Lasso penalize the magnitude of the weights. Large weights tend to overfit the training dataset because &gt; TODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#solving-linear-regression",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#solving-linear-regression",
    "title": "Logistic Regression (LR)",
    "section": "Solving linear regression",
    "text": "Solving linear regression\n\nHere we show how we can analytically solve linear regression to get a closed form solution.\nWe first make b as part of \\mathbf{w} to simplify the derivation process, which is done by adding b as an extra weight into \\mathbf{w} vector. The result weight vector \\mathbf{\\hat{w}} \\in \\mathbb{R}^{d + 1} has one extra dimension:\n \\mathbf{\\hat{w}} = (b, w_{1}, w_{2}, \\dots, w_{d}). \nThen we add a dummy input x_{0} = 1 to all input instances, so that\n \\mathbf{\\hat{x}} = (1, x_{1}, x_{2}, \\dots, x_{d}). \nAs a result, we have\n \\mathbf{\\hat{w}} \\cdot \\mathbf{\\hat{x}} = \\mathbf{w} \\cdot \\mathbf{x} + b. \nThe equation that we want to solve is\n \\min \\quad \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{\\hat{w}} \\cdot \\mathbf{\\hat{x}}_{i} - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\rVert_{2}^{2} \nSince this equation is a convex function, it can be directly solved by setting its derivative w.r.t its parameters (\\mathbf{\\hat{w}}) to 0."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#solving-linear-regression-1",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#solving-linear-regression-1",
    "title": "Logistic Regression (LR)",
    "section": "Solving linear regression",
    "text": "Solving linear regression\nHere we show how we can analytically solve linear regression with L_{2} regularization to get a closed form solution.\n \\min \\quad \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\rVert_{2}^{2} \nSince this equation is a convex function, it can be directly solved by taking its derivative w.r.t its parameters (\\mathbf{w} and b).\n\n\\begin{align}\n\\frac{\\partial}{\\partial \\mathbf{w}_{j}} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{\\partial}{\\partial \\mathbf{w}_{j}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i}))^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i, j} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i})) + 2 \\lambda \\mathbf{w}_{j} & = 0 \\\\\n\\end{align}\n\n\n\\begin{align}\n\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i} + (b - y_{i}))^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w} \\cdot \\mathbf{x}_{i})^{2} + 2(\\mathbf{w} \\cdot \\mathbf{x}_{i})(b - y_{i}) + (b - y_{i})^{2} + \\lambda \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^{n} 2(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{i})\\mathbf{w} + 2\\mathbf{x}_{i}(b - y_{i}) + 2 \\lambda \\mathbf{w} & = 0\\\\\n\\frac{2 \\mathbf{w}}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} \\cdot \\mathbf{x}_{i} + \\frac{2b}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} - \\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{x}_{i} y_{i} + 2 \\lambda \\mathbf{w} & = 0 \\\\\n\\end{align}\n\n\nTODO\n\n\n\\begin{align}\n\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} ((\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - y_{i})^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} (b + (\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}))^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^{n} b^{2} + 2b(\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) + (\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i})^{2} + \\lVert \\mathbf{w} \\Vert_{2}^{2} & = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^{n} 2b + 2(\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) & = 0 \\\\\n2b + \\frac{2}{n} \\sum_{i=1}^{n} （\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) & = 0 \\\\\nb & = - \\frac{1}{n} \\sum_{i=1}^{n} （\\mathbf{w} \\cdot \\mathbf{x}_{i} - y_{i}) \\\\\n\\end{align}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#logistic-regression-for-classification-problems",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#logistic-regression-for-classification-problems",
    "title": "Logistic Regression (LR)",
    "section": "## Logistic regression for classification problems",
    "text": "## Logistic regression for classification problems\n\nFrom regression to classification using sigmoid function\nHow can we use linear regression on a binary classification problem where the labels are 0 and 1?\nAnswer: take the output of a linear regression model and pass it to a sigmoid (logistic) function:\n \\sigma(x) = \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}} \nSigmoid function has the following characterstics that are suitable for binary classification 1. Sigmoid function maps range (-\\inf, \\inf) to range (0, 1), which can be interpreted as the possibility of being class 1. 1. Positive inputs map to output larger than 0.5 and negative inputs map to output less than 0.5.\nThus, given an instance \\mathbf{x}, the binary output can be derived by setting a threshold \\theta (usually set to 0.5) to the output of the logistic regression,\n \\hat{y} = \\sigma(\\mathbf{w}\\mathbf{x} + b) \n\n\\hat{y}_{\\text{label}} =\n\\begin{cases}\n1, & \\hat{y} \\geq \\theta \\\\\n0, & \\hat{y} &lt; \\theta \\\\\n\\end{cases}\n\nNote another commonly used function is logit function,\n \\sigma^{-1}(x) = \\mathrm{logit}(x) = \\log \\frac{x}{1 - x} \nThe inverse of the sigmoid function is the logit function, which can be derived by exchange the input and the output of the sigmoid function:\n\n\\begin{align}\nx &= \\frac{1}{1 + e^{-y}} \\\\\n\\frac{1}{x} &= 1 + e^{-y} \\\\\ne^{-y} &= \\frac{1 - x}{x} \\\\\ne^{y} &= \\frac{x}{1 - x} \\\\\ny &= \\log\\frac{x}{1 - x} \\\\\n\\end{align}\n\n\n\nBinary cross entropy (log loss) instead of mean squared error\nAlthough sigmoid function can work for binary classification problem, it doesn’t work quite well with MSE loss. The primary reason is that MSE with sigmoid function is not a convex function anymore.\n L_{\\text{MSE}} = \\frac{1}{n} \\sum_{i}^{n} (\\frac{1}{1 + e^{-(\\mathbf{w}\\mathbf{x} + b)}} -y_{i})^{2} = \\frac{1}{n} \\sum_{i}^{n} (\\sigma(\\mathbf{w}\\mathbf{x} + b) -y_{i})^{2} \nTo prove a function is convex or not, one way is to see if the second derivative of L_{\\text{MSE}} w.r.t to \\mathbf{w} is positive semidefinite.\n\n\\begin{align}\n\\frac{\\partial L_{\\text{MSE}}}{\\partial \\mathbf{w}} & = \\frac{\\partial L_{\\text{MSE}}}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial \\mathbf{w}} & \\text{[chain rule]} \\\\\n& = \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{1}{n} \\sum_{i}^{n} (\\sigma -y_{i})^{2} \\right) \\sigma(1 - \\sigma) \\mathbf{x} & \\text{[$\\sigma' = \\sigma(1 - \\sigma)$]} \\\\\n& = \\frac{2}{n} \\sum_{i}^{n} (\\sigma - y_{i}) \\sigma(1 - \\sigma) \\mathbf{x} \\\\\n& = \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2}  \\\\\n\\end{align}\n\n\n\\begin{align}\n\\frac{\\partial^{2} L_{\\text{MSE}}}{\\partial \\mathbf{w}^{2}} & = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2} \\right) \\\\\n& = \\frac{\\partial}{\\partial \\sigma} \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} \\sigma^{2} - \\sigma^{3} - y_{i}\\sigma - y_{i}\\sigma^{2} \\right) \\frac{\\partial \\sigma}{\\partial \\mathbf{w}}^{T} \\\\\n& = \\left( \\frac{2 \\mathbf{x}}{n} \\sum_{i}^{n} 2\\sigma - 3\\sigma^{2} - y_{i} - 2y_{i}\\sigma \\right) \\sigma(1 - \\sigma) \\mathbf{x}^{T} \\\\\n\\end{align}\n\n\nTODO: prove the hessian matrix is not positive semidefinite.\n\nThus, instead of MSE, binary cross entropy (BCE) loss (log loss) is used with sigmoid to create a convex objective.\n \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y}))) \nBCE assumes both inputs y and \\hat{y} are in the range [0, 1]. Since normally the labels y_{i} are 0 or 1, BCE can be interpreted by decomposing to two cases for each prediction and label pair:\n\n\\begin{cases}\n-\\log(\\hat{y}_{i}) &  y_{i} = 1 \\\\\n-\\log(1 - \\hat{y}_{i}) & y_{i} = 0 \\\\\n\\end{cases}\n\n\nfrom IPython.display import IFrame\nIFrame(\"https://www.desmos.com/calculator/ojpmcptvt0?embed\", width=500, height=500)"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Logistic Regression (LR).html#references",
    "href": "Knowledge/Supervised Learning/Logistic Regression (LR).html#references",
    "title": "Logistic Regression (LR)",
    "section": "References",
    "text": "References\n\n\nhttps://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c\nhttps://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/notes_on_linear_regression.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Adaboost (ADA).html#weak-learner",
    "href": "Knowledge/Supervised Learning/Adaboost (ADA).html#weak-learner",
    "title": "AdaBoost (Ada)",
    "section": "Weak learner",
    "text": "Weak learner\n\n\nTODO"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Adaboost (ADA).html#adaboost-algorithm",
    "href": "Knowledge/Supervised Learning/Adaboost (ADA).html#adaboost-algorithm",
    "title": "AdaBoost (Ada)",
    "section": "## AdaBoost algorithm",
    "text": "## AdaBoost algorithm\nGiven a dataset \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} \\in \\mathbb{R}^{d} with labels y_{1}, y_{2}, \\dots, y_{n} \\in \\{-1, 1\\}.\n\nInitialize the observation weight D_{1} for each instance.\n D_{1}(x_{i}) = \\frac{1}{n}, \\quad i \\in \\{1, 2, \\dots, n\\} \nFor t = 1 \\dots T,\n\nFit a classifier h_{t} to the training instances with weights D_{t}.\nCompute the error rate of the classifier:\n \\epsilon_{t} = \\sum_{i=1}^{n} D_{t}(x_{i}) \\times \\mathbb{1}(y_i \\neq h_{t}(x_i)) \nCompute the classifier weight:\n \\alpha_{t} = \\frac{1}{2} \\ln \\frac{1 - \\epsilon_{t}}{\\epsilon_{t}}  \nUpdate the observation weight for each instance:\n D_{t + 1}(x_{i}) = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)), \\quad i \\in \\{1, 2, \\dots, n\\} \nwhere Z_{t} is the normalized factor that makes \\sum_{i = 1}^{n} D_{t + 1}(x_{i}) = 1.\n {Z_{t}} = \\sum_{i = 1}^{n} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \n\nFinal output of AdaBoost:\n F(x) = \\operatorname{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_{t}(x) \\right) \n\n\nFacts about AdaBoost:\n\n\\alpha_{t} is always positive.\n:::{admonition} Proof :class: dropdown\nSince the requirement for the weak learner is to have the weighed training error \\epsilon_{t} &lt; \\frac{1}{2} and thus $ 1 - _{t} $,\n \\frac{1 - \\epsilon_{t}}{\\epsilon_{t}} &gt; 1 \\Rightarrow \\alpha_{t} = \\frac{1}{2} \\ln{\\frac{1 - \\epsilon_{t}}{\\epsilon_{t}}} &gt; 0 \n:::\nThe smaller the classification error of h_{t}, the larger the weight \\alpha_{t} and thus the more impact that h_{t} will have on the final classifier.\nThe weights of correctly classified points are reduced and the weights of incorrectly classified points are increased. Hence, the incorrectly classified points will receive more attention in the next run.\n:::{admonition} Proof :class: dropdown\n\n\\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) =\n\\begin{cases}\n\\exp(-\\alpha_{t}) & \\text{if } y_{i} = h_{t}(x_{i}) \\\\\n\\exp(\\alpha_{t}) & \\text{if } y_{i} \\neq h_{t}(x_{i}) \\\\\n\\end{cases}\n\n:::\nThe weighted training error of the weak rule h_{t} on the reweighted training set in the next iteration with weights D_{t + 1} is always 0.5.\n:::{admonition} Proof :class: dropdown\n\nAssume that the weighted training error of h_{t} on iteration t is \\epsilon,\n \\epsilon = \\sum_{x_{i} \\in E} D_{t}(x_{i}) \nwhere E is the set of the incorrectly classified instances.\nBy adding h_{t}, the new weights of the incorrectly classified training instances are\n\n\\begin{align}\nD_{t + 1}(x_{i}) & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\exp(-\\alpha_{t} \\times (-1)) \\\\\n& = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\exp \\left(\\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon}{\\epsilon} \\right) \\right) \\\\\n& = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}}\n\\end{align}\n\nSince the set of incorrectly classified instances E doesn’t change, the weighted training error of h_{t} on the reweighted training set is\n\n\\begin{align}\n\\hat{\\epsilon} & = \\sum_{x_{i} \\in E} D_{t + 1}(x_{i}) \\\\\n& = \\frac{1}{Z_{t}} \\sum_{x_{i} \\in E} D_{t}(x_{i}) \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}} \\\\\n& = \\frac{1}{Z_{t}} \\epsilon \\sqrt{\\frac{1 - \\epsilon}{\\epsilon}} \\\\\n& = \\frac{1}{Z_{t}} \\sqrt{(1 - \\epsilon) \\epsilon}\n\\end{align}\n\nDenote C to be the set of the correctly classified instances.\n\n\\begin{align}\n{Z_{t}} & = \\sum_{i = 1}^{n} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n& = \\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) + \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) \\\\\n& = \\sqrt{(1 - \\epsilon) \\epsilon} + (1 - \\epsilon) \\sqrt{\\frac{\\epsilon}{1 - \\epsilon}} \\\\\n& = 2 \\sqrt{(1 - \\epsilon) \\epsilon} \\\\\n\\end{align}\n\nThus,\n \\hat{\\epsilon} = \\frac{\\sqrt{(1 - \\epsilon) \\epsilon}}{2 \\sqrt{(1 - \\epsilon) \\epsilon}} = 0.5 \n\n:::"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Adaboost (ADA).html#convergence-analysis",
    "href": "Knowledge/Supervised Learning/Adaboost (ADA).html#convergence-analysis",
    "title": "AdaBoost (Ada)",
    "section": "Convergence analysis",
    "text": "Convergence analysis\n\nAdaBoost is a greedy algorithm that minimizes a loss function that upper bounds the classification error.\n\nAdaBoost is a greedy algorithm that minimizes the loss function:\n \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \n:::{admonition} Proof :class: dropdown\nWe can expand D_{t + 1}(x_{i}) recursively\n\n\\begin{align}\nD_{t + 1}(x_{i}) & = \\frac{1}{Z_{t}} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n& = \\frac{1}{Z_{t}} \\left( \\frac{1}{Z_{t - 1}} D_{t - 1}(x_{i}) \\times \\exp(-\\alpha_{t - 1} \\times y_{i}h_{t - 1}(x_i)) \\right) \\times \\exp(-\\alpha_{t} \\times y_{i}h_{t}(x_i)) \\\\\n& = \\frac{1}{Z_{t} Z_{t - 1}} D_{t - 1}(x_{i}) \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i))) \\\\\n& = \\dots & [\\text{expand } Z_{t - 1}, Z_{t - 2}, \\dots, Z_{1}] \\\\\n& = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} D_{1}(x_{i}) \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))) \\\\\n& = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}(\\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))) & [D_{1}(x_{i}) = \\frac{1}{n}] \\\\\n& = \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}F_{t}(x_{i})) & [F_{t}(x_{i}) = \\alpha_{t}h_{t}(x_i) + \\alpha_{t - 1}h_{t - 1}(x_i) + \\dots \\alpha_{1}h_{1}(x_i))] \\\\\n\\end{align}\n\nSince the sum of the weights is 1:\n\n\\begin{align}\n1 & = \\sum_{i = 1}^{n} D_{t + 1}(x_{i})  \\\\\n1 & = \\sum_{i = 1}^{n} \\frac{1}{\\prod_{k=1}^{t} Z_{k}} \\frac{1}{n} \\times \\exp(- y_{i}F_{t}(x_{i})) \\\\\n\\prod_{k=1}^{t} Z_{k} & = \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \\\\\n\\end{align}\n\nThus, the loss function is the sum of the normalization terms.\nIn iteration t, all previous Z_{1}, Z_{2}, \\dots, Z_{t - 1} don’t depend on \\alpha_{t} and h_{t}. Thus the loss function that AdaBoost minimizes in iteration t is just Z_{t}.\n\n\\begin{align}\n\\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) & = \\prod_{k=1}^{t} Z_{k} \\\\\n  & = Z_{t} \\prod_{k=1}^{t - 1} Z_{k}\n\\end{align}\n\nWe know that Z_{t} depends on the choice of \\alpha_{t} and h_{t}. Minimizing Z_{t} can be solved by taking its derivative w.r.t \\alpha_{t} and h_{t} and set it to 0. Here we show that the \\alpha_{t} that minimizes the Z_{t} is exactly the one used in AdaBoost algorithm.\n\n\\begin{align}\n\\frac{\\partial Z_{t}}{\\partial \\alpha_{t}} & = 0 \\\\\n\\frac{\\partial}{\\partial \\alpha_{t}} \\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) + \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) & = 0 \\\\\n\\sum_{i \\in E} D_{t}(x_{i}) \\times \\exp(\\alpha_{t}) - \\sum_{i \\in C} D_{t}(x_{i}) \\times \\exp(-\\alpha_{t}) & = 0 \\\\\n\\frac{\\exp(\\alpha_{t})}{\\exp(-\\alpha_{t})} \\sum_{i \\in E} D_{t}(x_{i}) & = \\sum_{i \\in C} D_{t}(x_{i}) \\\\\n\\exp(2 \\alpha_{t}) \\epsilon & = 1 - \\epsilon \\\\\n\\alpha_{t} & = \\frac{1}{2} \\ln \\frac{1 - \\epsilon}{\\epsilon}\n\\end{align}\n\n:::\nThe loss function is an upper bound on the weighted training error (0-1 loss) in any iteration t:\n Err(F_{t}) = \\frac{1}{n} \\sum_{i}^{n} \\mathbb{1}(y_{i} \\neq \\operatorname{sign}(F_{t}(x_{i}))) \\leq \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \n:::{admonition} Proof :class: dropdown\nFor all i,\n \\mathbb{1} (y_{i} \\neq \\operatorname{sign}(F_{t}(x_{i})) = -\\operatorname{step}(y_{i} F_{t}(x_{i}) \\leq  \\exp(- y_{i}F_{t}(x_{i}) \n:::\nAfter t iterations, the weighted training error of F_{t} is bounded by\n Err(F_{t}) \\leq \\exp(-2 \\sum_{k = 1}^{t} \\gamma_{k}^{2}) \nwhere \\gamma_{k} = 0.5 - \\epsilon_{k} to denote how much better h_{t} is than the random guessing,\n:::{admonition} Proof :class: dropdown\n\n\\begin{align}\nErr(F_{t}) & \\leq \\frac{1}{n} \\sum_{i = 1}^{n} \\exp(- y_{i}F_{t}(x_{i})) \\\\\n& \\leq \\prod_{k=1}^{t} Z_{k} \\\\\n& \\leq \\prod_{k=1}^{t} 2 \\sqrt{(1 - \\epsilon_{k}) \\epsilon_{k}} \\\\\n& \\leq \\prod_{k=1}^{t} 2 \\sqrt{(0.5 + \\gamma_{k}) (0.5 - \\gamma_{k})} & [\\epsilon_{k} = 0.5 - \\gamma_{k}] \\\\\n& \\leq \\prod_{k=1}^{t} \\sqrt{1 - 4 \\gamma_{k}^{2}} \\\\\n& \\leq \\prod_{k=1}^{t} \\exp(-2 \\gamma_{k}^{2}) & [\\sqrt{1 - 4 \\gamma_{t}^{2}} \\leq \\exp(-2 \\gamma^{2})] \\\\\n& \\leq \\exp(-2 \\sum_{k=i}^{t} \\gamma_{k}^{2})\n\\end{align}\n\n:::"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Adaboost (ADA).html#references",
    "href": "Knowledge/Supervised Learning/Adaboost (ADA).html#references",
    "title": "AdaBoost (Ada)",
    "section": "## References",
    "text": "## References\n\nhttps://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html\n\nhttps://arxiv.org/pdf/1403.1452.pdf\nhttps://cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_boosting.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Random Forest (RF).html#bootstrap",
    "href": "Knowledge/Supervised Learning/Random Forest (RF).html#bootstrap",
    "title": "Random Forest (RF)",
    "section": "Bootstrap",
    "text": "Bootstrap"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Random Forest (RF).html#bagging-bootstrap-aggregating",
    "href": "Knowledge/Supervised Learning/Random Forest (RF).html#bagging-bootstrap-aggregating",
    "title": "Random Forest (RF)",
    "section": "Bagging (Bootstrap aggregating)",
    "text": "Bagging (Bootstrap aggregating)\nBagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Random Forest (RF).html#references",
    "href": "Knowledge/Supervised Learning/Random Forest (RF).html#references",
    "title": "Random Forest (RF)",
    "section": "## References",
    "text": "## References\n\nhttps://victorzhou.com/blog/intro-to-random-forests/"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Gradient Boosting (GB).html#gradient-boosting-regression-tree-gbrt-algorithm",
    "href": "Knowledge/Supervised Learning/Gradient Boosting (GB).html#gradient-boosting-regression-tree-gbrt-algorithm",
    "title": "Gradient Boosting (GB)",
    "section": "Gradient boosting regression tree (GBRT) algorithm",
    "text": "Gradient boosting regression tree (GBRT) algorithm\n\nGiven a loss function L(\\cdot), a training set X = \\{\\mathbf{x_{i}}\\}, \\mathbf{y} = \\{y_{i}\\}, a learning rate \\alpha, and a number of iterations M, the algorithm to train a GBRT is as follows: 1. Intiailize G(x) by fitting CART on D 1. For m = 1 \\dots M, 1. Evaluate the loss over the current G(x) 1. Calculate the gradient of the loss w.r.t the labels to get the residuals \\tilde{\\mathbf{y}}:  \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}} Note \\tilde{\\mathbf{y}} has the same shape as \\mathbf{y}. 1. Use X and residuals \\tilde{\\mathbf{y}} as the new training set to train a CART g(x). 1. Add the new weak leaner into the current model:  G(x) = G(x) + \\alpha g(x)"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Gradient Boosting (GB).html#references",
    "href": "Knowledge/Supervised Learning/Gradient Boosting (GB).html#references",
    "title": "Gradient Boosting (GB)",
    "section": "## References",
    "text": "## References\n\nhttps://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/0_Linear_Discriminant.html",
    "href": "Knowledge/Supervised Learning/0_Linear_Discriminant.html",
    "title": "Linear Discriminant",
    "section": "",
    "text": "(linear-discriminant)="
  },
  {
    "objectID": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#preliminary",
    "href": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#preliminary",
    "title": "Linear Discriminant",
    "section": "Preliminary",
    "text": "Preliminary\n\nLinear Algebra\n\nAffine Projection"
  },
  {
    "objectID": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#hyperplances",
    "href": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#hyperplances",
    "title": "Linear Discriminant",
    "section": "Hyperplances",
    "text": "Hyperplances\nRecall that a hyperplane \\mathcal{H} in \\mathbb{R}^{d} is an affine space that is expressed as\n\n\\mathcal{H} = \\left\\{ \\mathbf{x} \\mid \\mathbf{w} \\mathbf{x} + b = 0 \\right\\},\n\nwhich can be viewed as the subspace \\mathbf{w}^{\\perp} translated by the vector \\mathbf{x}_{0}\n\n\\mathcal{H} = \\left\\{ \\mathbf{u} + \\mathbf{x}_{0} \\mid \\mathbf{u} \\in \\mathbf{w}^{\\perp} \\right\\}.\n\nwhere\n\n\\mathbf{w}^{\\perp} is the subspace that is perpendicular to the vector \\mathbf{w},\n\\mathbf{v} = - \\frac{b}{\\mathbf{w}^{T} \\mathbf{w}} \\mathbf{w}.\n\nAlso recall that given a vector \\mathbf{x} \\in \\mathbb{R}^{d}, its orthogonal projection onto \\mathcal{H} is\n\n\\mathbf{p} = \\mathbf{x} -\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{x} + b\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\mathbf{w}.\n\nTherefore, the distance between \\mathbf{x} and \\mathcal{H} is the length the vector that connects \\mathbf{p} and \\mathbf{x}\n\n\\begin{aligned}\n\\lVert \\mathbf{x} - \\mathbf{p} \\rVert\n& = \\left\\lVert \\frac{\n    \\mathbf{w}^{T} \\mathbf{x} + b\n}{\n    \\mathbf{w}^{T} \\mathbf{w}\n} \\mathbf{w} \\right\\rVert\n\\\\\n& = \\left\\lvert\n    \\frac{\n        \\mathbf{w}^{T} \\mathbf{x} + b\n    }{\n        \\mathbf{w}^{T} \\mathbf{w}\n    }\n\\right\\rvert \\lVert \\mathbf{w} \\rVert\n\\\\\n& = \\frac{\n    \\lvert \\mathbf{w}^{T} \\mathbf{x} + b \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert^{2}\n}\n\\lVert \\mathbf{w} \\rVert\n\\\\\n& = \\frac{\n    \\lvert \\mathbf{w}^{T} \\mathbf{x} + b \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n}.\n\\end{aligned}\n\n\nHyperplanes as linear discriminants\nThe linear function defined by\n\nf (\\mathbf{x}) = \\mathbf{w} \\mathbf{x} + b\n\ndivides the points in \\mathbb{R}^{d} into 3 spaces\n\nf (\\mathbf{x}) = 0: the points on the hyperplane \\mathcal{H}.\nf (\\mathbf{x}) &gt; 0: the points on the positive side of f (\\mathbf{x}), which is the side that \\mathbf{w} points to.\nf (\\mathbf{x}) &lt; 0: the points on the negative side of f (\\mathbf{x}).\n\nTherefore, f (\\mathbf{x}) is a linear discriminant if we classify the points in \\mathbb{R}^{d} based on the following decision rule\n\ng (\\mathbf{x}) = \\text{sign} (f (\\mathbf{x})) = \\begin{cases}\n1 & f (\\mathbf{x}) &gt; 0 \\\\\n0 & f (\\mathbf{x}) &lt; 0 \\\\\n\\end{cases}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#margin",
    "href": "Knowledge/Supervised Learning/0_Linear_Discriminant.html#margin",
    "title": "Linear Discriminant",
    "section": "Margin",
    "text": "Margin\nThe margin of the instance \\mathbf{x} is its signed distance from the hyperplane f\n\n\\gamma (\\mathbf{x}) = \\frac{\\hat{y} f (\\mathbf{x})}{\\lVert \\mathbf{w} \\rVert},\n\nwhere \\hat{y} \\in \\{1, -1\\} is the label of \\mathbf{x}, but the negative label is denoted by -1\nThe margin of a hyperplane f is the distance from the hyperplane to the closest point in the training set\n\n\\gamma = \\min_{i} \\frac{\n    \\lvert \\gamma (\\mathbf{x}) \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n} = \\min_{i} \\frac{\n    \\lvert f (\\mathbf{x}) \\rvert\n}{\n    \\lVert \\mathbf{w} \\rVert\n}.\n\nA linear discriminant with a positive margin classifies all training instances correctly\n\n\\gamma &gt; 0 \\Leftrightarrow y_i (\\mathbf{w} \\mathbf{x}_i + b) &gt; 0, \\forall i.\n\n\nMargin loss\nThe margin loss is a loss function that is defined with respect to the margin of the instances\n\nL (y, g (\\mathbf{x})) = \\phi (\\gamma (\\mathbf{x}))."
  },
  {
    "objectID": "Knowledge/Learning Theory/3_Effective_Class_Size.html",
    "href": "Knowledge/Learning Theory/3_Effective_Class_Size.html",
    "title": "Effective Class Size",
    "section": "",
    "text": "If there is an infinite number of possible hypotheses in a hypothesis class, all the bounds that are positively related to the size of the hypothesis class will be infinite as well, which provides no information and therefore become useless.\nHowever, even if a hypothesis class \\mathcal{H} can contain an infinite number of hypotheses, the number of unique ways that the hypotheses in \\mathcal{H} can label any finite set of instances is finite, which is at most 2^{n} if the set has n instances.\n\n\nGiven a set of instances \\mathcal{S} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the projection of a hypothesis class \\mathcal{H} onto \\mathcal{S} is the set of all distinct labels that \\mathcal{H} can produce onto \\mathcal{S}\n\n\\mathcal{H} (\\mathcal{S}) = \\{ \\{ h (x_{1}), \\dots, h (x_{n}) \\}, \\forall h \\in \\mathcal{H} \\}.\n\nThe concept of the growth function (shattering coefficient) is to measure the richness of the decisions that a hypothesis class can make with respect to a dataset of size n.\n\nDefinition 1 The growth function of a hypothesis class \\mathcal{H} is defined as the maximum number of unique ways that the hypotheses in \\mathcal{H} can label any set of n instances\n\n\\Pi_{\\mathcal{H}} (n) = \\sup_{\\mathcal{S}: \\lvert \\mathcal{S} \\rvert = n} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert.\n\n\nNote that \\Pi_{\\mathcal{H}} (n) \\leq 2^{n} for any \\mathcal{H} with binary labels.\n\n\n\nWe say that a sample \\mathcal{S} is shattered by the hypothesis class \\mathcal{H} if a \\mathcal{H} can label \\mathcal{S} in every possible way. That is, \\mathcal{S} is shattered by \\mathcal{H} if\n\n\\mathcal{H} (\\mathcal{S}) = 2^{\\lvert \\mathcal{S} \\rvert}.\n\n\nThe Vapnik-Chervonenkis (VC) dimension of \\mathcal{H} is the size of the largest set that is shattered by \\mathcal{H}\n\n\\mathrm{VC} (\\mathcal{H}) = \\max_{n: \\mathcal{H} (\\mathcal{S}) = 2^{n}} n.\n\n\n\n\n\nSauer’s lemma shows that the growth function of any hypothesis class \\mathcal{H} is upper-bounded by a function of its VC dimension.\n\nTheorem 1 For any hypothesis class \\mathcal{H} and any dataset size n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove the lemma for any set of n instances and any hypothesis class \\mathcal{H} with \\mathrm{VC} (\\mathcal{H}) = d using induction on n and d.\nThat is, we will prove\n\nthe lemma is correct under the base cases where (n, d) = (0, d) and (n, d) = (n, 0),\nthe lemma is correct under general case for (n ,d) by assuming the lemma is true under the previous cases (m, c) where m &lt; n and c &lt; d.\n\nFirst the base case (n, d) = (0, d) implies that\n\n\\Pi_{\\mathcal{H}} (0) = 1 = {0 \\choose 0} = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {0 \\choose i}\n\nbecause any hypothesis class can have at most 1 distinct label on an empty set (n = 0).\nThen the base case (n, d) = (n, 0) means that \\mathcal{H} can only shatter the empty set \\emptyset and therefore according to the definition of shattering\n\n\\Pi_{\\mathcal{H}} (n) = \\mathcal{H} (\\emptyset) = 2^{0} = 1 = {n \\choose 0} = \\sum_{i = 0}^{0} {n \\choose i}.\n\nTherefore we have proven the lemma is true in the base case, that is,\n\n\\Pi_{\\mathcal{H}} (n) = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}\n\nis true when (n, d) = (0, d) and (n, d) = (n, 0).\nTo prove the general case of the lemma, take any set \\mathcal{S} = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n} \\} with n instances and any hypothesis class \\mathcal{H}, we can classify each unique way of the label assignment \\pi \\in \\mathcal{H} (\\mathcal{S}) into 2 groups \\mathcal{G}_{1} and \\mathcal{G}_{2} based on whether \\pi can form a pair in \\mathcal{H} (\\mathcal{S}).\n\n(\\pi, \\pi') form a pair if \\pi (x_{i}) = \\pi' (x_{i}), \\forall i \\in [1, n - 1] and \\pi (x_{n}) = \\pi' (x_{n}) (\\pi agree with \\pi' for all x_{1}, \\dots, x_{n - 1} but not for x_{n}). We add the both \\pi and \\pi' to \\mathcal{G}_{1}.\n\\pi belongs to \\mathcal{G}_{2} if doesn’t belong to \\mathcal{G}_{1}.\n\nSince the pairs (\\pi, \\pi') \\in \\mathcal{G}_{1} have the same labels for \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} if we create \\mathcal{G}'_{1} by removing the labels for \\mathbf{x}_{n} in each \\pi \\in \\mathcal{G}_{1}\n\n\\mathcal{G}'_{1} = \\{ (\\pi (\\mathbf{x}_{1}), \\dots, \\pi (\\mathbf{x}_{n - 1})), \\forall \\pi \\in \\mathcal{G}_{1} \\} \\\\\n\nthen all of the pairs in \\mathcal{G}_{1} become the same label assignment, but if we create \\mathcal{G}_{2}' using the same procedure above, the resulting label assignments in \\mathcal{G}_{2}' are still unique.\nAlso, by the definition of \\mathcal{G}_{1}, \\mathcal{G}_{2}, \\mathcal{G}_{1}', and \\mathcal{G}_{2}', the label assignments in \\mathcal{G}_{1}' and \\mathcal{G}_{2}' do not overlap, and therefore\n\n\\mathcal{H} (\\mathcal{S}) = \\lvert \\mathcal{G}_{1} \\rvert + \\lvert \\mathcal{G}_{2} \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert.\n\nThen we will define 2 new hypotheses classes \\mathcal{H}_{1}, \\mathcal{H}_{2} whose domain is a set \\mathcal{S}' that is constructed by removing x_{n} from \\mathcal{S}\n\n\\mathcal{S}' = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n - 1} \\}.\n\nand whose projections on \\mathcal{S}' are defined as\n\n\\mathcal{H}_{1} (\\mathcal{S}') = \\mathcal{G}_{1}' \\cup \\mathcal{G}_{2}' \\\\\n\\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}',\n\nand therefore,\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert = (\\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert) + \\lvert \\mathcal{G}_{1}' \\rvert = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert.\n\nAlthough we never exactly defined what \\mathcal{H}_{1} and \\mathcal{H}_{2} are, we have completely specified their projections on the entire domain \\mathcal{S}', using which we can derive\n\n\\mathrm{VC} (\\mathcal{H}_{1}) \\leq \\mathrm{VC} (\\mathcal{H}),\n\nsince the \\mathcal{H} has all the same label assignments for \\mathcal{S}' = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} \\} that \\mathcal{H}_{1} has and any subset of \\mathcal{S}' that is shattered by \\mathcal{H}_{1} is shattered by \\mathcal{H}.\nFurthermore, since \\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}, if a subset \\mathcal{T} \\subseteq \\mathcal{S}' is shattered by \\mathcal{H}_{2}, then the set \\mathcal{T} \\cup \\{ x_{n} \\} must be shattered by \\mathcal{H}, which means\n\n\\mathrm{VC} (\\mathcal{H}_{2}) + 1 \\leq \\mathrm{VC} (\\mathcal{H}).\n\nNow we are ready to prove the general case of the lemma using the all results we proved above with \\mathcal{H}_{1} and \\mathcal{H}_{2}.\nAccording to the definition of growth function, for any hypothesis class \\mathcal{H} and any set \\mathcal{S} of size n\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (n)\n\nand since we have assumed that the lemma is correct for the case (m, c) where m &lt; n, c &lt; d,\n\n\\Pi_{\\mathcal{H}} (m) \\leq \\sum_{i = 0}^{c} {m \\choose i},\n\nwe have\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert\n\\\\\n& \\leq \\Pi_{\\mathcal{H}_{1}} (n - 1) + \\Pi_{\\mathcal{H}_{2}} (n - 1).\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n\\end{aligned}\n\nSince we have proved that the relations between \\mathrm{VC} (\\mathcal{H}_{1}), \\mathrm{VC} (\\mathcal{H}_{2}) and \\mathrm{VC} (\\mathcal{H}),\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}) - 1} {n - 1 \\choose i}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 1}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1} + {n - 1 \\choose -1}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1}\n& [{n - 1 \\choose - 1} = 0]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} \\left[\n    {n - 1 \\choose i} + {n - 1 \\choose i - 1}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\\end{aligned}\n\nSince all of the above proof is for any \\mathcal{S}, it also works for the largest \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert,\n\n\\sup_{\\mathcal{S}} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = \\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i},\n\nwhich proves the lemma under the general case.\n\n\n\nThe following theorem uses Sauer’s lemma to provide a closed form upper-bound of the growth function of any hypothesis class with its VC dimension.\n\nTheorem 2 For any 1 &lt; d = \\mathrm{VC} (\\mathcal{H}) &lt; n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{d} {n \\choose i} \\leq \\left(\n    \\frac{ e }{ d } n\n\\right)^{d} = O (n^d).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that (\\frac{ d }{ n })^{d} &lt; (\\frac{ d }{ n })^{i}, i &lt; d since d &lt; n, and therefore\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\end{aligned}\n\nBy applying Binomial theorem (x + y)^{n} = \\sum_{i = 0}^{n} {n \\choose i} x^{i} y^{n - i}\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& = \\left(\n    \\frac{ d }{ n } + 1\n\\right)^{n}\n\\\\\n& \\leq e^{d}.\n\\end{aligned}\n\n\n\n\nThe theorem above shows that the VC dimension marks the threshold between the exponential growth and polynomial growth of the growth function.\n\nWhen n &lt; d, by the definition of the VC dimension, we can always find a set of instances of size n \\mathcal{H} can shatter, so the growth function \\Pi_{\\mathcal{H}} (n) = 2^{n}, which means it grows exponentially with a factor of 2 as n increases,\nWhen n &gt; d, by the theorem above, the growth function is upper bounded by n^{d}, so it only grows in polynomials as n increases."
  },
  {
    "objectID": "Knowledge/Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "href": "Knowledge/Learning Theory/3_Effective_Class_Size.html#growth-function-shattering-coefficient",
    "title": "Effective Class Size",
    "section": "",
    "text": "Given a set of instances \\mathcal{S} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the projection of a hypothesis class \\mathcal{H} onto \\mathcal{S} is the set of all distinct labels that \\mathcal{H} can produce onto \\mathcal{S}\n\n\\mathcal{H} (\\mathcal{S}) = \\{ \\{ h (x_{1}), \\dots, h (x_{n}) \\}, \\forall h \\in \\mathcal{H} \\}.\n\nThe concept of the growth function (shattering coefficient) is to measure the richness of the decisions that a hypothesis class can make with respect to a dataset of size n.\n\nDefinition 1 The growth function of a hypothesis class \\mathcal{H} is defined as the maximum number of unique ways that the hypotheses in \\mathcal{H} can label any set of n instances\n\n\\Pi_{\\mathcal{H}} (n) = \\sup_{\\mathcal{S}: \\lvert \\mathcal{S} \\rvert = n} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert.\n\n\nNote that \\Pi_{\\mathcal{H}} (n) \\leq 2^{n} for any \\mathcal{H} with binary labels."
  },
  {
    "objectID": "Knowledge/Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "href": "Knowledge/Learning Theory/3_Effective_Class_Size.html#vapnik-chervonenkis-vc-dimension",
    "title": "Effective Class Size",
    "section": "",
    "text": "We say that a sample \\mathcal{S} is shattered by the hypothesis class \\mathcal{H} if a \\mathcal{H} can label \\mathcal{S} in every possible way. That is, \\mathcal{S} is shattered by \\mathcal{H} if\n\n\\mathcal{H} (\\mathcal{S}) = 2^{\\lvert \\mathcal{S} \\rvert}.\n\n\nThe Vapnik-Chervonenkis (VC) dimension of \\mathcal{H} is the size of the largest set that is shattered by \\mathcal{H}\n\n\\mathrm{VC} (\\mathcal{H}) = \\max_{n: \\mathcal{H} (\\mathcal{S}) = 2^{n}} n."
  },
  {
    "objectID": "Knowledge/Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "href": "Knowledge/Learning Theory/3_Effective_Class_Size.html#sauers-lemma",
    "title": "Effective Class Size",
    "section": "",
    "text": "Sauer’s lemma shows that the growth function of any hypothesis class \\mathcal{H} is upper-bounded by a function of its VC dimension.\n\nTheorem 1 For any hypothesis class \\mathcal{H} and any dataset size n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe will prove the lemma for any set of n instances and any hypothesis class \\mathcal{H} with \\mathrm{VC} (\\mathcal{H}) = d using induction on n and d.\nThat is, we will prove\n\nthe lemma is correct under the base cases where (n, d) = (0, d) and (n, d) = (n, 0),\nthe lemma is correct under general case for (n ,d) by assuming the lemma is true under the previous cases (m, c) where m &lt; n and c &lt; d.\n\nFirst the base case (n, d) = (0, d) implies that\n\n\\Pi_{\\mathcal{H}} (0) = 1 = {0 \\choose 0} = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {0 \\choose i}\n\nbecause any hypothesis class can have at most 1 distinct label on an empty set (n = 0).\nThen the base case (n, d) = (n, 0) means that \\mathcal{H} can only shatter the empty set \\emptyset and therefore according to the definition of shattering\n\n\\Pi_{\\mathcal{H}} (n) = \\mathcal{H} (\\emptyset) = 2^{0} = 1 = {n \\choose 0} = \\sum_{i = 0}^{0} {n \\choose i}.\n\nTherefore we have proven the lemma is true in the base case, that is,\n\n\\Pi_{\\mathcal{H}} (n) = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}\n\nis true when (n, d) = (0, d) and (n, d) = (n, 0).\nTo prove the general case of the lemma, take any set \\mathcal{S} = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n} \\} with n instances and any hypothesis class \\mathcal{H}, we can classify each unique way of the label assignment \\pi \\in \\mathcal{H} (\\mathcal{S}) into 2 groups \\mathcal{G}_{1} and \\mathcal{G}_{2} based on whether \\pi can form a pair in \\mathcal{H} (\\mathcal{S}).\n\n(\\pi, \\pi') form a pair if \\pi (x_{i}) = \\pi' (x_{i}), \\forall i \\in [1, n - 1] and \\pi (x_{n}) = \\pi' (x_{n}) (\\pi agree with \\pi' for all x_{1}, \\dots, x_{n - 1} but not for x_{n}). We add the both \\pi and \\pi' to \\mathcal{G}_{1}.\n\\pi belongs to \\mathcal{G}_{2} if doesn’t belong to \\mathcal{G}_{1}.\n\nSince the pairs (\\pi, \\pi') \\in \\mathcal{G}_{1} have the same labels for \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} if we create \\mathcal{G}'_{1} by removing the labels for \\mathbf{x}_{n} in each \\pi \\in \\mathcal{G}_{1}\n\n\\mathcal{G}'_{1} = \\{ (\\pi (\\mathbf{x}_{1}), \\dots, \\pi (\\mathbf{x}_{n - 1})), \\forall \\pi \\in \\mathcal{G}_{1} \\} \\\\\n\nthen all of the pairs in \\mathcal{G}_{1} become the same label assignment, but if we create \\mathcal{G}_{2}' using the same procedure above, the resulting label assignments in \\mathcal{G}_{2}' are still unique.\nAlso, by the definition of \\mathcal{G}_{1}, \\mathcal{G}_{2}, \\mathcal{G}_{1}', and \\mathcal{G}_{2}', the label assignments in \\mathcal{G}_{1}' and \\mathcal{G}_{2}' do not overlap, and therefore\n\n\\mathcal{H} (\\mathcal{S}) = \\lvert \\mathcal{G}_{1} \\rvert + \\lvert \\mathcal{G}_{2} \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert.\n\nThen we will define 2 new hypotheses classes \\mathcal{H}_{1}, \\mathcal{H}_{2} whose domain is a set \\mathcal{S}' that is constructed by removing x_{n} from \\mathcal{S}\n\n\\mathcal{S}' = \\{\\mathbf{x}_{1} \\dots, \\mathbf{x}_{n - 1} \\}.\n\nand whose projections on \\mathcal{S}' are defined as\n\n\\mathcal{H}_{1} (\\mathcal{S}') = \\mathcal{G}_{1}' \\cup \\mathcal{G}_{2}' \\\\\n\\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}',\n\nand therefore,\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = 2 \\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert = (\\lvert \\mathcal{G}_{1}' \\rvert + \\lvert \\mathcal{G}_{2}' \\rvert) + \\lvert \\mathcal{G}_{1}' \\rvert = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert.\n\nAlthough we never exactly defined what \\mathcal{H}_{1} and \\mathcal{H}_{2} are, we have completely specified their projections on the entire domain \\mathcal{S}', using which we can derive\n\n\\mathrm{VC} (\\mathcal{H}_{1}) \\leq \\mathrm{VC} (\\mathcal{H}),\n\nsince the \\mathcal{H} has all the same label assignments for \\mathcal{S}' = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n - 1} \\} that \\mathcal{H}_{1} has and any subset of \\mathcal{S}' that is shattered by \\mathcal{H}_{1} is shattered by \\mathcal{H}.\nFurthermore, since \\mathcal{H}_{2} (\\mathcal{S}') = \\mathcal{G}_{1}, if a subset \\mathcal{T} \\subseteq \\mathcal{S}' is shattered by \\mathcal{H}_{2}, then the set \\mathcal{T} \\cup \\{ x_{n} \\} must be shattered by \\mathcal{H}, which means\n\n\\mathrm{VC} (\\mathcal{H}_{2}) + 1 \\leq \\mathrm{VC} (\\mathcal{H}).\n\nNow we are ready to prove the general case of the lemma using the all results we proved above with \\mathcal{H}_{1} and \\mathcal{H}_{2}.\nAccording to the definition of growth function, for any hypothesis class \\mathcal{H} and any set \\mathcal{S} of size n\n\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (n)\n\nand since we have assumed that the lemma is correct for the case (m, c) where m &lt; n, c &lt; d,\n\n\\Pi_{\\mathcal{H}} (m) \\leq \\sum_{i = 0}^{c} {m \\choose i},\n\nwe have\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& = \\lvert \\mathcal{H}_{1} (\\mathcal{S}') \\rvert + \\lvert \\mathcal{H}_{2} (\\mathcal{S}') \\rvert\n\\\\\n& \\leq \\Pi_{\\mathcal{H}_{1}} (n - 1) + \\Pi_{\\mathcal{H}_{2}} (n - 1).\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n\\end{aligned}\n\nSince we have proved that the relations between \\mathrm{VC} (\\mathcal{H}_{1}), \\mathrm{VC} (\\mathcal{H}_{2}) and \\mathrm{VC} (\\mathcal{H}),\n\n\\begin{aligned}\n\\lvert \\mathcal{H} (\\mathcal{S}) \\rvert\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{1})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}_{2})} {n - 1 \\choose i}\n\\\\\n& \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H}) - 1} {n - 1 \\choose i}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 1}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1} + {n - 1 \\choose -1}\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i} + \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n - 1 \\choose i - 1}\n& [{n - 1 \\choose - 1} = 0]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} \\left[\n    {n - 1 \\choose i} + {n - 1 \\choose i - 1}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i}.\n\\end{aligned}\n\nSince all of the above proof is for any \\mathcal{S}, it also works for the largest \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert,\n\n\\sup_{\\mathcal{S}} \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert = \\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{\\mathrm{VC} (\\mathcal{H})} {n \\choose i},\n\nwhich proves the lemma under the general case.\n\n\n\nThe following theorem uses Sauer’s lemma to provide a closed form upper-bound of the growth function of any hypothesis class with its VC dimension.\n\nTheorem 2 For any 1 &lt; d = \\mathrm{VC} (\\mathcal{H}) &lt; n, we have\n\n\\Pi_{\\mathcal{H}} (n) \\leq \\sum_{i = 0}^{d} {n \\choose i} \\leq \\left(\n    \\frac{ e }{ d } n\n\\right)^{d} = O (n^d).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that (\\frac{ d }{ n })^{d} &lt; (\\frac{ d }{ n })^{i}, i &lt; d since d &lt; n, and therefore\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i}\n\\right]\n\\\\\n& = \\sum_{i = 0}^{d} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\end{aligned}\n\nBy applying Binomial theorem (x + y)^{n} = \\sum_{i = 0}^{n} {n \\choose i} x^{i} y^{n - i}\n\n\\begin{aligned}\n\\sum_{i = 0}^{d} {n \\choose i} \\left(\n    \\frac{ d }{ n }\n\\right)^{d}\n& \\leq \\sum_{i = 0}^{n} \\left[\n    {n \\choose i} \\left(\n        \\frac{ d }{ n }\n    \\right)^{i} 1^{n - i}\n\\right]\n\\\\\n& = \\left(\n    \\frac{ d }{ n } + 1\n\\right)^{n}\n\\\\\n& \\leq e^{d}.\n\\end{aligned}\n\n\n\n\nThe theorem above shows that the VC dimension marks the threshold between the exponential growth and polynomial growth of the growth function.\n\nWhen n &lt; d, by the definition of the VC dimension, we can always find a set of instances of size n \\mathcal{H} can shatter, so the growth function \\Pi_{\\mathcal{H}} (n) = 2^{n}, which means it grows exponentially with a factor of 2 as n increases,\nWhen n &gt; d, by the theorem above, the growth function is upper bounded by n^{d}, so it only grows in polynomials as n increases."
  },
  {
    "objectID": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html",
    "href": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html",
    "title": "Empirical Risk Minimization",
    "section": "",
    "text": "Since the underlying distribution \\mathbb{P}_{X, Y} is unknown, the true risk cannot be calculated and therefore Bayesian decision theory can not be applied in practice.\nInstead, we use the empirical risk minimization (ERM) algorithm in practice, which produces the empirical risk minimizer that is similar to Bayer’s classifier as it minimizes the empirical risk instead of the unknown true risk.\n\n\nThe no-free-lunch (NFL) theorem for machine learning states that there is no algorithm that can generate perfect hypothesis for any distribution using a finite training set.\n\nTheorem 1 TODO\n\nAnother interpretation of the NFL theorem is that any two algorithms are equivalent when their performance is averaged across all possible distributions. Therefore, one must make some biased assumptions about the underlying distribution about the targeted problems, so that the algorithm can be designed to have good performance on the interested problem but having bad performance on the problems that we don’t care.\n\n\n\nTo apply the NFL theorem to ERM, we make assumptions about the underlying distribution by adding inductive bias to the ERM algorithm. The inductive bias implies that we have a predetermined preference for some hypotheses over other hypotheses. One reasonable approach to inductive bias is to restrict the hypothesis class that ERM considers. Therefore, instead of looking for the best hypothesis over all possible functions, ERM method minimizes the risk over a selected hypothesis class \\mathcal{H} to derive the empirical risk minimizer h_{n}\n\nh_{n} = \\argmin_{h \\in \\mathcal{H}} R_{n} (h).\n\n\nLemma 1 A special property of the ERM over any hypothesis class \\mathcal{H} is that\n\n\\forall h \\in \\mathcal{H}: R (h) - R (h_{n}) \\leq 2 \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, for all h \\in \\mathcal{H},\n\n\\begin{aligned}\nR (h) - R (h_{n})\n& = (R (h) + R_{n} (h) - R_{n} (h)) - (R (h_{n}) + R_{n} (h_{n}) - R_{n} (h_{n}))\n\\\\\n& = (R_{n} (h) - R_{n} (h_{n})) + (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\end{aligned}\n\nSince h_{n} is the one that minimizes R_{n},\n\nR_{n} (h) - R_{n} (h_{n}) \\geq 0,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\nSince both h, h_{n} \\in \\mathcal{H},\n\nR (h) - R_{n} (h) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert\n\\\\\nR_{n} (h_{n}) - R (h_{n}) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert."
  },
  {
    "objectID": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "href": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html#no-free-lunch-theorem",
    "title": "Empirical Risk Minimization",
    "section": "",
    "text": "The no-free-lunch (NFL) theorem for machine learning states that there is no algorithm that can generate perfect hypothesis for any distribution using a finite training set.\n\nTheorem 1 TODO\n\nAnother interpretation of the NFL theorem is that any two algorithms are equivalent when their performance is averaged across all possible distributions. Therefore, one must make some biased assumptions about the underlying distribution about the targeted problems, so that the algorithm can be designed to have good performance on the interested problem but having bad performance on the problems that we don’t care."
  },
  {
    "objectID": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "href": "Knowledge/Learning Theory/4_Empirical_Risk_Minimization.html#erm-with-inductive-bias",
    "title": "Empirical Risk Minimization",
    "section": "",
    "text": "To apply the NFL theorem to ERM, we make assumptions about the underlying distribution by adding inductive bias to the ERM algorithm. The inductive bias implies that we have a predetermined preference for some hypotheses over other hypotheses. One reasonable approach to inductive bias is to restrict the hypothesis class that ERM considers. Therefore, instead of looking for the best hypothesis over all possible functions, ERM method minimizes the risk over a selected hypothesis class \\mathcal{H} to derive the empirical risk minimizer h_{n}\n\nh_{n} = \\argmin_{h \\in \\mathcal{H}} R_{n} (h).\n\n\nLemma 1 A special property of the ERM over any hypothesis class \\mathcal{H} is that\n\n\\forall h \\in \\mathcal{H}: R (h) - R (h_{n}) \\leq 2 \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition, for all h \\in \\mathcal{H},\n\n\\begin{aligned}\nR (h) - R (h_{n})\n& = (R (h) + R_{n} (h) - R_{n} (h)) - (R (h_{n}) + R_{n} (h_{n}) - R_{n} (h_{n}))\n\\\\\n& = (R_{n} (h) - R_{n} (h_{n})) + (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\end{aligned}\n\nSince h_{n} is the one that minimizes R_{n},\n\nR_{n} (h) - R_{n} (h_{n}) \\geq 0,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\nSince both h, h_{n} \\in \\mathcal{H},\n\nR (h) - R_{n} (h) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert\n\\\\\nR_{n} (h_{n}) - R (h_{n}) \\leq \\max_{\\hat{h} \\in \\mathcal{H}} \\lvert R (\\hat{h}) - R_{n} (\\hat{h}) \\rvert,\n\nso\n\nR (h) - R (h_{n}) \\leq (R (h) - R_{n} (h)) + (R_{n} (h_{n}) - R (h_{n}))\n\\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert."
  },
  {
    "objectID": "Knowledge/Learning Theory/6_PAC_Learning.html",
    "href": "Knowledge/Learning Theory/6_PAC_Learning.html",
    "title": "PAC Learning",
    "section": "",
    "text": "Probably approximately correct (PAC) learning is a statistical learning objective that requires that the algorithm learns a hypothesis h from a hypothesis class \\mathcal{H} using a given sample set \\mathcal{S} with the following properties.\n\nCorrect: h should achieve low true risk R (h).\nApproximately: R (h) should be approximately closed to the lowest true risk that any hypothesis can achieve R (h) \\approx \\min_{\\hat{h}} R (\\hat{h}).\nProbably: the event R (h) \\approx \\min_{\\hat{h}} R (\\hat{h}) should have high probability.\n\n\n\nUnder the realizable assumption, it is assumed that there exists a perfect concept from a concept class c \\in \\mathcal{C} such that all labels of the instances are labeled according to c and the hypothesis class that our algorithm ERM considers is the concept class \\mathcal{H} = \\mathcal{C}.\n\nDefinition 1 (Consistent) We say that a hypothesis h is consistent with a set of labeled instances \\mathcal{S} = \\{ (\\mathbf{x}_{1}, y_{1}), \\dots, (\\mathbf{x}_{n}, y_{n}) \\} if h (\\mathbf{x}_{i}) = y_{i} for all i.\n\nTherefore, under the realizable assumption, ERM can always find a hypothesis that is consistent with any given training set, and therefore we say that ERM learns in the consistency model.\n\n\nLearning in the consistency model requires the algorithm to always predict correctly on the training set, but doesn’t care much about the generalization of the performance on the test set.\n\nDefinition 2 (Consistency model) An algorithm A learns the hypothesis class \\mathcal{H} = \\mathcal{C} in the consistency model if\n\ngiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C},\nA can find a concept h \\in \\mathcal{H} that is consistent with \\mathcal{S} if h exists, or A outputs False if no such concept exists.\n\n\n\n\n\nLearning in the PAC model is more applicable in real world, as it emphasizes more on the generalization ability of the learned function from the algorithm.\n\nDefinition 3 (PAC model) An algorithm A learns the concept class \\mathcal{C} in the PAC model by the hypothesis class \\mathcal{H} = \\mathcal{C} if,\n\ngiven a set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C}, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where its true risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (R (h) \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n\nHere we present some results about the generalization error of the algorithms using the definitions of consistency model and PAC model. Since ERM learns the hypothesis class in the consistency model, the following theorems naturally apply to it.\nThe following theorem states that a finite concept class \\mathcal{C} is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{C} is learnable in the consistency model, and proves its sample complexity as a function of the size of the hypothesis class.\n\nTheorem 1 If an algorithm A learns a finite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAnother way to state the PAC learnability with the consistency model is\n\n\\mathbb{P}_{\\mathcal{S}} (\\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon) \\leq \\delta\n\nwhen n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta).\nGiven h \\in \\mathcal{H}, by definition of the empirical risk we can write the probability that h is consistent with \\mathcal{S} as\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0)\n& = \\mathbb{P}_{\\mathcal{S}} (h (\\mathbf{x_{i}}) = y_{i}, \\forall (\\mathbf{x}_{i}, y_{i}) \\in \\mathcal{S})\n\\\\\n& \\stackrel{(1)}{=} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) = y_{i})\n\\\\\n& = \\prod_{i = 1}^{n} 1 - \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) \\neq y_{i})\n\\\\\n& \\stackrel{(2)}{=} \\prod_{i = 1}^{n} 1 - R (h)\n\\\\\n& = (1 - R (h))^{n}.\n\\end{aligned}\n\n\n\nfollows because the labeled instances in \\mathcal{S} are independent.\n\n\nfollows because the true risk of h is the probability of h makes a mistake on a given labeled instance when the loss function is the 0-1 loss.\n\n\nIf we add the fact that R (h) \\geq \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n& \\leq (1 - \\epsilon)^{n}\n\\\\\n& \\leq e^{- n \\epsilon}\n\\end{aligned}\n\nwhere the last inequality uses the fact that\n\n1 - x &lt; e^{-x}, \\forall x \\in [0, 1].\n\nWe can add the part \\exists h \\in \\mathcal{H} by applying the union bound\n\n\\mathbb{P}_{\\mathcal{S}} (\\exists h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n\\leq \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon},\n\nand make \\delta = \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon}, we can derive\n\nn \\geq \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\nThe following theorem states a similar results as above: an infinite concept class \\mathcal{C} it is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{H} is learnable in the consistency model, and proves the sample complexity as a function of the growth function of \\mathcal{H}.\n\nTheorem 2 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has the true risk larger than \\epsilon\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R (h) \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has empirical risk on \\mathcal{S}' larger than \\frac{ \\epsilon }{ 2 }\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R_{S'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 }.\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause the event B' (\\mathcal{S}, \\mathcal{S}') is the event R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 } if the event B (\\mathcal{S}) is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), we can apply the lower tail case of the Chernoff bound for the average of Bernoulli variables and set X = R_{\\mathcal{S}'} (h), \\mu = R (h), \\delta = \\frac{ 1 }{ 2 }\n\n\\begin{aligned}\n\\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu)\n& \\leq \\exp \\left[\n    -\\frac{ n \\delta^{2} \\mu }{ 2 }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right)\n& \\leq \\exp \\left[\n    -\\frac{ n R (h) }{ 8 }\n\\right].\n\\end{aligned}\n\nSince R (h) \\geq \\epsilon and the assumption states that n &gt; \\frac{ 8 }{ \\epsilon }\n\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right) \\leq \\exp \\left[\n    \\frac{ - n R(h) }{ 8 }\n\\right] \\leq \\exp \\left[\n    \\frac{ - R(h) }{ \\epsilon }\n\\right] \\leq \\frac{ 1 }{ e } \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\geq \\frac{ 1 }{ 2 }\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon }{ 2 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nRemember that \\mathcal{S}, \\mathcal{S}' all have n instances and therefore there are n pairs of instances (\\mathbf{x}_{1}, \\mathbf{x}_{1}'), \\dots, (\\mathbf{x}_{n}, \\mathbf{x}_{n}'). There are 3 cases for the corrections of the predictions made by h for each pair (h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}')).\n\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are incorrect.\nEither h (\\mathbf{x}_{i}) or h (\\mathbf{x}_{i}') is incorrect (correct).\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are correct.\n\nFirst if there is a pair in \\mathcal{S}, \\mathcal{S}' with case 1, then\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}} (h) &gt; 0 no matter how to generate \\mathcal{S}_{\\sigma} by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nThen denoted by r the number of pairs in \\mathcal{S}, \\mathcal{S}' that case 2 is true, if r &lt; \\frac{ \\epsilon n }{ 2 },\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}'} (h) &lt; \\frac{ \\epsilon }{2} no matter how to generate \\mathcal{S}_{\\sigma}' by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nWhen r \\geq \\frac{ \\epsilon n }{ 2 }, the event R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } is possible and its possibility is\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = \\left(\n    \\frac{ 1 }{ 2 }\n\\right)^{r} \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}\n\nbecause the independent Rademacher random variables in \\sigma must take 1 with probability \\frac{ 1 }{ 2 } for all r' mistakes that were in \\mathcal{S} and swapped to be in \\mathcal{S}_{\\sigma}', and take -1 with probability \\frac{ 1 }{ 2 } for the r - r' mistakes that were in \\mathcal{S}' and are stayed in \\mathcal{S}_{\\sigma}'.\nSince the probability of the case 3 is already included in the calculation of the above probabilities, we can prove the claim by adding probabilities for all cases\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}.\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ \\epsilon n }{ 2 }},\n\\\\\n\\end{aligned}\n\nwhere the last inequality is because of the definition of the growth function states that\n\n\\lvert \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}') \\rvert \\leq \\Pi_{H} (\\lvert \\mathcal{S} \\rvert + \\lvert \\mathcal{S}' \\rvert) = \\Pi_{\\mathcal{H}} (2 n).\n\nNote that the term \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }} doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon }{ 2 }\n    \\right]\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}.\n\\end{aligned}\n\nBy setting \\delta = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }},\n\n\\begin{aligned}\n\\delta\n& = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}\n\\\\\nn\n& = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}\n\n\n\n\nNow we can use the Sauer’s lemma to get a nice closed form expression on sample complexity result for the infinite class.\n\nTheorem 3 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n},\n\nwhere d = \\mathrm{VC} (\\mathcal{H}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Sauer’s lemma to the sample complexity results for the infinite classes\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{\n    4 \\log \\left(\n        \\frac{ 2 e n }{ d }\n    \\right)^{d} + 2 \\log \\frac { 2 }{ \\delta }\n}{\n    \\epsilon\n}\n\\\\\n& = \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d  }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\end{aligned}\n\nSince \\log x \\leq a x - \\log a - 1 for a, x &gt; 0, we can show that\n\n\\begin{aligned}\n\\log n\n& \\leq \\frac{ \\epsilon n }{ 8 d } - \\log \\frac{ \\epsilon }{ 8 d  } - 1\n\\\\\n\\frac{ 4 d }{ \\epsilon } \\log n\n& \\leq \\frac{ 4 d }{ \\epsilon } \\left(\n    \\frac{ \\epsilon n }{ 8 d } + \\log \\frac{ 8 d }{ \\epsilon } - 1\n\\right)\n\\\\\n& = \\frac{ n }{ 2 } + \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }.\n\\end{aligned}\n\nBy combining the results above,\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }.\n\\end{aligned}\n\nTherefore, if we have a training set that has a number of instances\n\n\\begin{aligned}\nn\n& \\geq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n\\frac{ n }{ 2 }\n& \\geq \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\nn\n& \\geq \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}\n\n\n\n\n\n\n\n\nThe PAC learning in the unrealizable setting is also called agnostic PAC learning where the perfect concept cannot be realized because either one of the following events happens\n\nthe concept that the algorithm A learns is not in the hypothesis class that A considers,\nany instance can have contradictory labels, amd therefore there doesn’t exist a concept that can perfectly label all instances in the input space.\n\n\n\n\nDefinition 4 An algorithm A learns the concept class \\mathcal{C} in the agnostic PAC model by the hypothesis class \\mathcal{H} if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where the difference between its true risk and the minimum true risk achieved by any hypothesis in \\mathcal{H} is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (\\lvert R (h) - \\min_{h \\in \\mathcal{H}} R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n\nThe uniform convergence result guarantees the agnostic PAC learnability of ERM.\n\nLemma 1 If A is the ERM algorithm that learns the hypothesis class \\mathcal{H}, which satisfies uniform convergence with sample complexity n_{\\mathcal{H}}^{u}, then A learns \\mathcal{H} in the agnostic PAC model with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet h_{n} be the hypothesis learned by ERM. According the property of the ERM, we have\n\nR (h) - R (h_{n}) \\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert.\n\nSince \\mathcal{H} has @def:uniform-convergence-property, if n \\geq n_{\\mathcal{H}}^{u} (\\hat{\\epsilon}, \\delta), then\n\n\\forall h \\in \\mathcal{H}, \\mathbb{P} (\\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon}) \\geq 1 - \\delta,\n\nwhich is equivalent of\n\n\\begin{aligned}\n\\mathbb{P} (\\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon})\n& \\geq 1 - \\delta\n\\\\\n\\mathbb{P} (R (h) - R (h_{n}) \\leq 2 \\hat{\\epsilon})\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nBy setting \\epsilon = 2 \\hat{\\epsilon}, we have the conclusion that if n \\geq n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta), then\n\n\\mathbb{P} (R (h) - R (h_{n}) \\leq \\epsilon) \\geq 1 - \\delta,\n\nwhich is the definition of agonistic PAC learning.\n\n\n\nBy the lemma Lemma 1, we can easily derive the sample complexity results for agnostic PAC by plugging $ = $ to the sample complexity results of the uniform convergence."
  },
  {
    "objectID": "Knowledge/Learning Theory/6_PAC_Learning.html#realizable-case",
    "href": "Knowledge/Learning Theory/6_PAC_Learning.html#realizable-case",
    "title": "PAC Learning",
    "section": "",
    "text": "Under the realizable assumption, it is assumed that there exists a perfect concept from a concept class c \\in \\mathcal{C} such that all labels of the instances are labeled according to c and the hypothesis class that our algorithm ERM considers is the concept class \\mathcal{H} = \\mathcal{C}.\n\nDefinition 1 (Consistent) We say that a hypothesis h is consistent with a set of labeled instances \\mathcal{S} = \\{ (\\mathbf{x}_{1}, y_{1}), \\dots, (\\mathbf{x}_{n}, y_{n}) \\} if h (\\mathbf{x}_{i}) = y_{i} for all i.\n\nTherefore, under the realizable assumption, ERM can always find a hypothesis that is consistent with any given training set, and therefore we say that ERM learns in the consistency model.\n\n\nLearning in the consistency model requires the algorithm to always predict correctly on the training set, but doesn’t care much about the generalization of the performance on the test set.\n\nDefinition 2 (Consistency model) An algorithm A learns the hypothesis class \\mathcal{H} = \\mathcal{C} in the consistency model if\n\ngiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C},\nA can find a concept h \\in \\mathcal{H} that is consistent with \\mathcal{S} if h exists, or A outputs False if no such concept exists.\n\n\n\n\n\nLearning in the PAC model is more applicable in real world, as it emphasizes more on the generalization ability of the learned function from the algorithm.\n\nDefinition 3 (PAC model) An algorithm A learns the concept class \\mathcal{C} in the PAC model by the hypothesis class \\mathcal{H} = \\mathcal{C} if,\n\ngiven a set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, where instances are sampled from any distribution \\mathbb{P}_{\\mathbf{X}} over the instance space and are labeled by any concept c \\in \\mathcal{C}, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where its true risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (R (h) \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n\nHere we present some results about the generalization error of the algorithms using the definitions of consistency model and PAC model. Since ERM learns the hypothesis class in the consistency model, the following theorems naturally apply to it.\nThe following theorem states that a finite concept class \\mathcal{C} is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{C} is learnable in the consistency model, and proves its sample complexity as a function of the size of the hypothesis class.\n\nTheorem 1 If an algorithm A learns a finite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAnother way to state the PAC learnability with the consistency model is\n\n\\mathbb{P}_{\\mathcal{S}} (\\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon) \\leq \\delta\n\nwhen n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta).\nGiven h \\in \\mathcal{H}, by definition of the empirical risk we can write the probability that h is consistent with \\mathcal{S} as\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0)\n& = \\mathbb{P}_{\\mathcal{S}} (h (\\mathbf{x_{i}}) = y_{i}, \\forall (\\mathbf{x}_{i}, y_{i}) \\in \\mathcal{S})\n\\\\\n& \\stackrel{(1)}{=} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) = y_{i})\n\\\\\n& = \\prod_{i = 1}^{n} 1 - \\mathbb{P}_{\\mathbf{X}} (h (\\mathbf{x}_{i}) \\neq y_{i})\n\\\\\n& \\stackrel{(2)}{=} \\prod_{i = 1}^{n} 1 - R (h)\n\\\\\n& = (1 - R (h))^{n}.\n\\end{aligned}\n\n\n\nfollows because the labeled instances in \\mathcal{S} are independent.\n\n\nfollows because the true risk of h is the probability of h makes a mistake on a given labeled instance when the loss function is the 0-1 loss.\n\n\nIf we add the fact that R (h) \\geq \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n& \\leq (1 - \\epsilon)^{n}\n\\\\\n& \\leq e^{- n \\epsilon}\n\\end{aligned}\n\nwhere the last inequality uses the fact that\n\n1 - x &lt; e^{-x}, \\forall x \\in [0, 1].\n\nWe can add the part \\exists h \\in \\mathcal{H} by applying the union bound\n\n\\mathbb{P}_{\\mathcal{S}} (\\exists h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0, R (h) \\geq \\epsilon)\n\\leq \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon},\n\nand make \\delta = \\lvert \\mathcal{H} \\rvert e^{- n \\epsilon}, we can derive\n\nn \\geq \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 1 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\nThe following theorem states a similar results as above: an infinite concept class \\mathcal{C} it is PAC learnable by the same hypothesis class \\mathcal{H} = \\mathcal{C} if \\mathcal{H} is learnable in the consistency model, and proves the sample complexity as a function of the growth function of \\mathcal{H}.\n\nTheorem 2 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has the true risk larger than \\epsilon\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R (h) \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} that is consistent with \\mathcal{S} but has empirical risk on \\mathcal{S}' larger than \\frac{ \\epsilon }{ 2 }\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}} (h) = 0,  R_{S'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\coloneqq \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 }.\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 }.\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause the event B' (\\mathcal{S}, \\mathcal{S}') is the event R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 } if the event B (\\mathcal{S}) is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), we can apply the lower tail case of the Chernoff bound for the average of Bernoulli variables and set X = R_{\\mathcal{S}'} (h), \\mu = R (h), \\delta = \\frac{ 1 }{ 2 }\n\n\\begin{aligned}\n\\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu)\n& \\leq \\exp \\left[\n    -\\frac{ n \\delta^{2} \\mu }{ 2 }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right)\n& \\leq \\exp \\left[\n    -\\frac{ n R (h) }{ 8 }\n\\right].\n\\end{aligned}\n\nSince R (h) \\geq \\epsilon and the assumption states that n &gt; \\frac{ 8 }{ \\epsilon }\n\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\leq \\frac{ R (h) }{ 2 }\n\\right) \\leq \\exp \\left[\n    \\frac{ - n R(h) }{ 8 }\n\\right] \\leq \\exp \\left[\n    \\frac{ - R(h) }{ \\epsilon }\n\\right] \\leq \\frac{ 1 }{ e } \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P} \\left(\n    R_{\\mathcal{S}'} (h) \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\geq \\frac{ 1 }{ 2 }\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon }{ 2 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nRemember that \\mathcal{S}, \\mathcal{S}' all have n instances and therefore there are n pairs of instances (\\mathbf{x}_{1}, \\mathbf{x}_{1}'), \\dots, (\\mathbf{x}_{n}, \\mathbf{x}_{n}'). There are 3 cases for the corrections of the predictions made by h for each pair (h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}')).\n\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are incorrect.\nEither h (\\mathbf{x}_{i}) or h (\\mathbf{x}_{i}') is incorrect (correct).\nBoth h (\\mathbf{x}_{i}), h (\\mathbf{x}_{i}') are correct.\n\nFirst if there is a pair in \\mathcal{S}, \\mathcal{S}' with case 1, then\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}} (h) &gt; 0 no matter how to generate \\mathcal{S}_{\\sigma} by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nThen denoted by r the number of pairs in \\mathcal{S}, \\mathcal{S}' that case 2 is true, if r &lt; \\frac{ \\epsilon n }{ 2 },\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = 0\n\nbecause R_{\\mathcal{S}_{\\sigma}'} (h) &lt; \\frac{ \\epsilon }{2} no matter how to generate \\mathcal{S}_{\\sigma}' by swapping instances in \\mathcal{S}, \\mathcal{S}'.\nWhen r \\geq \\frac{ \\epsilon n }{ 2 }, the event R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } is possible and its possibility is\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) = \\left(\n    \\frac{ 1 }{ 2 }\n\\right)^{r} \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}\n\nbecause the independent Rademacher random variables in \\sigma must take 1 with probability \\frac{ 1 }{ 2 } for all r' mistakes that were in \\mathcal{S} and swapped to be in \\mathcal{S}_{\\sigma}', and take -1 with probability \\frac{ 1 }{ 2 } for the r - r' mistakes that were in \\mathcal{S}' and are stayed in \\mathcal{S}_{\\sigma}'.\nSince the probability of the case 3 is already included in the calculation of the above probabilities, we can prove the claim by adding probabilities for all cases\n\n\\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right) \\leq 2^{- \\frac{ \\epsilon n }{ 2 }}.\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    R_{\\mathcal{S}_{\\sigma}} (h) = 0, R_{\\mathcal{S}_{\\sigma}'} (h) \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ \\epsilon n }{ 2 }},\n\\\\\n\\end{aligned}\n\nwhere the last inequality is because of the definition of the growth function states that\n\n\\lvert \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}') \\rvert \\leq \\Pi_{H} (\\lvert \\mathcal{S} \\rvert + \\lvert \\mathcal{S}' \\rvert) = \\Pi_{\\mathcal{H}} (2 n).\n\nNote that the term \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }} doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon }{ 2 }\n    \\right]\n\\right] \\leq \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon }{ 2 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}.\n\\end{aligned}\n\nBy setting \\delta = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }},\n\n\\begin{aligned}\n\\delta\n& = 2 \\Pi_{\\mathcal{H}} (2 n) 2^{- \\frac{ n \\epsilon }{ 2 }}\n\\\\\nn\n& = 2 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}\n\n\n\n\nNow we can use the Sauer’s lemma to get a nice closed form expression on sample complexity result for the infinite class.\n\nTheorem 3 If an algorithm A learns an infinite concept class \\mathcal{C} in the consistency model, then A learns the concept class \\mathcal{C} by the hypothesis class \\mathcal{H} = \\mathcal{C} in the PAC model with\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n},\n\nwhere d = \\mathrm{VC} (\\mathcal{H}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Sauer’s lemma to the sample complexity results for the infinite classes\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{\n    4 \\log \\left(\n        \\frac{ 2 e n }{ d }\n    \\right)^{d} + 2 \\log \\frac { 2 }{ \\delta }\n}{\n    \\epsilon\n}\n\\\\\n& = \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d  }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\end{aligned}\n\nSince \\log x \\leq a x - \\log a - 1 for a, x &gt; 0, we can show that\n\n\\begin{aligned}\n\\log n\n& \\leq \\frac{ \\epsilon n }{ 8 d } - \\log \\frac{ \\epsilon }{ 8 d  } - 1\n\\\\\n\\frac{ 4 d }{ \\epsilon } \\log n\n& \\leq \\frac{ 4 d }{ \\epsilon } \\left(\n    \\frac{ \\epsilon n }{ 8 d } + \\log \\frac{ 8 d }{ \\epsilon } - 1\n\\right)\n\\\\\n& = \\frac{ n }{ 2 } + \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }.\n\\end{aligned}\n\nBy combining the results above,\n\n\\begin{aligned}\n\\frac{\n    4 \\log \\Pi_{\\mathcal{H}} (2 n) + 2 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}\n& \\leq \\frac{ 4 d }{ \\epsilon } \\log n\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 8 d }{\\epsilon e }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 2 e }{ d }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n& \\leq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }.\n\\end{aligned}\n\nTherefore, if we have a training set that has a number of instances\n\n\\begin{aligned}\nn\n& \\geq \\frac{ n }{ 2 }\n+ \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\n\\frac{ n }{ 2 }\n& \\geq \\frac{ 4 d }{ \\epsilon } \\log \\frac{ 16 }{\\epsilon }\n+ \\frac { 2 }{ \\epsilon }\\log \\frac { 2 }{ \\delta }\n\\\\\nn\n& \\geq \\frac{\n    8 d \\log \\frac{ 16 }{ \\epsilon} + 4 \\log \\frac{ 2 }{ \\delta }\n}{\n    \\epsilon\n}.\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "href": "Knowledge/Learning Theory/6_PAC_Learning.html#unrealizable-case",
    "title": "PAC Learning",
    "section": "",
    "text": "The PAC learning in the unrealizable setting is also called agnostic PAC learning where the perfect concept cannot be realized because either one of the following events happens\n\nthe concept that the algorithm A learns is not in the hypothesis class that A considers,\nany instance can have contradictory labels, amd therefore there doesn’t exist a concept that can perfectly label all instances in the input space.\n\n\n\n\nDefinition 4 An algorithm A learns the concept class \\mathcal{C} in the agnostic PAC model by the hypothesis class \\mathcal{H} if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some $&gt; 0 $ and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nA returns a hypothesis h \\in \\mathcal{H}, where the difference between its true risk and the minimum true risk achieved by any hypothesis in \\mathcal{H} is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P} (\\lvert R (h) - \\min_{h \\in \\mathcal{H}} R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\n\n\n\nThe uniform convergence result guarantees the agnostic PAC learnability of ERM.\n\nLemma 1 If A is the ERM algorithm that learns the hypothesis class \\mathcal{H}, which satisfies uniform convergence with sample complexity n_{\\mathcal{H}}^{u}, then A learns \\mathcal{H} in the agnostic PAC model with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet h_{n} be the hypothesis learned by ERM. According the property of the ERM, we have\n\nR (h) - R (h_{n}) \\leq 2 \\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert.\n\nSince \\mathcal{H} has @def:uniform-convergence-property, if n \\geq n_{\\mathcal{H}}^{u} (\\hat{\\epsilon}, \\delta), then\n\n\\forall h \\in \\mathcal{H}, \\mathbb{P} (\\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon}) \\geq 1 - \\delta,\n\nwhich is equivalent of\n\n\\begin{aligned}\n\\mathbb{P} (\\max_{h \\in \\mathcal{H}} \\lvert R (h) - R_{n} (h) \\rvert \\leq \\hat{\\epsilon})\n& \\geq 1 - \\delta\n\\\\\n\\mathbb{P} (R (h) - R (h_{n}) \\leq 2 \\hat{\\epsilon})\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nBy setting \\epsilon = 2 \\hat{\\epsilon}, we have the conclusion that if n \\geq n_{\\mathcal{H}}^{u} (\\frac{ \\epsilon }{ 2 }, \\delta), then\n\n\\mathbb{P} (R (h) - R (h_{n}) \\leq \\epsilon) \\geq 1 - \\delta,\n\nwhich is the definition of agonistic PAC learning.\n\n\n\nBy the lemma Lemma 1, we can easily derive the sample complexity results for agnostic PAC by plugging $ = $ to the sample complexity results of the uniform convergence."
  },
  {
    "objectID": "Knowledge/Learning Theory/5_Uniform_Convergence.html",
    "href": "Knowledge/Learning Theory/5_Uniform_Convergence.html",
    "title": "Uniform Convergence",
    "section": "",
    "text": "Uniform convergence means convergence with respect to the difference between the empirical error and the true error, which is uniform with respect to all hypotheses in the class. That is, uniform convergence bound the difference between true and empirical error across all hypotheses in the class simultaneously.\n\n\nThe uniform convergence property of a given hypothesis class \\mathcal{H} states that there exists a large enough sample size such that for all hypotheses in the class \\mathcal{H}, the empirical risk is close to the true risk with high probability, regardless of the underlying distribution \\mathbb{P}_{Z}.\n\nDefinition 1 (Uniform convergence property) A hypothesis class \\mathcal{H} has the uniform convergence property if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some \\epsilon &gt; 0 and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nfor every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P}_{\\mathcal{S}} (\\lvert R (h) - R_{\\mathcal{S}} (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\nNote that this definition is stated using the sample complexity n_{\\mathcal{H}} (\\epsilon, \\delta), which is the sample size that we need to supply, so that with an arbitrarily high probability 1 - \\delta, the considered event has an arbitrarily small error \\epsilon.\n\n\n\nThe following theorems state that a hypothesis class has the uniform convergence property if either it has a finite number of hypotheses or has a finite VC dimension.\n\nTheorem 1 (Uniform convergence theorem) Any finite hypothesis class \\mathcal{H} has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the true risk of a hypothesis is the expectation of the empirical risk with respect to the joint distribution \\mathbb{P}_{Z}\n\nR(h) = \\mathbb{E}_{Z} \\left[\n    R_{n} (h)\n\\right] = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (z_{i}))\n\\right]\n\nand we can view the 0-1 loss on an instance as a bounded random variable\n\nL_{i} = L (h (\\mathbf{X_{i}}), Y_{i}) = \\mathbb{1} \\left[\n    h (\\mathbf{X_{i}}) \\neq Y_{i}\n\\right] \\in [0, 1],\n\nwe can apply Hoeffding’s inequality on L_{i} for a fixed hypothesis h \\in \\mathcal{H},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i} - \\mathbb{E}_{Z} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i}\n        \\right]\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - \\mathbb{E}_{Z} \\left[\n        R_{n} (h)\n    \\right] \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ n }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - R (h) \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nThe above inequality only works for one h \\in \\mathcal{F}. we can apply union bound to extend it for all f \\in \\mathcal{F},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\geq \\epsilon\n\\right)\n& \\leq \\sum_{i = 1}^{\\lvert \\mathcal{F} \\rvert} \\mathbb{P} \\left(\n    \\lvert R_{n} (f_{i}) - R (f_{i}) \\rvert \\geq \\epsilon\n\\right)\n\\\\\n& \\leq 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nSince \\mathbb{P} (X \\geq a) = 1 - \\mathbb{P} (X \\leq a)\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\leq \\epsilon\n\\right)\n& \\geq 1 - 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\n& \\geq 1 - \\delta\n\\end{aligned}\n\nwhere\n\n\\begin{aligned}\n\\delta\n& = 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\nn\n& = \\frac{\n    \\log \\lvert \\mathcal{F} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\\end{aligned}\n\n\n\n\n\nTheorem 2 Any infinite hypothesis class \\mathcal{H} with a finite VC dimension has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}\n}\n\nwhere n is the number of samples in the training set.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon,\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) -  R (h) \\rvert \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S} and empirical risk on \\mathcal{S}' is larger than \\frac{ \\epsilon }{ 2 },\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\n\\begin{aligned}\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma)\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}_{\\sigma}} (h) - R_{\\mathcal{S}_{\\sigma}'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\\\\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n        \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}) \\neq y_{i}\n        \\right] - \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}') \\neq y_{i}'\n        \\right]\n    \\right)\n\\right\\rvert \\geq \\frac{ \\epsilon }{ 2 },\n\\\\\n\\end{aligned}\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 },\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause it is the same as the \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left( \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 } \\right) if the event B (\\mathcal{S}) \\coloneqq \\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\geq \\epsilon is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), the probability of the difference between R (h) and R_{\\mathcal{S}'} (h) can be upper bounded by applying Chebyshev’s inequality with X = R_{\\mathcal{S}'} (h), \\mu = R (h), t = \\frac{ \\epsilon }{ 2 }, \\sigma^{2} = \\mathrm{Var} [R_{\\mathcal{S}'} (h)]\n\n\\begin{aligned}\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R_{S'} (h) - R (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 4 \\mathrm{Var} [R_{\\mathcal{S}'} (h)] }{\\epsilon^{2}}.\n\\end{aligned}\n\nNote that h (\\mathbf{x}_{i}) \\neq y_{i} is a Bernoulli random variable whose variance is less than \\frac{ 1 }{ 4 }\n\n\\mathrm{Var} [R_{\\mathcal{S}'} (h)] = \\mathrm{Var} \\left[\n    \\frac{ 1 }{ n } \\sum_{\\mathbf{x} \\in \\mathcal{S}'} h (\\mathbf{x}_{i}) \\neq y_{i}\n\\right] = \\frac{ 1 }{ n^{2} } \\sum_{\\mathbf{x_{i} \\in \\mathcal{S}'}} \\mathrm{Var} [h (\\mathbf{x}) \\neq y_{i}] \\leq \\frac{ 1 }{ 4 n },\n\nand therefore,\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\frac{ 1 }{n \\epsilon^{2}}.\n\nAssume that n \\epsilon^{2} \\geq 2\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\geq \\frac{ 1 }{ 2 }.\n\\\\\n\\end{aligned}\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nConsider the following probability for a fixed h \\in \\mathcal{H},\n\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n            \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}) \\neq y_{i}\n            \\right] - \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}') \\neq y_{i}'\n            \\right]\n        \\right)\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right).\n\nSince \\mathcal{S}, \\mathcal{S}' are given, the value \\alpha_{i} = \\mathbb{1} \\left[ h (\\mathbf{x}_{i}) \\neq y_{i} \\right] - \\mathbb{1} \\left[ h (\\mathbf{x}_{i}') \\neq y_{i}' \\right] is a fixed value and therefore\n\n\\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n    \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}) \\neq y_{i}\n    \\right] - \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}') \\neq y_{i}'\n    \\right]\n\\right) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\nis a random variable with\n\n\\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\\right] = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nApplying Hoeffding’s inequality with X_{i} = \\alpha_{i} \\sigma_{i}, \\mu = 0, t = \\frac{ \\epsilon }{ 2 },\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i} - \\mathbb{E} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n        \\right]\n    \\right\\rvert \\geq t\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i} - 0\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ 2 n^{2} \\frac{ \\epsilon^{2} }{ 4 } }{ 4 n }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& =\n\\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\\\\n\\end{aligned}\n\nNote that the term 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right] doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon^{2} }{ 8 }\n    \\right]\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nBy setting \\delta = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right],\n\n\\begin{aligned}\n\\delta\n& = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right]\n\\\\\nn\n& = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}   \n}.\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "href": "Knowledge/Learning Theory/5_Uniform_Convergence.html#uniform-convergence-property",
    "title": "Uniform Convergence",
    "section": "",
    "text": "The uniform convergence property of a given hypothesis class \\mathcal{H} states that there exists a large enough sample size such that for all hypotheses in the class \\mathcal{H}, the empirical risk is close to the true risk with high probability, regardless of the underlying distribution \\mathbb{P}_{Z}.\n\nDefinition 1 (Uniform convergence property) A hypothesis class \\mathcal{H} has the uniform convergence property if,\n\ngiven a set of labeled instances \\mathcal{S}, where instances and labels are sampled from any joint distribution \\mathbb{P}_{Z} over the instance space and the label space, and there exists a function for some \\epsilon &gt; 0 and \\delta &gt; 0 such that\n\n  n \\geq n_{\\mathcal{H}} (\\epsilon, \\delta),\n  \nfor every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than \\epsilon with probability at least 1 - \\delta\n\n  \\mathbb{P}_{\\mathcal{S}} (\\lvert R (h) - R_{\\mathcal{S}} (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta.\n  \n\n\nNote that this definition is stated using the sample complexity n_{\\mathcal{H}} (\\epsilon, \\delta), which is the sample size that we need to supply, so that with an arbitrarily high probability 1 - \\delta, the considered event has an arbitrarily small error \\epsilon."
  },
  {
    "objectID": "Knowledge/Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "href": "Knowledge/Learning Theory/5_Uniform_Convergence.html#uniform-convergence-results",
    "title": "Uniform Convergence",
    "section": "",
    "text": "The following theorems state that a hypothesis class has the uniform convergence property if either it has a finite number of hypotheses or has a finite VC dimension.\n\nTheorem 1 (Uniform convergence theorem) Any finite hypothesis class \\mathcal{H} has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = \\frac{\n    \\log \\lvert \\mathcal{H} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince the true risk of a hypothesis is the expectation of the empirical risk with respect to the joint distribution \\mathbb{P}_{Z}\n\nR(h) = \\mathbb{E}_{Z} \\left[\n    R_{n} (h)\n\\right] = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (z_{i}))\n\\right]\n\nand we can view the 0-1 loss on an instance as a bounded random variable\n\nL_{i} = L (h (\\mathbf{X_{i}}), Y_{i}) = \\mathbb{1} \\left[\n    h (\\mathbf{X_{i}}) \\neq Y_{i}\n\\right] \\in [0, 1],\n\nwe can apply Hoeffding’s inequality on L_{i} for a fixed hypothesis h \\in \\mathcal{H},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i} - \\mathbb{E}_{Z} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L_{i}\n        \\right]\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - \\mathbb{E}_{Z} \\left[\n        R_{n} (h)\n    \\right] \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} \\epsilon^{2} }{ n }\n\\right]\n\\\\\n\\mathbb{P} \\left(\n    \\lvert R_{n} (h) - R (h) \\rvert \\geq \\epsilon\n\\right)\n& \\leq 2 \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nThe above inequality only works for one h \\in \\mathcal{F}. we can apply union bound to extend it for all f \\in \\mathcal{F},\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\geq \\epsilon\n\\right)\n& \\leq \\sum_{i = 1}^{\\lvert \\mathcal{F} \\rvert} \\mathbb{P} \\left(\n    \\lvert R_{n} (f_{i}) - R (f_{i}) \\rvert \\geq \\epsilon\n\\right)\n\\\\\n& \\leq 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right].\n\\end{aligned}\n\nSince \\mathbb{P} (X \\geq a) = 1 - \\mathbb{P} (X \\leq a)\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\exist f \\in \\mathcal{F}, \\lvert R_{n} (f) - R (f) \\rvert \\leq \\epsilon\n\\right)\n& \\geq 1 - 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\n& \\geq 1 - \\delta\n\\end{aligned}\n\nwhere\n\n\\begin{aligned}\n\\delta\n& = 2 \\lvert \\mathcal{F} \\rvert \\exp \\left[\n    - 2 n \\epsilon^{2}\n\\right]\n\\\\\nn\n& = \\frac{\n    \\log \\lvert \\mathcal{F} \\rvert + \\log \\frac{ 2 }{ \\delta }\n}{\n    2 \\epsilon^{2}\n}.\n\\end{aligned}\n\n\n\n\n\nTheorem 2 Any infinite hypothesis class \\mathcal{H} with a finite VC dimension has the uniform convergence property with the sample complexity\n\nn_{\\mathcal{H}} (\\epsilon, \\delta) = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}\n}\n\nwhere n is the number of samples in the training set.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s first define 3 “bad” events that are useful in the following proof.\nGiven any set of labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\}, let B (\\mathcal{S}) denote the event that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its true risk and empirical risk on \\mathcal{S} is larger than \\epsilon,\n\nB (\\mathcal{S}) \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) -  R (h) \\rvert \\geq \\epsilon.\n\nand therefore we want to prove\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq \\delta.\n\nNow let’s draw the “ghost samples”, which is another set of i.i.d labeled instances \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} from the distribution \\mathbb{P}_{Z}, and define another event B' as a function of \\mathcal{S} and \\mathcal{S}', which states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S} and empirical risk on \\mathcal{S}' is larger than \\frac{ \\epsilon }{ 2 },\n\nB' (\\mathcal{S}, \\mathcal{S}') \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }.\n\nFinally, let’s define an event B (\\mathcal{S}, \\mathcal{S}', \\sigma) as a function of \\mathcal{S}, \\mathcal{S}', and a set of independent Rademacher random variables \\sigma_{1}, \\dots, \\sigma_{n} that takes values -1 or 1 with equal probabilities\n\n\\begin{aligned}\nB'' (\\mathcal{S}, \\mathcal{S}', \\sigma)\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\lvert R_{\\mathcal{S}_{\\sigma}} (h) - R_{\\mathcal{S}_{\\sigma}'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\\\\n& \\coloneqq \\exist h \\in \\mathcal{H}: \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n        \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}) \\neq y_{i}\n        \\right] - \\mathbb{1} \\left[\n            h (\\mathbf{x}_{i}') \\neq y_{i}'\n        \\right]\n    \\right)\n\\right\\rvert \\geq \\frac{ \\epsilon }{ 2 },\n\\\\\n\\end{aligned}\n\nwhere the samples \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' are created by swapping the labeled instances in \\mathcal{S}, \\mathcal{S}' based on the values of \\sigma\n\nz_{i} and z_{i}' are swapped if the corresponding \\sigma_{i} = 1,\nand z_{i} and z_{i}' are not swapped if the corresponding \\sigma_{i} = -1.\n\nThe event B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) states that there exists a hypothesis h \\in \\mathcal{H} such that the difference between its empirical risk on \\mathcal{S}_{\\sigma} and empirical risk on \\mathcal{S}_{\\sigma}' is larger than \\frac{ \\epsilon }{ 2 },\nClaim 1: \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) is upper-bounded by 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')),\n\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S})) \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\nSince the probability of an event cannot be larger than its conjunction with another event,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n& \\geq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\cap B (\\mathcal{S}))\n\\\\\n& = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n\\end{aligned}\n\nNow consider the probability of the event\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})),\n\nwhich can be written as\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n\nbecause it is the same as the \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left( \\lvert R_{\\mathcal{S}} (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 } \\right) if the event B (\\mathcal{S}) \\coloneqq \\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\geq \\epsilon is given.\nSince R (h) is the mean of R_{\\mathcal{S}'} (h), the probability of the difference between R (h) and R_{\\mathcal{S}'} (h) can be upper bounded by applying Chebyshev’s inequality with X = R_{\\mathcal{S}'} (h), \\mu = R (h), t = \\frac{ \\epsilon }{ 2 }, \\sigma^{2} = \\mathrm{Var} [R_{\\mathcal{S}'} (h)]\n\n\\begin{aligned}\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R_{S'} (h) - R (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 4 \\mathrm{Var} [R_{\\mathcal{S}'} (h)] }{\\epsilon^{2}}.\n\\end{aligned}\n\nNote that h (\\mathbf{x}_{i}) \\neq y_{i} is a Bernoulli random variable whose variance is less than \\frac{ 1 }{ 4 }\n\n\\mathrm{Var} [R_{\\mathcal{S}'} (h)] = \\mathrm{Var} \\left[\n    \\frac{ 1 }{ n } \\sum_{\\mathbf{x} \\in \\mathcal{S}'} h (\\mathbf{x}_{i}) \\neq y_{i}\n\\right] = \\frac{ 1 }{ n^{2} } \\sum_{\\mathbf{x_{i} \\in \\mathcal{S}'}} \\mathrm{Var} [h (\\mathbf{x}) \\neq y_{i}] \\leq \\frac{ 1 }{ 4 n },\n\nand therefore,\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right) \\leq \\frac{ 1 }{n \\epsilon^{2}}.\n\nAssume that n \\epsilon^{2} \\geq 2\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\geq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\leq \\frac{ 1 }{ 2 }\n\\\\\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} \\left(\n    \\lvert R (h) - R_{S'} (h) \\rvert \\leq \\frac{ \\epsilon }{ 2 }\n\\right)\n& \\geq \\frac{ 1 }{ 2 }.\n\\\\\n\\end{aligned}\n\nThen we have proved the claim\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}') \\mid B (\\mathcal{S})) \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\frac{ 1 }{ 2 } \\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')).\n\\end{aligned}\n\nClaim 2: the probability of event B' (\\mathcal{S}, \\mathcal{S}') is the same as the expectation of the probability that B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) happens given \\mathcal{S}, \\mathcal{S}'\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n= \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right].\n\nSince the event B' (\\mathcal{S}, \\mathcal{S}') and B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) only differ on the set of instances \\mathcal{S}, \\mathcal{S}' and \\mathcal{S}_{\\sigma}, \\mathcal{S}_{\\sigma}' and they can both be seen as the set of instances i.i.d sampled from the \\mathbb{P}_{Z}, their probability should be the same\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}')) = \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)).\n\nThen, we can prove the claim by using marginalization of the probability\n\n\\mathbb{P}_{\\mathcal{S}, \\mathcal{S}', \\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma)) = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')].\n\nClaim 3: \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} [\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')] is upper-bounded by 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right]\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nConsider the following probability for a fixed h \\in \\mathcal{H},\n\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n            \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}) \\neq y_{i}\n            \\right] - \\mathbb{1} \\left[\n                h (\\mathbf{x}_{i}') \\neq y_{i}'\n            \\right]\n        \\right)\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right).\n\nSince \\mathcal{S}, \\mathcal{S}' are given, the value \\alpha_{i} = \\mathbb{1} \\left[ h (\\mathbf{x}_{i}) \\neq y_{i} \\right] - \\mathbb{1} \\left[ h (\\mathbf{x}_{i}') \\neq y_{i}' \\right] is a fixed value and therefore\n\n\\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} \\left(\n    \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}) \\neq y_{i}\n    \\right] - \\mathbb{1} \\left[\n        h (\\mathbf{x}_{i}') \\neq y_{i}'\n    \\right]\n\\right) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\nis a random variable with\n\n\\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n\\right] = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nApplying Hoeffding’s inequality with X_{i} = \\alpha_{i} \\sigma_{i}, \\mu = 0, t = \\frac{ \\epsilon }{ 2 },\n\n\\begin{aligned}\n\\mathbb{P} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i} - \\mathbb{E} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} X_{i}\n        \\right]\n    \\right\\rvert \\geq t\n\\right)\n& \\leq 2 \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i} - 0\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ 2 n^{2} \\frac{ \\epsilon^{2} }{ 4 } }{ 4 n }\n\\right]\n\\\\\n\\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n& \\leq 2 \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTo get the probability for any h \\in \\mathcal{H}, we apply union bound on all possible label assignments that \\mathcal{H} can make over the set \\mathcal{S} \\cup \\mathcal{S}',\n\n\\begin{aligned}\n\\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n& =\n\\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H}: \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& = \\mathbb{P}_{\\sigma} \\left(\n    \\exist h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}'): \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq \\sum_{h \\in \\mathcal{H} (\\mathcal{S} \\cup \\mathcal{S}')} \\mathbb{P}_{\\sigma} \\left(\n    \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\alpha_{i} \\sigma_{i}\n    \\right\\rvert \\geq \\frac{ \\epsilon }{ 2 } \\mid \\mathcal{S}, \\mathcal{S}'\n\\right)\n\\\\\n& \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\\\\n\\end{aligned}\n\nNote that the term 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right] doesn’t depend on \\mathcal{S}, \\mathcal{S}'. Since the expectation of a constant is that constant, we have proved the claim\n\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    \\mathbb{P}_{\\sigma} (B'' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right] \\leq \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}} \\left[\n    2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n        - \\frac{ n \\epsilon^{2} }{ 8 }\n    \\right]\n\\right] \\leq 2 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\nFinally we can prove the theorem by using all of the claims above\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (B (\\mathcal{S}))\n& \\leq 2 \\mathbb{P}_{\\mathcal{S}, \\mathcal{S}'} (B' (\\mathcal{S}, \\mathcal{S}'))\n\\\\\n& = 2 \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\mathbb{P}_{\\sigma} (B' (\\mathcal{S}, \\mathcal{S}', \\sigma) \\mid \\mathcal{S}, \\mathcal{S}')\n\\right]\n\\\\\n& \\leq 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nBy setting \\delta = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[ - \\frac{ n \\epsilon^{2} }{ 8 } \\right],\n\n\\begin{aligned}\n\\delta\n& = 4 \\Pi_{\\mathcal{H}} (2 n) \\exp \\left[\n    - \\frac{ n \\epsilon^{2} }{ 8 }\n\\right]\n\\\\\nn\n& = 8 \\frac{\n    \\log \\Pi_{\\mathcal{H}} (2 n) + \\log \\frac{ 4 }{ \\delta }\n}{\n    \\epsilon^{2}   \n}.\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html",
    "href": "Knowledge/Unsupervised Learning/K-means.html",
    "title": "K-means",
    "section": "",
    "text": "Updated 01-12-2023 (First commited 02-27-2022)"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#preliminary",
    "href": "Knowledge/Unsupervised Learning/K-means.html#preliminary",
    "title": "K-means",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nEstimator\nIn statistics, an estimator is a function that takes as inputs a set of observations sampled from an unknown probability distribution P_{\\theta}(X) with the true parameter \\theta and outputs the best guess of the parameter \\hat{\\theta} of P_{\\theta}(X).\n\nAn estimator is also a random variable.\n\n\n\nBias\nThe bias of an estimator measures whether the expectation of the estimator is the same as the true parameter. Let \\hat{\\theta} be the output of an estimator for \\theta. The bias of \\hat{\\theta} as an estimator for \\theta is\n \\operatorname{Bias}(\\hat{\\theta}, \\theta) = \\mathbb{E}[\\hat{\\theta}] - \\theta \n\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) = 0, then we say \\hat{\\theta} is an unbiased estimator of \\theta.\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) &gt; 0, \\hat{\\theta} typically overestimates \\theta.\n\\operatorname{Bias}(\\hat{\\theta}, \\theta) &lt; 0, \\hat{\\theta} typically underestimates \\theta.\n\n\n\nVariance (of an estimator)\nThe variance of an estimator measures the degree to which the estimated parameters vary depending on the sampled observations. Let \\hat{\\theta} be the output of an estimator for \\theta. The variance of \\hat{\\theta} as an estimator is\n \\operatorname{Var}(\\hat{\\theta}) = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^{2}] \n\n\nMean squared error\nIn statistics, the mean squared error (MSE) is a risk (loss) function measures the difference between an estimator \\hat{\\theta} with the true parameter \\theta.\n \\operatorname{MSE}(\\hat{\\theta}, \\theta) = \\mathbb{E}[(\\hat{\\theta} - \\theta)^{2}] \nIf the estimator \\hat{\\boldsymbol{\\theta}} and the true parameter \\boldsymbol{\\theta} are vectors,\n \\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) = \\mathbb{E}[\\lVert \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta} \\rVert^{2}] \n\n\nBias-variance decomposition\nA risk (loss) function can be often decomposed into a bias, a variance and a noise term. Here we take MSE as an example and expand it into a bias and variance term (noise is omitted).\n \\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} \n:::{admonition} Proof :class: dropdown\n\n\\begin{align}\n\\operatorname{MSE}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta}) & = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta} \\rVert^{2} ] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] + \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2}] & [\\text{add and subtract } \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] ] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert^{2} + 2 \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert + \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2} ] & [(a + b)^{2} = a^{2} + 2ab + b^{2}] \\\\\n& = \\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert^{2} ] + \\mathbb{E}[ 2 \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert ] + \\mathbb{E}[ \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert^{2} ] & [\\text{Linearity of Expectation}] \\\\\n& = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + 2\\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert] \\mathbb{E}[ \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\boldsymbol{\\theta} \\rVert ] + \\mathbb{E}[ \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} ] \\\\\n& = \\operatorname{Var}(\\hat{\\boldsymbol{\\theta}}) + \\operatorname{Bias}(\\hat{\\boldsymbol{\\theta}}, \\boldsymbol{\\theta})^{2} & [\\mathbb{E}[ \\lVert \\hat{\\boldsymbol{\\theta}} - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert] = \\lVert \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}] \\rVert = 0] \\\\\n\\end{align}\n\n:::"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#problem-formulation",
    "href": "Knowledge/Unsupervised Learning/K-means.html#problem-formulation",
    "title": "K-means",
    "section": "## Problem formulation",
    "text": "## Problem formulation\n\nK-means clustering algorithm is a unsupervised learning algorithm that aims to cluster similar instances into the same group.\nThe input to the algorithm is n instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} \\in \\mathbb{R}^{d} without labels and the output of the algorithm is k centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k} \\in \\mathbb{R}^{d}, where k is a hyperparameter used to control the number of centroids desired.\nThe goal of K-means clustering is to find the best k centroids that minimizes a loss function (often MSE loss of the L_{2} norm or euclidean distance of the difference vector) that captures the overall distances between the instances and the centroids assigned.\n \\operatorname{loss}(\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}) = \\sum_{i = 1}^{n} \\lVert \\mathbf{x}_{i} - \\mathbf{u}_{x_{i}} \\rVert^{2} \nwhere \\mathbf{u}_{x_{i}} is the centroid that instance \\mathbf{x}_{i} is assigned to.\nAnother way to write the loss function is to consider the distances between each centroid and the instances in the cluster that the centroid represents. The benefit of this formulation is that the clusters that the centroids represent are separated as variables, which is easier to do algorithm analyzing.\n \\operatorname{loss}(\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}}) = \\sum_{i = 1}^{k} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}_{i}}} \\lVert \\mathbf{x} - \\mathbf{u}_{i} \\rVert^{2} \nwhere C_{\\mathbf{u}_{i}} is the cluster that centroid \\mathbf{u}_{i} represents."
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#k-means-using-lloyds-algorithm",
    "href": "Knowledge/Unsupervised Learning/K-means.html#k-means-using-lloyds-algorithm",
    "title": "K-means",
    "section": "K-means using Lloyd’s algorithm",
    "text": "K-means using Lloyd’s algorithm\n\n\nAlgorithm\n\nFunction: K-means\nInput: a set of instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} and a hyperparameter k.\nOutput: a set of centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k}. 1. Initialize cluster centroids \\mathbf{u}_{1}, \\mathbf{u}_{2}, \\dots, \\mathbf{u}_{k} by randomly drawing k instances as the centroids. 2. Repeat until convergence or a fixed number of iterations: 1. Assignment step: get the nearest centroid \\mathbf{c}_{i} for each instance \\mathbf{x}_{i}:\n    $$ \\mathbf{c}_{i} = \\arg \\min_{j} \\lVert \\mathbf{x}_{i} - \\mathbf{u}_{j} \\rVert^{2} $$\n\n2. **Refitting step**: update each centroid based on the instances in its cluster:\n\n    $$ \\mathbf{u}_{j} = \\frac{\\sum_{i}^{m} \\mathbb{1}_{\\mathbf{c}_i = j} \\mathbf{x}_{i}}{\\sum_{i}^{m} \\mathbb{1}_{\\mathbf{c}_{i} = j}} $$\n\n\n\nConvergence of Lloyd’s algorithm\nK-means solved using Lloyd’s algorithm is guaranteed to converge to a local minimum because: - The loss value is guaranteed to be smaller or stay the same in the assignment step because each instance \\mathbf{x}_{i} gets the nearest centroid.\n$$ \\operatorname{loss}( \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \\leq \\operatorname{loss}( \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t} ) $$\n    \n\nThe loss value is guaranteed to be smaller or stay the same in the refitting step.\n \\operatorname{loss}( (\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k})^{t + 1}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \\leq \\operatorname{loss}( (\\mathbf{u}_{1}, \\dots, \\mathbf{u}_{k})^{t}, (C_{\\mathbf{u}_{1}}, \\dots, C_{\\mathbf{u}_{k}})^{t + 1} ) \nTo see why this is true, consider a single centroid-cluster pair \\mathbf{u} and C_{\\mathbf{u}}, for all instances \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\dots, \\mathbf{x}_{n} that belongs to the cluster C_{\\mathbf{u}}, the loss function \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) will be minimized when \\mathbf{u} is the average of the instances in C_{\\mathbf{u}}:\n \\boldsymbol{\\mu} = \\frac{1}{n} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\mathbf{x}_{i} = \\arg \\min_{\\mathbf{u} \\in \\mathbb{R}^{d}} \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} = \\arg \\min_{\\mathbf{u} \\in \\mathbb{R}^{d}} \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) \nbecause of the equation below derived from the bias-variance decomposition of MSE function:\n \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) = \\operatorname{loss}(\\mathbf{\\boldsymbol{\\mu}}, C_{\\mathbf{\\mathbf{u}}}) + n \\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert \nwhere n is the number of instances in the cluster C_{\\mathbf{u}}.\n:::{admonition} Proof :class: dropdown\n\n  \\begin{align}\n  \\operatorname{loss}(\\mathbf{u}, C_{\\mathbf{u}}) & = \\operatorname{loss}(\\boldsymbol{\\mu}, C_{\\mathbf{u}}) + n\\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\boldsymbol{\\mu} \\rVert^{2} + n\\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\left( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\mathbf{x} \\rVert \\lVert \\boldsymbol{\\mu} \\rVert + \\lVert \\boldsymbol{\\mu} \\rVert^{2} \\right) + n\\left( \\lVert \\boldsymbol{\\mu} \\rVert^{2} - 2 \\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} \\right) \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\left( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\mathbf{x} \\rVert \\lVert \\frac{S}{n} \\rVert + \\lVert \\frac{S}{n} \\rVert^{2} \\right) + n\\left( \\lVert \\frac{S}{n} \\rVert^{2} - 2 \\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} \\right) & \\left[ \\text{replace some } \\boldsymbol{\\mu} \\text{ with } \\frac{S}{n} \\text{ where } S = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\mathbf{x} \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} 2\\lVert \\mathbf{x} \\rVert \\lVert \\frac{S}{n} \\rVert + \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\frac{S}{n} \\rVert^{2} + n\\lVert \\frac{S}{n} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - 2 \\lVert S \\rVert \\lVert \\frac{S}{n} \\rVert + n\\lVert \\frac{S}{n} \\rVert^{2} + n\\lVert \\frac{S}{n} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} & \\left[ \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert x \\rVert = \\lVert S \\rVert \\text{ and } \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} 1 = n \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} \\rVert^{2} - 2n\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + n\\lVert \\mathbf{u} \\rVert^{2} & \\left[ \\lVert S \\rVert \\lVert \\frac{S}{n} \\rVert = n\\lVert \\frac{S}{n} \\rVert^{2}  \\right] \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} ( \\lVert \\mathbf{x} \\rVert^{2} - 2\\lVert \\boldsymbol{\\mu} \\rVert \\lVert \\mathbf{u} \\rVert + \\lVert \\mathbf{u} \\rVert^{2} ) \\\\\n  & = \\sum_{\\mathbf{x} \\in C_{\\mathbf{u}}} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} \\\\\n  \\end{align}\n  \nSince \\lvert C_{\\mathbf{\\mathbf{u}}} \\rvert \\cdot \\lVert \\boldsymbol{\\mu} - \\mathbf{u} \\rVert is always positive,\n \\operatorname{loss}(\\boldsymbol{\\mu}, C_{\\mathbf{u}}) \\leq \\operatorname{loss}(\\mathbf{\\mathbf{u}}, C_{\\mathbf{\\mathbf{u}}}) \n:::"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#the-k-means-initializer",
    "href": "Knowledge/Unsupervised Learning/K-means.html#the-k-means-initializer",
    "title": "K-means",
    "section": "## The K-means++ initializer",
    "text": "## The K-means++ initializer\nAlthough the default behavior of the K-means algorithm is to initialize the centroids randomly, the quality of the final solution depends heavily on the initialization because K-means is only guaranteed to converge to a local point.\nThe K-means++ initializer is a special way of initializing the centroids so that - the convergence of K-means is faster, - the final loss is bounded (the quality of the final solution won’t be very bad).\n\n\nPick an instance \\mathbf{x} uniformly at random and set T \\gets \\{\\mathbf{x}\\}\nWhile \\lvert T \\rvert &lt; k:\n\nPick an instance \\mathbf{x} at random, with probability proportional to\n \\operatorname{cost}(\\mathbf{x}, T) = \\min_{\\mathbf{u} \\in T} \\lVert \\mathbf{x} - \\mathbf{u} \\rVert^{2} \nAdd \\mathbf{x} to T."
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#reference",
    "href": "Knowledge/Unsupervised Learning/K-means.html#reference",
    "title": "K-means",
    "section": "Reference",
    "text": "Reference\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html\nhttps://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/K-means.html#implementation",
    "href": "Knowledge/Unsupervised Learning/K-means.html#implementation",
    "title": "K-means",
    "section": "Implementation",
    "text": "Implementation\n\n\n#https://takoscribe.com/2020/12/29/kmeans-clustering-with-pytorch/\n\nimport functools\n\nimport tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\nclass GradientKMeans(nn.Module):\n    def __init__(self, num_centroids, n_epochs, batch_size, lr=1e-2):\n        super().__init__()\n        \n        self.num_centroids = num_centroids\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.lr = lr\n\n    def _initialize(self, x):\n        assignment = [i % self.num_centroids for i in range(x.size(0))]\n        random_indices = torch.randperm(len(assignment))\n        random_assignment = torch.LongTensor(assignment)[random_indices]\n        for i in range(self.num_centroids):\n            self.centroids.data[i] = x[random_assignment == i].mean(0)\n        \n    def _assign(self, x):\n        indices = ((x[:,None] - self.centroids) ** 2).mean(2).argmin(1)\n        \n        return indices\n \n    def forward(self, x):\n        return self._assign(x)\n    \n    def fit(self, X):\n        self.centroids = nn.Parameter(torch.zeros(self.num_centroids, X.shape[1]))\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        self.loss = nn.MSELoss()\n        centroids_init = False\n        \n        cost_window = 25\n        costs = []\n\n        X_t = torch.utils.data.TensorDataset(torch.Tensor(X), torch.zeros((X.shape[0], )))\n        iterator = torch.utils.data.DataLoader(X_t, batch_size=self.batch_size, shuffle=True)\n        for i in range(self.n_epochs):\n            with tqdm.tqdm(total=len(X) // self.batch_size) as progress_bar:\n                for x, _ in iterator:\n                    if not centroids_init: \n                        self._initialize(x)\n                        centroids_init = True\n\n                    assignment = self._assign(x)\n                    \n                    self.optimizer.zero_grad()\n                    means = self.centroids[assignment]\n                    cur_cost = self.loss(x, means)\n                    cur_cost.backward()\n                    self.optimizer.step()\n                    \n                    costs.append(cur_cost.item())\n                    \n                    progress_bar.set_postfix({\n                        'KMeans': float(functools.reduce(lambda x, y: x + y, costs[-cost_window:])) /  len(costs[-cost_window:])\n                    })\n                    progress_bar.update(1) # 1 step\n                    \n    def predict(self, X):\n        Y = self(torch.Tensor(X))\n        \n        return Y\n    \n    def get_centroids(self):\n        centroids = self.centroids.cpu().detach().numpy()\n        \n        return centroids\n\ncenters = [[-1, 1], [1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nn_samples = 6000\nX, Y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=0.7, random_state=40)\n\ngradient_kmeans = GradientKMeans(2, 10, 64)\ngradient_kmeans.fit(X)\n\ny_true = gradient_kmeans.predict(X)\ncentroids = gradient_kmeans.get_centroids()\n\nplt.figure(1)\nfor k, col in enumerate([\"r\", \"b\", \"g\", \"m\", \"y\", \"c\"]):\n    cluster_data = y_true == k\n    plt.scatter(X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\n\nplt.scatter(centroids[:, 0], centroids[:, 1], c=\"w\", s=50)\nplt.show()\n\n94it [00:00, 591.45it/s, KMeans=1.01]                                                                                                                                                                                                                                                   \n94it [00:00, 1217.76it/s, KMeans=0.943]                                                                                                                                                                                                                                                 \n94it [00:00, 1212.54it/s, KMeans=0.921]                                                                                                                                                                                                                                                 \n94it [00:00, 1192.24it/s, KMeans=0.941]                                                                                                                                                                                                                                                 \n94it [00:00, 1196.55it/s, KMeans=0.928]                                                                                                                                                                                                                                                 \n94it [00:00, 1179.02it/s, KMeans=0.89]                                                                                                                                                                                                                                                  \n94it [00:00, 1181.19it/s, KMeans=0.922]                                                                                                                                                                                                                                                 \n94it [00:00, 1178.50it/s, KMeans=0.932]                                                                                                                                                                                                                                                 \n94it [00:00, 1096.57it/s, KMeans=0.932]                                                                                                                                                                                                                                                 \n94it [00:00, 1188.89it/s, KMeans=0.909]"
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "",
    "text": "Updated 01-11-2023 (First commited 01-08-2023)\n(bayesian-decision-theory)=\nBayesian decision theory is a statistical view to the machine learning problems. It makes the assumption that the decision problem is posed in probabilistic terms, and that all of the relevant probability values are known. The classifiers obtained under the framework of BDT will always give the best decision rule for each given test instance to minimize the expected total cost defined by the loss function."
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#preliminary",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#preliminary",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "Preliminary",
    "text": "Preliminary\n\nStatistics\n\nJoint Probability\nChain rule (probability)\nBayes Theorem\nThe log trick"
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#basic-concepts",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#basic-concepts",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "Basic concepts",
    "text": "Basic concepts\n\nProbability view of machine learning\nWhen modeling a machine learning problem in the probability setting, both instances \\mathbf{x} and labels y are sampled from different random variables.\n\nAll instances with d features are sampled from a random process of d random variables \\mathbf{X} = \\{ X_{1}, \\dots, X_{d} \\}.\nAll possible labels are also sampled from a random variable Y.\n\nThus, there is always a probability associated with each term:\n\n\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}): the probability that the instance \\mathbf{x} happens in the real world (the joint probability of different features that happen in the real world).\n\\mathbb{P}_{Y}(y): the probability that label y happens in the real world.\n\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y): the joint probability that the both \\mathbf{x} and y happens in the real world.\n\nWe are particularly interested in \\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y), as we can know what should be the correct label y_{t} for the test instance \\mathbf{x}_{t} by selecting y that has the highest \\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}_{t}, y).\nWe can decompose the joint probability according to the chain rule:\n\n\\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y) = \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y),\n\nwhere\n\n\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) is called class conditional probability, which gives the probability of the instance if we know the label is y.\n\\mathbb{P}_{Y}(y) is the class probability.\n\n\n\nDecision function\nGiven an instance \\mathbf{x}, the decision function g(\\cdot) determines its label \\hat{y} according to some rules.\n\n\\hat{y} = g(\\mathbf{x}).\n\n\n\nLoss function\nGiven two labels, which usually are the predicted label from the decision function \\hat{y} and an arbitrary label y, the loss function L(\\hat{y}, y) defines the cost of predicting label \\hat{y} with respect to the label y.\n\nThe cost returned by loss functions should be a non-negative value.\nFor classification problems where the label is a discrete random variable, the loss function can be specified by a matrix \\mathbf{L} = \\mathbb{R}^{d \\times d}, where the cost of predicting label 1 with respect to the label 2 is \\mathbf{L}_{1, 2}."
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#bayes-decision-rule",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#bayes-decision-rule",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "Bayes decision rule",
    "text": "Bayes decision rule\n\nRisk\nAssuming we have a probability model \\mathbb{P}_{\\mathbf{X}, Y}(\\mathbf{x}, y) of the joint probability of \\mathbf{X} and Y, the risk function of the decision function g is defined as the expectation of the loss function over the joint probability\n\n\\begin{aligned}\nR(g)\n& = \\mathbb{E}_{\\mathbf{X}, Y} \\left[\n    L (g(\\mathbf{x}), y)\n\\right]\n\\\\\n& = \\int \\int \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{X}, y) L (g(x), y) \\mathop{d \\mathbf{x}} \\mathop{dy}\n& [\\text{definition of expectation}]\n\\\\\n& = \\int \\int \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{X}) \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) L (g(x), y) \\mathop{d \\mathbf{x}} \\mathop{dy}\n& [\\text{probability chain rule}]\n\\\\\n& = \\int \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) \\int \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{X}) L (g(x), y) \\mathop{dy} \\mathop{d \\mathbf{x}}\n\\\\\n& = \\mathbb{E}_{\\mathbf{X}} \\left[\n    \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n        L (g(x), y)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{X}} \\left[\n    R \\left(\n        \\mathbf{x}, g\n    \\right)\n\\right],\n\\\\\n\\end{aligned}\n\nwhere R(\\mathbf{x}, g) is conditional risk (Bayes risk?), which is the risk given that \\mathbf{x} is known.\n\n\nBayes decision rule (BDR)\nBayes decision rule is the particular decision function g^{*}(\\mathbf{x}) that minimizes the risk\n\n\\begin{aligned}\ng^{*} (\\mathbf{x})\n& = \\arg\\min_{g (\\mathbf{x})} R (g)\n\\\\\n& = \\arg\\min_{g (\\mathbf{x})} R (\\mathbf{x}, g) & [\\text{only } R (\\mathbf{x}, g) \\text{ contains } g (\\mathbf{x})].\n\\\\\n\\end{aligned}\n\nThe risk that Bayes decision rule achieves is called Bayes Risk, which is the minimum risk that any decision function can achieve, if we know the true probability model and its parameters \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})."
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#example-bdr-with-0-1-loss",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#example-bdr-with-0-1-loss",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "Example: BDR with 0-1 loss",
    "text": "Example: BDR with 0-1 loss\nOften time, we are dealing with the classification problem where \\mathbf{X} is a group of continuous random variables and Y is a discrete random variable with m unique values. 0-1 loss is frequently used for the classification problem.\n\n0-1 loss\nThe 0-1 loss is a simple and robust loss function for the classification problems. The 0-1 loss function can be written as:\n\nL(g(\\mathbf{x}), y) =\n\\begin{cases}\n1 & g(\\mathbf{x}) \\neq y \\\\\n0 & g(\\mathbf{x}) = y, \\\\\n\\end{cases}\n\nwhich can also be written as a matrix of \\mathbb{R}^{m}, where the entries in the diagonal are all 0 (g(\\mathbf{x}) = y) and rest are all 1 (g(\\mathbf{x}) \\neq y).\n\n\nMAP rule\nIf we choose 0-1 loss as the loss function for BDR,\n\n\\begin{aligned}\ng^{*} (\\mathbf{x})\n& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    L (g(x), y)\n\\right]\n\\\\\n& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) L (g(\\mathbf{x}), y)\n\\\\\n& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y = g (\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) \\times 0 + \\sum_{y \\neq g(\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x}) \\times 1\n\\\\\n& = \\arg\\min_{g (\\mathbf{x})} \\sum_{y \\neq g (\\mathbf{x})}^{m} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x})\n\\\\\n& = \\arg\\min_{g (\\mathbf{x})} 1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (g (\\mathbf{x}) \\mid \\mathbf{x})\n\\\\\n& = \\arg\\max_{g (\\mathbf{x})} \\mathbb{P}_{Y \\mid \\mathbf{X}} (g (\\mathbf{x}) \\mid \\mathbf{x}) & [\\arg\\min_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))]\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}).\n\\end{aligned}\n\nSince the last equation is maximizing the posterior probability according to Bayes Theorem, the optimal decision rule for 0-1 loss is also called maximum a-posteriori probability (MAP) rule.\nAccording to Bayes Theorem,\n\n\\begin{aligned}\n\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})}\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y],\n\\\\\n\\end{aligned}\n\nMAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior).\nIn practice, the class conditional probability and class probability can be more easily obtained from the data than the posterior probability.\n\n\nThe log trick\nUsing the log trick, the BDR for 0-1 loss is often calculated using:\n\n\\begin{aligned}\n\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)\n\\\\\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#example-bdr-with-squared-error-loss",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#example-bdr-with-squared-error-loss",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "Example: BDR with squared error loss",
    "text": "Example: BDR with squared error loss\nFor regression problems where the labels are continuous values, a common loss function is squared error loss\n\nL (g(x), y) = (g(x) - y)^{2}.\n\nPlug the loss function in BDR\n\n\\begin{aligned}\ng^{*} (\\mathbf{x})\n& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    L (g(x), y)\n\\right]\n\\\\\n& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    (g (\\mathbf{x}) - y)^{2}\n\\right]\n\\\\\n& = \\arg\\min_{g(\\mathbf{x})} \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    (g (\\mathbf{x})^{2} - 2 g (\\mathbf{x})^{2} y + y^{2}\n\\right]\n\\\\\n& = \\arg\\min_{g(\\mathbf{x})} g (\\mathbf{x})^{2} - 2 g (\\mathbf{x}) \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    y\n\\right] + \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    y^{2}\n\\right].\n\\end{aligned}\n\nThe minimization problem can be solved by setting the its derivative w.r.t g (\\mathbf{x}) to 0\n\n\\begin{aligned}\n\\frac{\n    \\mathop{d}\n}{\n    \\mathop{d g (\\mathbf{x})}\n} \\left[\n    g (\\mathbf{x})^{2} - 2 g (\\mathbf{x}) \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n        y\n    \\right] + \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n        y^{2}\n    \\right]\n\\right]\n& = 0\n\\\\\n2 g (\\mathbf{x}) - 2 \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    y\n\\right]\n& = 0\n\\\\\ng (\\mathbf{x})\n& = \\mathbb{E}_{Y \\mid \\mathbf{X}} \\left[\n    y\n\\right]\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#references",
    "href": "Knowledge/Statistical Learning/1_Bayesian_Decision_Theory.html#references",
    "title": "Bayesian Decision Theory (BDT)",
    "section": "References",
    "text": "References\n\nhttp://pillowlab.princeton.edu/teaching/mathtools16/slides/lec18_BayesianEstim.pdf"
  },
  {
    "objectID": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html",
    "href": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html",
    "title": "Maximum Likelihood Estimation (MLE)",
    "section": "",
    "text": "Updated 01-11-2023 (First commited 01-08-2023)\n(maximum-likelihood-estimation)=\nMaximum likelihood estimation (MLE) is a principle to find the best parameter for a probability distribution that best explains a sampled dataset. The instances in the dataset are supposed be independently sampled from the same distribution. MLE gives a relatively easy way to estimate parameters from the data."
  },
  {
    "objectID": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html#maximum-likelihood",
    "href": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html#maximum-likelihood",
    "title": "Maximum Likelihood Estimation (MLE)",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\nLikelihood function\nGiven the dataset \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}, the term likelihood function simply refers to the probability function of \\mathcal{X} with the distribution parameters for \\mathbf{X} being \\boldsymbol{\\theta}\n\nf(\\boldsymbol{\\theta}) = \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}),\n\nwhich measures how likely are the parameters \\boldsymbol{\\theta} given the data \\mathcal{X}.\nNotes:\n\nSince the dataset are known, the probability function is only dependent on the parameters \\boldsymbol{\\theta}, and thus doesn’t have the same shape as the probability density of the variable \\mathbf{X}.\nThe semicolon indicate that \\boldsymbol{\\theta} is a parameter (fixed value) instead of a random variable.\n\n\n\nMLE Procedures\n\nWe collect a set of instances \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} that we believe should be from the same distribution.\nWe select a parametric model \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta}) that we think can best explains the data.\nWe select the parameters \\boldsymbol{\\theta}^{*} to be the ones that maximize the probability of the data:\n\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X}; \\boldsymbol{\\theta})\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\mathcal{X} \\text{ are i.i.d samples }]\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\ln \\prod_{i = 1}^{n} \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}) & [\\text{the log trick}]\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i = 1}^{n} \\log \\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}_{i} ; \\boldsymbol{\\theta}).\n\\end{aligned}\n\n\n\nOptimization methods\nIf the likelihood function is concave, the best parameters \\boldsymbol{\\theta}^{*} that maximize the likelihood function are the ones that make the gradient of f(\\boldsymbol{\\theta}) with respect to \\boldsymbol{\\theta} 0 and at the same time has negative semidefinite hessian matrix.\n\n\\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\mathbf{X}}(\\mathcal{X} ; \\boldsymbol{\\theta}) = 0\n\n\n\\nabla_{\\boldsymbol{\\theta}}^{2} f(\\boldsymbol{\\theta}) \\preceq 0\n\nOtherwise, other numerical methods or algorithms might need to employed to solve the optimization problem."
  },
  {
    "objectID": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html#example-linear-regression",
    "href": "Knowledge/Statistical Learning/2_Maximum_Likelihood_Estimation.html#example-linear-regression",
    "title": "Maximum Likelihood Estimation (MLE)",
    "section": "Example: linear regression",
    "text": "Example: linear regression\nGiven an instance \\mathbf{x} \\in \\mathbb{R}^{d}, the function f (\\cdot) is a linear function if it has the form\n\n\\begin{aligned}\nf (\\mathbf{x})\n& = \\mathbf{w}^{T} \\mathbf{x} + b\n\\\\\n& = \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}},\n\\\\\n\\end{aligned}\n\nwhere\n\n\\mathbf{w} is weight vector and b is the bias term,\n\\boldsymbol{\\theta} is the parameter vector that includes both weights and bias\n\n  \\boldsymbol{\\theta} = [ \\mathbf{w}, b ]^{T},\n  \n\\hat{\\mathbf{x}} is the instance vector appended with a constant 1\n\n  \\hat{\\mathbf{x}} = [ \\mathbf{x}, 1 ]^{T}.\n  \n\nGiven a dataset with instances \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} and labels \\mathcal{Y} = \\{ y_{1}, \\dots, y_{n} \\}, linear regression is often formulated as a problem of finding parameters \\boldsymbol{\\theta} that minimize the mean squared error (MSE) loss function\n\n\\begin{aligned}\n& \\arg\\min_{\\boldsymbol{\\theta}} L_{\\text{MSE}}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{n} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\right)^{2}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\right)^{2}\n\\\\\n\\end{aligned}\n\nwhich can be written in matrix notations by treating \\mathcal{X} as a matrix \\mathbf{X} \\in \\mathbb{R}^{n \\times d} and \\mathcal{Y} \\in \\mathbb{R}^{n \\times 1} as a vector \\mathbf{y}\n\n\\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2},\n\nwhere \\hat{\\mathbf{X}} is the instance matrix append with a column of 1 in the last column.\n\nLinear regression as MLE\nSolving the optimization problem of linear regression with MSE loss can be formulated as solving MLE of parameters for a univariate normal distribution.\nFirst we can consider the difference \\epsilon between y and \\boldsymbol{\\theta}^{T} \\mathbf{x} for any instance \\mathbf{x} as a random variable that follows the a univariate normal distribution with zero mean and known variance\n\ny - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}} = \\epsilon \\sim \\mathcal{G} \\left(\n    \\epsilon, 0, \\sigma^{2}\n\\right).\n\nSince \\boldsymbol{\\theta}^{T} \\mathbf{x} is a not a random variable (constant), the label y for any instance \\mathbf{x} is thus a univariate normal random variable with its mean being the predicted label of the linear function\n\n\\begin{aligned}\ny\n& = \\epsilon + \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}\n\\\\\n& \\sim \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right),\n\\end{aligned}\n\nIn other words, the conditional probability of any label given the instance \\mathbf{x} follows a univariate normal distribution\n\n\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right) = \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right).\n\n%%markdown\nGiven a set of instances \\mathcal{X} and their labels \\mathcal{Y}, \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left( y \\mid \\mathbf{x} \\right) is a likelihood function of parameters \\boldsymbol{\\theta} and thus MLE can be used to find the best parameters, which can be shown to have the optimization problem of fitting a linear function with MSE loss\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\rVert^{2}_{2}.\n\\end{aligned}\n\n:::{admonition} Proof: \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left( y \\mid \\mathbf{x} \\right) = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left( y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i} \\right)^{2} :class: dropdown\n\n\\begin{aligned}\n\\boldsymbol{\\theta}^{*}\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log \\mathcal{G} \\left(\n    y, \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}, \\sigma^{2}\n\\right)\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\log - \\frac{\n    1\n}{\n    \\sqrt{2 \\pi \\sigma^{2}}\n} \\exp{ - \\frac{\n    \\left(\n        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n    \\right)^{2}\n}{\n    2 \\sigma^{2}\n}}\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} - \\frac{1}{2} \\log \\left(\n    2 \\pi \\sigma^{2}\n\\right) - \\sum_{i=1}^{n} \\frac{\n    \\left(\n        y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n    \\right)^{2}\n}{\n    2 \\sigma^{2}\n}\n\\\\\n& = \\arg\\max_{\\boldsymbol{\\theta}} - \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n& [\\text{only need term with } \\boldsymbol{\\theta}]\n\\\\\n& = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{i=1}^{n} \\left(\n    y_{i} - \\boldsymbol{\\theta}^{T} \\hat{\\mathbf{x}}_{i}\n\\right)^{2}\n\\\\\n\\end{aligned}\n\n:::\n\n\nSolving linear regression\nSolving linear regression is the same in both formulations by setting the gradient w.r.t \\boldsymbol{\\theta} to 0 and solve for \\boldsymbol{\\theta}\n\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\theta}} \\lVert \\mathbf{y} - \\hat{\\mathbf{X}} \\boldsymbol{\\theta} \\lVert^{2}_{2}\n& = 0\n\\\\\n2 \\left(\n    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta} - \\hat{\\mathbf{X}}^{T} \\mathbf{y}\n\\right)\n& = 0\n\\\\\n\\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}} \\boldsymbol{\\theta}\n& = \\hat{\\mathbf{X}}^{T} \\mathbf{y}\n\\\\\n\\boldsymbol{\\theta}\n& = \\left(\n    \\hat{\\mathbf{X}}^{T} \\hat{\\mathbf{X}}\n\\right)^{-1} \\hat{\\mathbf{X}}^{T} \\mathbf{y}.\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html",
    "title": "Joeyonng",
    "section": "",
    "text": "The union bound states that the probability that at least one event in any finite set of events happens A_{1}, \\dots, A_{n} is no greater than the sum of the probabilities of the individual events\n\n\\mathbb{P} \\left(\n    \\exists i \\in [1, n]: A_{i}\n\\right) = \\mathbb{P} \\left(\n    \\bigcup_{i = 1}^{n} A_{i}\n\\right) \\leq \\sum_{i = 1}^{n} \\mathbb{P} (A_{i})."
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#booles-inequality-union-bound",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#booles-inequality-union-bound",
    "title": "Joeyonng",
    "section": "",
    "text": "The union bound states that the probability that at least one event in any finite set of events happens A_{1}, \\dots, A_{n} is no greater than the sum of the probabilities of the individual events\n\n\\mathbb{P} \\left(\n    \\exists i \\in [1, n]: A_{i}\n\\right) = \\mathbb{P} \\left(\n    \\bigcup_{i = 1}^{n} A_{i}\n\\right) \\leq \\sum_{i = 1}^{n} \\mathbb{P} (A_{i})."
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#markovs-inequality",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#markovs-inequality",
    "title": "Joeyonng",
    "section": "Markov’s inequality",
    "text": "Markov’s inequality\nMarkov’s inequality states that the probability of a random variable larger than t is less than \\frac{\\mu}{t}.\nLet X \\geq 0 be a non-negative random variable with finite mean \\mu. Then for any t &gt; 0,\n\n\\mathbb{P}_{X} \\left(\n    x \\geq t\n\\right) \\leq \\frac{ \\mu }{ t }.\n\n:::{prf:proof} Markov’s inequality :label:markov’s-inequality :class:dropdown\nAssuming X is a discrete random variable,\n\n\\mu = \\sum_{x} \\mathbb{P}_{X} (x) x = \\sum_{x \\geq t} \\mathbb{P}_{X} (x) x + \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x.\n\nSince X \\geq 0, \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x is non-negative, and therefore\n\n\\begin{aligned}\n\\mu\n& = \\sum_{x \\geq t} \\mathbb{P}_{X} (x) x + \\sum_{x &lt; t} \\mathbb{P}_{X} (x) x.\n\\\\\n& \\geq \\sum_{x \\geq t} \\mathbb{P}_{X} (x)\n\\\\\n& = t \\sum_{x \\geq t} \\mathbb{P}_{X} (x)\n\\\\\n& = t \\mathbb{P}_{X} (x \\geq t)\n\\end{aligned}\n\nWe have Markov’s inequality by rearranging the term\n\n\\mathbb{P}_{X} (x \\geq t) \\leq \\frac{\\mu}{t}.\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#chebyshevs-inequality",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#chebyshevs-inequality",
    "title": "Joeyonng",
    "section": "Chebyshev’s inequality",
    "text": "Chebyshev’s inequality\nChebyshev’s inequality states that the probability of the difference between a random variable and its mean larger than a is less than \\frac{\\sigma^{2}}{t^{2}}.\nLet X be a random variable with finite mean \\mu and finite variance \\sigma^{2}. Then for any t &gt; 0,\n\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right) \\leq \\frac{ \\sigma^{2} }{ t^{2} }.\n\n:::{prf:proof} Chebyshev’s inequality :label:chebyshev’s-inequality :class:dropdown\nLet Y = (x - \\mathbb{E}_{X} [x])^{2} and apply Markov’s inequality on Y to get\n\n\\begin{aligned}\n\\mathbb{P}_{Y} (y \\geq t^{2})\n& \\leq \\frac{ \\mathbb{E}_{Y} [y] }{ t^{2} }\n\\\\\n& = \\frac{\n    \\mathbb{E}_{X} [(x - \\mathbb{E}_{X} [x])^{2}]\n}{\n    a^{2}\n}\n\\\\\n& = \\frac{\n    \\mathrm{Var}_{X} [x]\n}{\n    t^{2}\n}.\n\\end{aligned}\n\nNote that\n\n\\mathbb{P}_{Y} (y \\geq t^{2}) = \\mathbb{P}_{X} \\left(\n    (x - \\mathbb{E}_{X} [x])^{2} \\geq t^{2}\n\\right) = \\mathbb{P}_{X} \\left(\n    \\lvert x - \\mathbb{E}_{X} [x] \\rvert \\geq t\n\\right).\n\nTherefore,\n\n\\begin{aligned}\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mathbb{E}_{X} [x] \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\mathrm{Var}_{X} [x] }{ t^{2} }.\n\\\\\n\\mathbb{P}_{X} \\left(\n    \\lvert x - \\mu \\rvert \\geq t\n\\right)\n& \\leq \\frac{ \\sigma^{2} }{ t^{2} }.\n\\end{aligned}\n\n:::\n\nAverage of random variables converge to mean\nLet X_{1}, \\dots, X_{n} be independent random variables with finite means \\mu_{1}, \\dots, \\mu_{n} and finite variances \\sigma_{1}^{2}, \\dots, \\sigma_{n}^{2}. Then, for any t &gt; 0\n\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} x_{i} - \\sum_{i = 1}^{n} \\mu_{i}\n    \\right\\rvert \\geq t\n\\right) \\leq \\frac{\n    \\sum_{i = 1}^{n} \\sigma_{i}^{2}\n}{\n    t^{2}\n}"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#weak-law-of-large-numbers",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#weak-law-of-large-numbers",
    "title": "Joeyonng",
    "section": "Weak law of large numbers",
    "text": "Weak law of large numbers\nThe weak law of large numbers (WLLN) states that the mean of the i.i.d random variables X_{1}, \\dots, X_{n} converges to the mean of the distribution as n increases.\nLet X_{1}, \\dots, X_{n} be i.i.d random variables with finite mean \\mu and finite variance \\sigma^{2}. Then for any a &gt; 0\n\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\frac{\\sum_{i = 1}^{n} x_{i}}{n} - \\mu\n        \\right\\rvert \\geq \\epsilon\n\\right) \\leq \\frac{\\sigma^{2}}{n \\epsilon^{2}}\n\n:::{prf:proof} Weak law of large number :label:weak-law-of-large-number :class:dropdown\nApplying the Chebyshev’s inequality over multiple random variables, we get\n\n\\begin{aligned}\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} x_{i} - \\sum_{i = 1}^{n} \\mu_{i}\n    \\right\\rvert \\geq a\n\\right)\n& \\leq \\frac{\\sum_{i = 1}^{n} \\sigma_{i}^{2}}{a^{2}}\n\\\\\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} x_{i} - n \\mu\n    \\right\\rvert \\geq a\n\\right)\n& \\leq \\frac{n \\sigma^{2}}{a^{2}}\n\\end{aligned}\n\nSetting a = n \\epsilon,\n\n\\begin{aligned}\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\sum_{i = 1}^{n} x_{i} - n \\mu\n    \\right\\rvert \\geq n \\epsilon\n\\right)\n& \\leq \\frac{n \\sigma^{2}}{n^{2} \\epsilon^{2}}.\n\\\\\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left(\n    \\left\\lvert\n        \\frac{\\sum_{i = 1}^{n} x_{i}}{n} - \\mu\n    \\right\\rvert \\geq \\epsilon\n\\right)\n& \\leq \\frac{\\sigma^{2}}{n \\epsilon^{2}}.\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#moment-generating-function",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#moment-generating-function",
    "title": "Joeyonng",
    "section": "Moment Generating Function",
    "text": "Moment Generating Function\nThe nth moment of a random variable X is \\mathbb{E}_{X} [x^{n}] and the nth central moment of X is \\mathbb{E}_{X} [(x - \\mathbb{E}_{X} [x])^{n}].\nThe moment generating function (MGF) of a random variable X is a function M_{X} (s) that is defined as\n\nM_{X} (s) = \\mathbb{E}_{X} [e^{s x}].\n\nThe nth moment of the random variable can be derived by taking the nth derivative of M_{X} (s) and evaluating it at s = 0\n\n\\mathbb{E}_{X} [x^{n}] = \\frac{d^{n}}{ds^{n}} M_{X} (s) \\left.\\right|_{s = 0}.\n\n:::{prf:proof} Finding moments from MGF :label: finding-moments-from-mgf :class:dropdown\nRecall that the Taylor series for e^{x} is\n\ne^{x} = 1 + x + \\frac{x^{2}}{2!} + \\dots = \\sum_{i = 0}^{\\infty} \\frac{x^{i}}{i !}.\n\nTherefore, the Taylor series of e^{s x} is\n\ne^{sx} = \\sum_{i = 0}^{\\infty} \\frac{(s x)^{i}}{i !} = \\sum_{i = 0}^{\\infty} \\frac{s^{i} x^{i}}{i !}.\n\nThe moment generating function can be written as\n\nM_{X} (s) = \\mathbb{E}_{X} [e^{s x}] = \\mathbb{E}_{X} \\left[\n    \\sum_{i = 0}^{\\infty} \\frac{s^{i} x^{i}}{i !}\n\\right] = \\sum_{i = 0}^{\\infty} \\frac{\\mathbb{E}_{X} [x^{i}] s^{i} }{i !}.\n\nThe nth moment is the coefficient of \\frac{s^{k}}{k !} in the Taylor series ofM_{X} (s), which can be obtained by taking nth derivative M_{X} (s) and evaluating it at s = 0.\n:::\n\nProperties of moment generating function\nSuppose X_{1}, \\dots, X_{n} are independent random variables and X = \\sum_{i = 1}^{n} X_{i}.\n\nM_{X} (s) = \\prod_{i = 1}^{n} M_{X_{i}} (s)\n\n:::{prf:proof} Finding moments from MGF :label: finding-moments-from-mgf :class:dropdown\n\n\\begin{aligned}\nM_{X} (s)\n& = \\mathbb{E} [e^{s X}]\n\\\\\n& = \\mathbb{E} [e^{s \\sum_{i = 1} X_{i}}]\n\\\\\n& = \\mathbb{E} [\\prod_{i = 1}^{n} e^{s X_{i}}]\n\\\\\n& = \\prod_{i = 1}^{n} [e^{s X_{i}}]\n& [X_{i}\\text{s are independent}]\n\\\\\n& = \\prod_{i = 1}^{n} M_{X_{i}} (s)\n\\\\\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities1.html#chernoff-bound",
    "href": "Knowledge/Probability and Statistics/Inequalities1.html#chernoff-bound",
    "title": "Joeyonng",
    "section": "Chernoff bound",
    "text": "Chernoff bound\n\nChernoff’s bounding method\nLet X be a random variable on \\mathbb{R}. Then for all the t &gt; 0\n\n\\mathbb{P}_{X} (x \\geq t) \\leq \\inf_{s &gt; 0} \\frac{\n    M_{X} (s)\n}{\n    e^{s t}\n} = \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s t}\n}\n\nwhere M_{X} (s) is the MGF of X.\n:::{prf:proof} Chernoff’s bounding method :label: chernoff’s-bounding-method :class:dropdown\nFor any t &gt; 0, we can apply Markov’s inequality\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& = \\mathbb{P}_{X} (e^{s x} \\geq e^{s t})\n\\\\\n& \\leq \\frac{ \\mathbb{E}_{X} [e^{s x}] }{ e^{s t} }\n\\\\\n& \\leq \\frac{M_{X} (s) }{ e^{s t} }.\n\\end{aligned}\n\nSince \\mathbb{P}_{X} (x \\geq t) \\leq \\frac{ M_{X} (s) }{ e^{s t} } for any s &gt; 0, \\mathbb{P}_{X} (x \\geq t) is less than the s that minimizes \\frac{ M_{X} (s) }{ e^{s t} }, which is written as\n\n\\mathbb{P}_{X} (x \\geq t) \\leq \\inf_{s &gt; 0} \\frac{M_{X} (s)}{e^{s t}}.\n\n:::\nThe closed-form solution of the optimization problem\n\n\\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s t}\n} = \\inf_{s &gt; 0} f (s)\n\ncan be obtained by solving f' (s) = 0 if f (s) is a convex function.\n\n\nChernoff bound for Bernoulli random variables\nLet X_{1}, \\dots, X_{n} be bounded independent Bernoulli random variables such that X_{i} \\sim \\mathrm{Ber} (p_{i}), \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} (x).\n\nUpper tail: for all \\delta &gt; 0\n\n  \\mathbb{P}_{X} (X \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu }{ 2 + \\delta }\n  \\right]\n  \nLower tail: for all 0 &lt; \\delta &lt; 1\n\n  \\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu }{ 2 }\n  \\right]\n  \n\n:::{prf:proof} Chernoff bound :label: chernoff-bound :class:dropdown\nLet X_{i} \\sim \\mathrm{Ber} (p) be a Bernoulli variable. Then the MGF of X_{i} can be written as\n\n\\begin{aligned}\nM_{X_{i}} (s)\n& = \\mathbb{E}_{X_{i}} (e^{s X_{i}})\n\\\\\n& = p e^{1 \\times s} + (1 - p) e^{0 \\times s}\n\\\\\n& = 1 + p (e^{s} - 1).\n\\\\\n\\end{aligned}\n\nWe can obtain the following inequality for MGF of X_{i}\n\nM_{X_{i}} (s) = 1 + p (e^{s} - 1) \\leq \\exp[p (e^{s} - 1)],\n\nwhich follows from the fact that 1 + a \\leq e^{a} for any a.\nAccording to the property of the MGF, the same inequality can be obtained for the MGF of X = \\sum_{i = 1}^{n} X_{i}\n\n\\begin{aligned}\n\\mathbb{E}_{X} [e^{s x}]\n& = M_{X} (s)\n\\\\\n& = \\prod_{i = 1}^{n} M_{X_{i}} (s)\n\\\\\n& \\leq \\prod_{i = 1}^{n} \\exp[p (e^{s} - 1)]\n\\\\\n& = \\exp \\left[\n    (e^{s} - 1) \\sum_{i = 1}^{n} p_{i}\n\\right]\n\\\\\n& = \\exp \\left[\n    (e^{s} - 1) \\mu\n\\right]\n\\end{aligned}\n\nwhere \\mu = \\mathbb{E}_{X} (x) = \\sum_{i = 1}^{n} p_{i}.\nBy applying the Chernoff bounding method with t = (1 + \\delta) \\mu, we can derive a upper bound\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq (1 + \\delta) \\mu)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\exp[(e^{s} - 1) \\mu]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right].\n\\end{aligned}\n\nSince \\exp [(e^{s} - 1) \\mu - (1 + \\delta) \\mu s] is a convex function\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right]\n& = 0\n\\\\\n(\\mu e^{s} - (1 + \\delta) \\mu) \\exp\\left[\n    (e^{s} - 1) \\mu - (1 + \\delta) \\mu s\n\\right]\n& = 0\n\\\\\n\\mu e^{s} - (1 + \\delta) \\mu\n& = 0\n\\\\\ns\n& = \\log [1 + \\delta].\n\\end{aligned}\n\nNow let’s select s = \\log [1 + \\delta] to get the tight upper bound\n\n\\begin{aligned}\n\\frac{\n    \\exp[(e^{s} - 1) \\mu]\n}{\n    e^{s (1 + \\delta) \\mu}\n}\n& = \\frac{\n    \\exp[(e^{\\log[1 + \\delta]} - 1) \\mu]\n}{\n    e^{\\log[1 + \\delta] (1 + \\delta) \\mu}\n}\n\\\\\n& = \\frac{\n    e^{\\delta \\mu}\n}{\n    (1 + \\delta)^{(1 + \\delta) \\mu}\n}\n\\\\\n& = \\left(\n    \\frac{e^{\\delta}}{(1 + \\delta)^{1 + \\delta}}\n\\right)^{\\mu}.\n\\end{aligned}\n\nTaking \\exp \\log on the upper bound to derive\n\n\\begin{aligned}\n\\left(\n    \\frac{ e^{\\delta} }{ (1 + \\delta)^{1 + \\delta} }\n\\right)^{\\mu}\n& = \\exp \\left[\n    \\log \\left[\n        \\left(\n            \\frac{ e^{\\delta} }{ (1 + \\delta)^{1 + \\delta} }\n        \\right)^{\\mu}\n    \\right]\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\log e^{\\delta} - \\log \\left[\n            (1 + \\delta)^{1 + \\delta}\n        \\right]\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\delta - (1 + \\delta) \\log [(1 + \\delta)]\n    \\right)\n\\right].\n\\end{aligned}\n\nSince \\log [1 + x] \\geq \\frac{ x }{ 1 + \\frac{x}{2} } for all x &gt; 0,\n\n\\begin{aligned}\n\\exp \\left[\n    \\mu \\left(\n        \\delta - (1 + \\delta) \\log [(1 + \\delta)]\n    \\right)\n\\right]\n& \\leq \\exp \\left[\n    \\mu \\left(\n        \\delta - \\frac{ (1 - \\delta) \\delta }{ 1 + \\frac{\\delta}{2} }\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    \\mu \\left(\n        \\frac{ (1 + \\frac{\\delta}{2}) \\delta }{ 1 + \\frac{ \\delta }{ 2 } } - \\frac{ (1 - \\delta) \\delta }{ 1 + \\frac{ \\delta }{ 2 } }\n    \\right)\n\\right]\n\\\\\n& = \\exp \\left[\n    - \\left(\n        \\frac{ \\delta^{2} }{ 2 + \\delta }\n    \\right) \\mu\n\\right].\n\\end{aligned}\n\nPutting all together to derive the upper tail of Chernoff bound\n\n\\mathbb{P}_{X} (x \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n    - \\left(\n        \\frac{ \\delta^{2} }{ 2 + \\delta }\n    \\right) \\mu\n\\right].\n\nThe proof for the lower tail is similar except that\n\ntaking s = \\log [1 - \\delta] to maximize the lower bound, and\napplies the inequality \\log [1 - x] \\geq -x + \\frac{ x^{2} }{ 2 } in the last step for 0 &lt; x &lt; 1.\n\n:::\n\n\nGeneral Chernoff bound\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a, b], \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} (x). Then for \\delta &gt; 0\n\nUpper tail:\n\n  \\mathbb{P}_{X} (X \\geq (1 + \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ 2 \\delta^{2} \\mu^{2} }{ n (b - a)^{2} }\n  \\right]\n  \nLower tail:\n\n  \\mathbb{P}_{X} (X \\leq (1 - \\delta) \\mu) \\leq \\exp \\left[\n      -\\frac{ \\delta^{2} \\mu^{2} }{ n (b - a)^{2} }\n  \\right]"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Random Variables.html",
    "href": "Knowledge/Probability and Statistics/Random Variables.html",
    "title": "Joeyonng",
    "section": "",
    "text": "\\mathbb{E}[aX + b] = a \\mathbb{E}[X] + b \n \\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y] \n \\mathbb{V}[aX + b] = a^{2} \\mathbb{V}[X] \n \\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y] + \\mathrm{Cov}(X, Y)"
  },
  {
    "objectID": "Notes/Birkhoff+.html",
    "href": "Notes/Birkhoff+.html",
    "title": "Birkhoff+",
    "section": "",
    "text": "07-12-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/Birkhoff+.html#problem-definition",
    "href": "Notes/Birkhoff+.html#problem-definition",
    "title": "Birkhoff+",
    "section": "Problem definition:",
    "text": "Problem definition:\nFor a given n \\times n doubly stochastic matrix X^{\\star} and an \\epsilon \\geq 0, our goal is to find a small collection of permutation matrices P_{1}, P_{2}, \\ldots, P_{k}, and weights \\theta_{1}, \\theta_{2}, \\ldots, \\theta_{k} with \\sum_{i=1}^{k}\\theta_{i} \\leq 1 such that \\lVert X^{\\star} - \\sum_{i=1}^{k}\\theta_{i}P_{i} \\rVert_{F} \\leq \\epsilon."
  },
  {
    "objectID": "Notes/Birkhoff+.html#definitions",
    "href": "Notes/Birkhoff+.html#definitions",
    "title": "Birkhoff+",
    "section": "Definitions",
    "text": "Definitions\n\nWhen applied to vectors or matrices, +, -, \\cdot are all element-wise operations and we write x &lt; (\\leq, &gt;, \\geq ) y when all elements of x are &lt; (\\leq, &gt;, \\geq) of all elements of y.\nDoubly stochastic matrix: a doubly stochastic matrix is a square matrix of non-negative real numbers, each of whose rows and columns sums to 1.\nFrobenius norm: \\lVert X \\rVert_{F} = \\sqrt{\\sum_{i, j}\\vert X(i, j) \\vert^2}, which is the same as the l2 norm of a vector: \\lVert x \\rVert_{2} = \\sqrt{\\sum_{i}\\vert x(i) \\vert^2}\nConvex hull \\mathrm{conv}(A): The convex hull of set A (\\mathrm{conv}(A)) is the smallest convex set that contains set A.\nBirkhoff polytope \\mathcal{B}: the convex set that contains all doubly stochastic matrices of size n \\times n.\nPermutation matrices \\mathcal{P}: the extreme points of the Birkhoff polytope.\nLinear program \\mathrm{LP}(c, \\mathcal{X}): minimize c^{T}x, subject to x \\in \\mathcal{X}. We assume that the solution returned is always an extreme point (x \\in \\mathcal{P} if \\mathcal{X} = \\mathcal{B}).\nBirkhoff step: \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) = \\min_{a,b}\\{(X^{\\star}(a, b) - X_{k-1}(a,b) - 1)P_{k}(a,b) + 1\\}. It computes the step size (weight) by taking the minimum non-zero element of the difference matrix (masked by the permutation matrix P_{k}) between X^{\\star} and X."
  },
  {
    "objectID": "Notes/Birkhoff+.html#algorithms-all-matrices-x-are-represented-using-vectors-x-by-stacking-the-matrix-columns",
    "href": "Notes/Birkhoff+.html#algorithms-all-matrices-x-are-represented-using-vectors-x-by-stacking-the-matrix-columns",
    "title": "Birkhoff+",
    "section": "Algorithms (All matrices (X) are represented using vectors (x) by stacking the matrix columns)",
    "text": "Algorithms (All matrices (X) are represented using vectors (x) by stacking the matrix columns)\n\nGeneral Birkhoff algorithm (Algorithm 1, 2 and 3)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1}\\theta_{i}) \\mathbin{/} n^2 // (Why this value)\n  p_{k} \\gets p \\in \\mathcal{I}_{k}(\\alpha)   // Get next permutation matrix\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)   // Get next weight based on the new permutation matrix\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nn is the size of X^{\\star} and d=n^2 is the number of elements in X^{\\star}.\n\\mathcal{I}_{k}(\\alpha) =\\{P \\in \\mathcal{P} \\mid X_{k-1}(a,b) + \\alpha P(a,b) \\leq X^{\\star}(a,b) \\}.\n\nIf P_{k} \\in \\mathcal{I}_{k}(\\alpha), then (X^{\\star} - X_{k-1}) \\cdot P_{k} \\geq \\alpha, (each element of the difference matrix between X^{\\star} and X_{k-1} masked by P_{k} is \\geq \\alpha).\nSince \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) is taking the minimum of non-zero element of (X^{\\star} - X_{k-1}) \\cdot P_{k} (definition 7), \\mathrm{STEP}(X^{\\star}, X_{k-1}, P_{k}) \\geq \\alpha\n\nDue to the definitions of \\mathcal{I}_{k}(\\alpha) and \\mathrm{STEP}, x_{k} \\leq x^{\\star} all the time.\n\nBirkhoff algorithm can be seen as constructing a path from the origin (x_{0} = 0) to x^{\\star} while always remaining in a dotted box, where x_{0} and x^{\\star} are two diagonal vertices of the box (x_{k} \\leq x^{\\star}).\n\nGeneral Birkhoff algorithm doesn’t specify which specific permutation to select from \\mathcal{I}_{k}(\\alpha).\n\n\nClassic Birkhoff algorithm (Algorithm 4)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  p_k \\gets \\mathrm{LP}(-\\lceil x^{\\star} - x_{k-1} \\rceil, \\mathcal{B}).\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)  \n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nThe only slight difference is line 3, which is essentially doing the same thing with general Birkhoff algorithm.\n\nSince x^{\\star} - x_{k-1} \\in [0, 1], its ceiling (\\lceil x^{\\star} - x_{k-1} \\rceil) makes all nonzero elements to be 1, which makes \\mathrm{LP} treat all nonzero elements equally.\n\n\n\n\n\nFrank-Wolfe (FW) algorithm (Algorithm 5)\n\n\nx_{0} \\in \\mathcal{P}, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  p_k \\gets \\mathrm{LP}(-(x^{\\star} - x_{k-1}), \\mathcal{B}) // Get the next extreme point\n  \\theta_{k} \\gets (x^{\\star} - x_{k-1})^{T}(p_{k} - x_{k-1}) \\mathbin{/} \\lVert p_{k} - x_{k-1} \\rVert_{2}^{2} // Calculate the step size\n  x_{k} \\gets x_{k-1} + \\theta_{k}(p_{k} - x_{k-1}) // Update x\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nThe assumption of the FW algorithm is that there always exists an extreme point in the Birkhoff polytope (permutation matrix p_{k}) that is a direction (p_{k} - x_{k-1}) in which it is possible to improve the objective function.\n\nSince each extreme point of the Birkhoff polytope is an permutation matrix and thus the each step size can be considered as the weight of the permutation matrix, FW algorithm can be used to solve Birkhoff problem.\n\nThe paper selects the objective function to be f(x) = \\frac{1}{2} \\lVert x - x^{\\star} \\rVert_{2}^{2}, which measures the square of the l_{2} norm of the difference between the current matrix x_{k} and the objective matrix x^{\\star}.\n\nThe solving of \\mathrm{LP} at line 3 is essentially trying to find an extreme point p_{k} in the Birkhoff polytope by solving {\\mathrm{argmin}_{u \\in \\mathcal{P}}}\\nabla f(x_{k-1})^{T}u, in which \\nabla f(x_{k-1}) = -(x^{\\star} - x_{k-1}).\nThe way to calculate \\theta_{k} at line 4 is derived by solving f(x_{k}) - f(x_{k-1}).\nNote at line 5 x_{k} = x_{k-1} + \\theta_{k}(p_{k} - x_{k-1}) = (1 - \\theta)x_{k-1} + \\theta_{k}p_{k}, so x_{k} is still a linear combination of permutation matrices.\n\nDifference with the Birkhoff algorithm:\n\nFW uses a random extreme point instead of 0 as the starting point, since it requires x_{k} \\in \\mathcal{B} at all time.\nFW does not require that x_{k} \\leq x^{\\star} and thus x^{\\star} can be approximated from any direction.\n\n\n\nFully Corrective Frank-Wolfe (Algorithm 6)\n\nFCFW provides the best approximation with the number of extreme points selected up to iteration k\n\n\nfrom IPython.display import Image\nImage(filename='1.png')"
  },
  {
    "objectID": "Notes/Birkhoff+.html#birkhoff-algorithm-algorithm-7",
    "href": "Notes/Birkhoff+.html#birkhoff-algorithm-algorithm-7",
    "title": "Birkhoff+",
    "section": "Birkhoff+ algorithm (Algorithm 7)",
    "text": "Birkhoff+ algorithm (Algorithm 7)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1} \\theta_i) \\mathbin{/} n^2\n  p_{k} \\gets \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha)))\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nBirkhoff+ combines Birkhoff algorithm and FW algorithm. This approach wants to use the path or permutations that FW would select while remaining in the box that characterizes the Birkhoff’s approach.\nThe only real difference between Birkhoff+ and Birkhoff is the way to compute p_{k} (line 4).\nf_{\\beta}(x) = f(x) - \\beta \\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j) + \\frac{\\epsilon}{d}), which consists f(x) (same as what is used in FW) and - \\beta \\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j) + \\frac{\\epsilon}{d}) (the barrier).\n\nThe barrier is a penalty term that restricts the x_{k} to always remain in the Birkhoff box.\n\nIf we ignore \\frac{\\epsilon}{d} and \\beta, -\\sum_{j=1}^{d} \\log(x^{\\star}(j) - x(j)) \\to \\infty if any x^{\\star}(j) \\approx x(j). That is, the penalty will be very high if any of the element of x is closed to x^{\\star}.\nSince x_{0} = 0 &lt; x^{\\star} at the starting point and the barrier will prevent all elements of x getting closed to x^{\\star}, the barrier can prevent x &gt; x^{\\star}.\n\n\\beta is a hyper-parameter that balances the objective function and the barrier. In the code, it is tuned smaller and smaller as the x_{k} is getting closed to x^{\\star}.\n\\frac{\\epsilon}{d} is used to avoid \\log \\to \\infty.\n\n\\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha))) = \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}) + b_{k}, \\mathcal{B}), where b_{k} = \\frac{\\epsilon}{d} \\cdot \\mathbb{1}(x^{\\star} - x_{k-1} \\leq \\alpha) is a penalty vector to force the solver to do not select the elements of vector (x^{\\star} - x_{k}) smaller than \\alpha.\n\nFor all elements of vector (x^{\\star} - x_{k}) smaller than \\alpha, b_{k} = \\frac{\\epsilon}{d}. Otherwise, b_{k} = 0.\n\n\n\nBirkhoff+ (max_rep) algorithm (Algorithm 8)\n\n\nx_{0} = 0, k = 0\nwhile \\lVert x^{\\star} - x_{k-1} \\rVert_{2} &gt; \\epsilon and k \\leq k_{max} do\n  \\alpha \\gets (1 - \\sum_{i=1}^{k-1} \\theta_i) \\mathbin{/} n^2\n  for i = 1,\\ldots, \\mathrm{max\\_rep} do\n    p_{i} \\gets \\mathrm{LP}(\\nabla f_{\\beta}(x_{k-1}), \\mathrm{conv}(\\mathcal{I}_{k}(\\alpha)))\n    \\theta_{i} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_{k})\n    if (\\theta_{i} &gt; \\alpha) \\alpha \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_{k})\n    else exit while loop\n    p_{k} \\gets p_{i}\n  end for\n  \\theta_{k} \\gets \\mathrm{STEP}(x^{\\star}, x_{k-1}, p_k)\n  x_{k} \\gets x_{k-1} + \\theta_{k}p_{k}\n  k \\gets k + 1\nend while\nreturn (p_{1}, \\ldots, p_{k-1}), (\\theta_{1}, \\ldots, \\theta_{k-1})\n\n\n\nInstead of using a constant \\alpha, Birkhoff+ (max_rep) searches the largest \\alpha in each iteration by alternatively updating \\alpha and p_{k} until \\alpha cannot be updated larger or \\mathrm{max\\_rep} iterations is reached.\n\nLarger \\alpha \\to larger \\theta \\to less number of permutation matrices.\n\\alpha is set to be the largest step size (weight from \\mathrm{STEP} function) found so far.\n\n\n\nfrom IPython.display import Image\nImage(filename='2.png') \n\n\n\n\n\n\nSource code with comment\nhttps://github.com/vvalls/BirkhoffDecomposition.jl\n\nusing JuMP\nusing Clp\nusing Random\nusing SparseArrays\n\n\n# functionsBD.jl\n\nstruct polytope\n    A;\n    b;\n    l;\n    u;\n    model;\n    x;\nend\n\n@doc raw\"\"\"\nSolve a linear programing problem\n\"\"\"\nfunction LP(c, P)\n    @objective(P.model, Min, c'* P.x)\n    optimize!(P.model)\n    return value.(P.x)\nend\n\n@doc raw\"\"\"\nGet a random stochastic matrix\n\"\"\"\nfunction randomDoublyStochasticMatrix(n; num_perm=n^2)\n    M = zeros(n, n);\n    α = rand(num_perm)\n\n    α = α / sum(α);\n\n    for i = 1:num_perm\n        perm = randperm(n);\n        for j = 1:n\n            M[perm[j], j] += α[i];\n        end\n    end\n\n    return M;\nend\n\n\n@doc raw\"\"\"\nCreate Birkhoff polytope ``\\mathcal{B}`` (Section V-A), which contains all possible doublely stochastic matrices.\n\nSince the paper assumes that the solutions by solving linear programs over are Birkhoff polytope extreme points, \nthe solutions are permutation matrices (which are also doublely stochastic). \n\"\"\"\nfunction birkhoffPolytope(n)\n    # x is a doublely stochastic matrix that is represented by a vector (flattened).\n    # Use a constant matrix A(M') and a constant vector b to specify that x is doublely stochastic.\n    \n    M = zeros(n*n, 2*n);\n    # Specify the sum of each row of x equals to 1\n    for i = 1:n\n        M[(i-1)*n*n + (i-1)*n + 1 : (i-1)*n*n + (i-1)*n + n] = ones(n,1);\n    end\n    # Specify the sum of each col of x equals to 1\n    for i = 1:n\n        for j=1:n\n            M[n*n*n + (i-1)*n*n + (j-1)*n + i] = 1;\n        end\n    end\n\n    A = sparse(M');\n    b = ones(2*n);\n\n    model = Model(Clp.Optimizer)\n    set_silent(model)\n    @variable(model, 0 &lt;= x[1:n*n] &lt;= 1)\n    @constraint(model, A * x .== b)\n\n    return polytope(A, b, 0, 1, model, x)\n\nend\n\nbirkhoffPolytope\n\n\n\n# stepsizes.jl\n\n@doc raw\"\"\"\nGet the step size (weight) by taking the minimum non-zero entry of the difference matrix \n(masked by the permutation matrix y) between x_star and x.\n\"\"\"\nfunction getBirkhoffStepSize(x_star, x, y)\n    return minimum((x_star - x).*y - (y.-1));\nend\n\ngetBirkhoffStepSize\n\n\n\n# extremepoints.jl\n\n@doc raw\"\"\"\nBirkhoff+ (max_rep) Psudocode line 4-10\n\"\"\"\nfunction getEPBplus(x_star, x, B, max_rep, ε)\n    n = sqrt(size(x_star, 1))\n    d = size(x_star, 1);\n    i = 1;\n    y = 0;\n    α = 0;\n\n    while(i &lt;= max_rep)\n        # Calculate \\beta for this iteration.\n        # \\beta should become smaller and smaller.\n        z = Int16.(x_star - x .&gt; ε)\n        s = getBirkhoffStepSize(x_star, x, z)\n        beta = (s + ε/d)*0.5\n\n        # c is the gradient of the objective function with barrier.\n        # b is an iterm added to make I_k(\\alpha) to be B (Birkhoff polytope).\n        # See the paragraph in section VI.B after Corollary 2 for b.\n        c = -ones(d) + beta ./ (x_star - x .+ ε/d)\n        b = (n/ε).*Int16.(x_star - x .&lt;= α)\n        y_iter = LP(c + b, B);\n        \n        # If new solution (y_iter) makes objective function larger/worse (c'*y_iter &gt; c'*y_z),\n        # fall back to the solution from the last iteration (y_z/x).\n        y_z = x;\n        if(c'*y_iter &gt; c'*y_z)\n            y_iter = y_z\n        end\n\n        # \\alpha should be the largest step size found. \n        # Algorithm terminates when \\alpha doesn't increase\n        α_iter = getBirkhoffStepSize(x_star, x, y_iter);\n        if(α &lt; α_iter)\n            α = α_iter;\n            y = y_iter;\n        else\n            return y;\n        end\n        i = i + 1;\n    end\n\n    return y\nend\n\ngetEPBplus (generic function with 1 method)\n\n\n\n# birkdecomp.jl\n\n@doc raw\"\"\"\nBirkhoff+ (max_rep)\n\"\"\"\nfunction birkdecomp(X, ε=1e-12; max_rep=1)\n    n = size(X, 1);                                 # get size of Birkhoff polytope\n    x_star = reshape(X, n*n);                       # reshape doubly stochastic to vector\n    B = birkhoffPolytope(n);                        # Birkhoff polytope\n    ε = max(ε, 1e-15);                              # fix the maximum minimum precision\n    max_iter = (n-1)^2 + 1;\n\n    x = zeros(n*n);                                 # initial point\n\n    extreme_points = zeros(n*n, max_iter);          # extreme points (permutation) matrix\n    θ = zeros(max_iter);                            # weights vector\n    approx = Inf;                                   # approximation error\n    i = 1;                                          # iteration index\n\n    while(approx &gt; ε)\n        # Get next extreme point \n        y = getEPBplus(x_star, x, B, max_rep, ε)\n        # Get next weight (step size)\n        θi = getBirkhoffStepSize(x_star, x, y)\n        # Update x_k\n        x = x + θi*y;\n        # Store the new weight\n        θ[i] = θi;\n\n        # Update the Frobenius norm\n        approx = sqrt(sum((abs.(x_star-x)).^2));\n        \n        # Store the new extreme point matrix\n        extreme_points[:,i] = y;\n        i = i + 1;\n    end\n\n    return extreme_points[:, 1:i-1], θ[1:i-1]\n\nend\n\nbirkdecomp (generic function with 2 methods)\n\n\n\n# Generate a random doubly stochastic matrix (n is the dimension)\nn = 3;\nx = randomDoublyStochasticMatrix(n);\nP, w = birkdecomp(x);\n\ndisplay(x)\ndisplay(P);\ndisplay(w);\n\n3×3 Array{Float64,2}:\n 0.0607488  0.590595   0.348656\n 0.70177    0.0291194  0.269111\n 0.237482   0.380286   0.382233\n\n\n9×5 Array{Float64,2}:\n 0.0  0.0  0.0  1.0  0.0\n 1.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  1.0\n 1.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0\n 0.0  1.0  0.0  1.0  0.0\n 0.0  1.0  0.0  0.0  1.0\n 0.0  0.0  1.0  1.0  0.0\n 1.0  0.0  0.0  0.0  0.0\n\n\n5-element Array{Float64,1}:\n 0.38223288955133716\n 0.31953666864401353\n 0.20836217653438616\n 0.06074883235172196\n 0.029119432918541188\n\n\n\nx_new = zeros(n, n)\nfor i=1:length(w)\n    x_new = x_new + w[i] * reshape(P[:, i], n, n) \nend\ndisplay(x_new)\n\n3×3 Array{Float64,2}:\n 0.0607488  0.590595   0.348656\n 0.70177    0.0291194  0.269111\n 0.237482   0.380286   0.382233"
  },
  {
    "objectID": "Notes/MLIC IMLI.html",
    "href": "Notes/MLIC IMLI.html",
    "title": "MLIC IMLI",
    "section": "",
    "text": "04-11-2022\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/MLIC IMLI.html#satisfiability-problem-sat",
    "href": "Notes/MLIC IMLI.html#satisfiability-problem-sat",
    "title": "MLIC IMLI",
    "section": "Satisfiability problem (SAT)",
    "text": "Satisfiability problem (SAT)\nLet \\{C_{1}, \\dots, C_{m}\\} be a set of Boolean clauses on variables x_{1}, \\dots, x_{n} where each clause is a disjunction of literals, each literal being a Boolean variable or its negation. SAT is the problem of finding an assignment of the boolean variables that makes all clauses true\n\nMaximum satisfiability problem (MaxSAT)\nLet there be a nonnegative weight W(C) = w_{c} associated with each clause. MaxSAT is the problem of finding an assignment of the boolean variables that maximizes the total weight of the satisfied clauses.\n\n\nPartial MaxSAT problem\nLet \\{C_{1}, \\dots, C_{m}\\} consist of soft clauses and hard clauses. Let there be a nonnegative weight W(C) = w_{c} associated with each soft clause. Partial MaxSAT is the problem of finding an assignment to the boolean variables that makes all hard clauses true and maximizes the total weight of the satisfied soft clauses."
  },
  {
    "objectID": "Notes/MLIC IMLI.html#problem-formulation",
    "href": "Notes/MLIC IMLI.html#problem-formulation",
    "title": "MLIC IMLI",
    "section": "Problem formulation",
    "text": "Problem formulation\nInputs:\n\nBinary matrix \\mathbf{X} \\in \\mathbb{R}^{n \\times m} of n instances with m binary features.\nBinary vector \\mathbf{y} \\in \\mathbb{R}^{n} of n binary labels.\nInteger k indicating the number of clauses in the CNF rule\nRegularization parameter \\lambda.\n\nOutputs:\n\na CNF rule.\n\nVariables:\n\nk \\times m binary matrix \\mathbf{B} of the variables \\{b_{1, 1}, b_{1, 2}, \\dots, b_{1, m}, \\dots, b_{k, m}\\} that represents the result CNF rule.\n\n\\mathbf{B}_{l, j} = 1 means that the feature f_{j} appears in clause C_{i} of the CNF rule.\n\\mathbf{B}_{l} means the lth row of the matrix \\mathbf{B}, which is also the lth clause C_{l}.\n\nn binary noise variables \\{\\eta_{1}, \\dots, \\eta_{n}\\} that indicates the instances that can be treated as noise.\n\nIf \\eta_{i} = 1, the results CNF rule doesn’t have to classifies the \\mathbf{x}_{i} correctly.\n\n\nPartial MaxSAT formulation:\n\nCNF constraint\n Q = \\bigwedge_{i=1}^{n} N_{i} \\wedge \\bigwedge_{i=1, j=1}^{i=k, j=m} V_{i, j} \\wedge \\bigwedge_{i=1}^{n} D_{i}  \nWe want the training accuracy to be higher. Each non-noise will have a \\lambda weight. (soft clauses)\n N_{i} = \\neg \\eta_{i} \\quad W(N_{i}) = \\lambda \nWe want the CNF rule to be sparse. Each don’t care literal will have a 1 weight. (soft clauses)\n V_{l, j} = \\neg \\mathbf{B}_{l, j} \\quad W(V_{l, j}) = 1 \nWe want each non-noise instance to be correctly classified by the CNF rule. (hard clauses)\n D_{i} = \\left( \\neg \\eta_{q} \\rightarrow \\left( y_{i} \\leftrightarrow \\bigwedge_{l = 1}^{k} \\left( \\bigvee_{j=1}^{m} \\mathbf{X}_{i, j} \\wedge \\mathbf{B}_{l, j} \\right) \\right) \\right) \\quad W(D_{i}) = \\infty"
  },
  {
    "objectID": "Notes/Quantization Survey.html",
    "href": "Notes/Quantization Survey.html",
    "title": "Quantization Survey",
    "section": "",
    "text": "This page contains my reading notes on"
  },
  {
    "objectID": "Notes/Quantization Survey.html#problem",
    "href": "Notes/Quantization Survey.html#problem",
    "title": "Quantization Survey",
    "section": "Problem:",
    "text": "Problem:\nGiven a full precision number x, which is either a weight or an activation in the network, we want to only use 2^{k} number of distinct values \\hat{x} to replace x in the inference time. 1. k here is called the bit-width. 1. The goal is to reduce the inference time and memory usage because less number of distinct values uses less memory and benefit from integer arithmetic hardware. 1. Unlike the application of quantization method used in signal processing, whose primary goal is to minimze the difference between the quantized values and the full-precision values, the quantization in neural network aims to minimize the accuracy drop, which can be achieved even if the average difference is huge. 1."
  },
  {
    "objectID": "Notes/Quantization Survey.html#uniform-quantization",
    "href": "Notes/Quantization Survey.html#uniform-quantization",
    "title": "Quantization Survey",
    "section": "Uniform Quantization",
    "text": "Uniform Quantization\n\nUniform quantization means that the values after the quantization are equally spaced: \\hat{x}_{n} - \\hat{x}_{n - 1} = \\hat{x}_{n - 1} - \\hat{x}_{n - 2}\nA widely used method of uniform quantization is as follows:\n\nThe quantization operator maps the real values to a set of consecutive integers in the range of [-2^{k-1}, 2^{k-1} - 1]:  Q(x) = \\mathrm{round}(\\frac{x - b}{s})  Here s is the scaling factor, b is the bias and \\mathrm{round}() is to round the float to nearest integer.\nBoth s and b can be directly calculated if we have selected a range of x to be [\\alpha, \\beta]:  s = \\frac{\\beta - \\alpha}{2^{k} - 1}   b = \\frac{\\beta - \\alpha}{2}  The scaling factor essentially divide the range (\\alpha, \\beta) into 2^{k} numbers of same size partitions. The bias shifts the selected range to be zero centered.\nFinally, the quantized value \\hat{x} that should be used in the inference can be mapped from Q(x):  \\hat{x} = sQ(x) + b \n\nSymmetric and asymmetric quantization\n\nIf the selected range [\\alpha, \\beta] is symmetric around 0 i.e. \\alpha = -\\beta, then the quantization is called symmertric. Otherwise, it is called asymmertric.\nSymmetric quantization doesn’t require b (b=0) since the selected range is already zero centered. However, it can cause unused/over-used quantized value if the x is not symmertric."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html",
    "href": "Notes/SGD Warm Restarts.html",
    "title": "SGD Warm Restarts",
    "section": "",
    "text": "08-02-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#why-it-is-needed",
    "href": "Notes/SGD Warm Restarts.html#why-it-is-needed",
    "title": "SGD Warm Restarts",
    "section": "Why it is needed",
    "text": "Why it is needed\nThis paper proposes a very simple yet quite effective learning rate scheduling technique. It alternates between a cosine annealing (gradually decreasing with a cosine form) phase and warm restarts step (rapidly increase to a high value) of the learning rate.\n\nFor the SGD with momentum, which is a more traditional gradient optimization algorithm, the only hyper-parameter is the learning rate.  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\lambda is the learning rate, \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t, \\mu is the momentum parameter that is typically 0.9, and v_{t} is the correct accumulated gradient direction at time t.\nWe usually always want to decrease the learning rate as the training time increases because we can quickly approach the target quickly at first with large learning rate and then slow down to take small steps around the local minima so that we don’t overshoot.\n\nInstead of using traditional step-wise or linearly decreasing, SGDR uses a wave form that is closed to cosine function.\n\nAnother paper is the first to propose that it is beneficial to periodically decrease and increase the learning rate for neural network training. It explains the intuition and imperatively demonstrate its effectiveness. The intuition is that the model will usually be stuck in the saddle point instead of the local minima and using the high learning rate at the proper time will help model jump out of the saddle point and traverse through the saddle point quickly.\n\nInstead of gradually increasing the learning rate, SGDR “restarts” the learning rate by directly setting it to a high value at some epochs."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#cosine-annealing",
    "href": "Notes/SGD Warm Restarts.html#cosine-annealing",
    "title": "SGD Warm Restarts",
    "section": "Cosine annealing",
    "text": "Cosine annealing\nAt given epoch t, the learning rate l is calculated as follows:  l = l_{\\mathrm{min}} + \\frac{1}{2}(l_{\\mathrm{max}} - l_{\\mathrm{min}})(1 + \\cos(\\frac{T_{\\mathrm{cur}}}{T}\\pi))  where T_{\\mathrm{cur}} is how many epochs have been performed since the last restart, l_{\\mathrm{min}} is the min learning rate, l_{\\mathrm{max}} is the max learning rate, and T defines how many epochs is one period (how many epochs to restart)."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#warm-restart",
    "href": "Notes/SGD Warm Restarts.html#warm-restart",
    "title": "SGD Warm Restarts",
    "section": "Warm restart",
    "text": "Warm restart\nWhen T_{\\mathrm{cur}} = 0, l = l_{\\mathrm{max}} and when T_{\\mathrm{cur}} = T, l = l_{\\mathrm{min}}. Whenever T_{\\mathrm{cur}} = T, we set the l directly to l_{\\mathrm{max}}."
  },
  {
    "objectID": "Notes/SGD Warm Restarts.html#notes",
    "href": "Notes/SGD Warm Restarts.html#notes",
    "title": "SGD Warm Restarts",
    "section": "Notes",
    "text": "Notes\n\nl_{\\mathrm{min}}, l_{\\mathrm{max}} and T are hyper-parameters. Typically l_{\\mathrm{min}} is set to 0 and l_{\\mathrm{max}} is set to the initial learning rate.\nThe figure below shows how the function looks like if we set l_{\\mathrm{max}} = 1, l_{\\mathrm{min}} = 0, and T = 1, that is, we gradually decrease the learning rate from 1 to 0 in the period of 1 epoch.\nT_{\\mathrm{cur}} can also be a fraction number that represents the number of batches in the epoch. If an epoch has 10 batches, T_{\\mathrm{cur}} is updated 0.1 after each batch.\nInstead of using the fixed period T, the authors suggested to define another hyper-parameter T_{\\mathrm{mult}} to make T increase after each restart.  T_{n+1} = T_{n} \\times T_{\\mathrm{mult}}  where n is the number of restarts.\nl_{\\mathrm{min}} and l_{\\mathrm{max}} can also be changed in each restart, but the authors suggest not to change it to reduce the number of hyper-parameters involved.\n\n\nfrom IPython.display import IFrame\nIFrame('https://www.desmos.com/calculator/9hrbpo2ajf?embed', width=500, height=500)"
  },
  {
    "objectID": "Notes/ML Q&A.html",
    "href": "Notes/ML Q&A.html",
    "title": "ML Q & A",
    "section": "",
    "text": "We always want the model to have low bias and low variance at the same time, but it is difficult.\nBias is the average difference between the predictions and the label.\nVariance refers to the sensitivity of our model to the fluctuations (noise) in the training set. OR variance describes how much a random variable of your model differs from its expected value.\nSince we only have training set that is a sample from the overall distribution, training our model to reduce bias will generally increase variance at the same time because we don’t know what is the noise.\nHigh bias and low variance lead to under-fitting, where your model is too simple to capture the regularities in your data. This will result in both low training accuracy and testing accuracy.\nLow bias and high variance lead to over-fitting, where your model is too complex that it captures all the patterns in your training data including noise. It will result in high training accuracy, but low testing accuracy.\n\n\n\n\n\nUnder-fitting is when your model is too simple to fit the training set correctly. This will result in low training accuracy and low testing accuracy, assuming that our training set is a reasonable sample of the overall distribution.\nTo combat under-fitting, the first we can do is to choose the model with more complexity. If decision tree under-performs on a dataset, we can try random forest. Second we can increase the parameters that our model have. Like increasing number of trees or the maximum depth of the decision tree. Last, for some models that uses iterative learning procedure such as neural network, we can increase the training time.\nOver-fitting is when your model is too complex such it can fit the training set very accurately. This means that it also fit the noise correctly and will result in high training accuracy and low testing accuracy.\nTo combat over-fitting, we first can reduce the number of parameters. Second, using ensemble of your current models is proven to reduce over-fitting. Third, without changing models, we can do data augmentation such as over-sampling more data to make the training better represent the actual distribution.\n\n\n\n\n\nSupervised learning task is usually provided with labeled dataset and goal is to predict the correct label for a given instance.\nUnsupervised learning only gives the instances themselves, without their labels. The goal is to learn patterns from those unlabeled data.\nSupervised learning include classification and regression. Classification is to predict a categorical label while regression is to predict a continuous value.\nUnsupervised learning include clustering and dimension reduction. Clustering is to group similar instances together and dimension reduction is to select the important features from all features.\n\n\n\n\n\nDimensionality refers to the number of features in your dataset.\nIt is harder for the models to search through a space as the number of features grows. The required number of training instances to achieve the same accuracy grows exponentially with the number of features. Since in practice the number of training instances are fixed, the performance of the models will typically decrease as the dimension increases.\nWe can use feature selection such as manual feature selection by human or feature extraction technique like PCA to reduce the dimensionality. The difference between selection and extraction is that selection selected subset of the original features and extraction extracts a set of new features from data.\n\n\n\n\n\nThe confusion matrix is used to evaluate the performance of an supervised classifier on a dataset. It has N rows and N columns where N is the number of classes in the dataset.\nEach row of the matrix gives the number of predictions from the classifier for a specific label and each column of the matrix gives the number of actual labels. If the classifier is for a binary classification dataset, the first row gives the number of true positives and the number of false positives and the second row will give false negatives and true negatives.\nOther performance evaluation methods such as accuracy or F1-score will be misleading on unbalanced dataset. For example, if we have a dataset that has 95 dogs and 5 cats and a classifier than always predict dog. We have 95% accuracy and 97.5% F1-score, which tells that our classifier is very good. Confusion matrix will give a whole picture of metrics that include true positive, true negative, false positive and false negative. If we add up true positive and false positive in the matrix, we can see that our classifier only predict positive labels.\n\nNotes: 1. For a binary classification problem, we can have several terms: 1. True positive: classifier gives positive class and the prediction is correct. 2. False positive: classifier gives positive class and the prediction is incorrect. 3. True negative: classifier gives negative class and the prediction is correct. 4. False negative: classifier gives negative class and the prediction is incorrect. 2. For a binary classification problem, we have several performance measures: 1. Sensitivity (true positive rate): number of positive predictions correctly labeled / number of instances with positive labels.  \\mathrm{\\frac{TP}{TP + FN}}  False positive rate: number of positive predictions incorrectly labeled / number of instances with negative labels.  \\mathrm{\\frac{FP}{FP + TN}}  2. Specificity (true negative rate): number of negative predictions correctly labeled / number of instances with negative labels.  \\mathrm{\\frac{TN}{TN + FP}}  3. Precision: number of positive instances correctly predicted / number of positive predictions.  \\mathrm{\\frac{TP}{TP + FP}}  4. Recall: same as sensitivity.  \\mathrm{\\frac{TP}{TP + FN}}  5. F1-score: harmonic mean of the precision and recall.  \\mathrm{2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}}\n\n\n\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc 1. An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The x axis is false positive rate and y axis is true positive rate. 2. An ROC curve plots TPR vs. FPR at different classification thresholds. For all models that first produce a score and then thresholded to give the classification, different thresholds mean different number of positive and negative predictions. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives and the curve will always go higher. 3. A perfect model will have a straight horizontal line at y = 1 while a perfectly wrong model will have a horizontal line at y = 0. A random guessing model will have a diagonal line, which means that the model has no class separation capacity. 4. AOC stands for area under the curve and measures the area under the ROC curve. It is used to measure a model’s performance across all possible classification thresholds.\n\n\n\n\nCalculate moving average of a set of data points is creating a series of averages of different subsets of the dataset. The dataset to be analyzed usually contains time series data (data that is indexed by timestamps). Exponential moving average (EMA) is one type of moving average algorithm.\nThe EMA value at timestamp t calculates the average of the data from the beginning to timestamp t, which is calculated by adding up the weighted value of the data at timestamp t and the weighted EMA value on the previous timestamp t-1.\nThe weighted part is called the smoothing factor and is a hyper-parameter that is between 0 and 1. Higher smoothing factor means the current data value is weighted more and the previous EMA value is weighted less in the calculation of the new EMA value.\nThis techniques is usually used by gradient optimizer to calculate new learning rate.\n\nNotes: 1. [EMA equation]: If the current timestamp is t, then the equation for EMA is:  y_{t} = \\alpha x_{t} + (1 - \\alpha)y_{t-1}  where y_{t} is the moving average at timestamp t, x_{t} is the data point at timestamp t, and \\alpha is the smoothing factor, which is the range [0, 1].\n\n\n\nTODO\n\n\n\n\n\n\n\nGradient descent is an optimization algorithm that can iteratively minimize the target function to find its local minimum. If the target function is convex, then gradient descent can also find its global minimum.\nFirst we calculate the derivatives of the target function with respect to the parameters of the model. This is the gradient.\nSecond we update the parameters to the opposite direction of the gradients to minimize the value of the target function. This is the descent.\nGradient descent will minimize the target function, while gradient ascent, which updates the parameters to the same direction of the gradients, will maximize the target function.\n\n\n\n\n\nTODO\n\n\n\n\nhttps://ruder.io/optimizing-gradient-descent/index.html#fn4 https://cs231n.github.io/neural-networks-3/#update 1. Normal gradient descent is batch gradient descent. One batch means one complete run of the training set. Thus, we need to evaluate all instances of the training set before one gradient update. Gradient descent is more slow, but guaranteed to converge to the local minimum. 2. Stochastic gradient descent means that one gradient update is performed for each instance evaluated. This approach converges faster and can be used on the fly as the new instance comes in, but can cause target function to fluctuate. 3. Mini-batch is the combination of the two above. Instead of whole batch or single instance, we take subset of training set as the mini-batch and evaluate them to get gradient for a single gradient update. This combines the benefits of two method above.\nNotes: 1. [Different gradient descent optimization algorithms] 1. SGD  \\theta_{t+1} = \\theta_{t} - \\lambda \\partial_{t}(\\theta)  where \\lambda is the learning rate and \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t. 2. SGD with Momentum: It will help the convergence speed of SGD because it reduces the oscillations of the SGD near the local minimas, which is done by building up the velocity in the correct direction that has consistent gradients  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\mu is the momentum parameter that is typically 0.9 and v_{t} is the correct accumulated gradient direction at time t. 3. Adagrad: It will make the learning rates of the weights that receive high gradients reduced, and the learning rates of the weights that receive small or infrequent updates increased. Thus we don’t have to manually tune the learning rates in the training progress. However, since there is no way to reduce the accumulated squared gradients in the denominator of the learning rate, the monotonically decreasing learning rate in the training process will eventually stop the training.  g_{t} = g_{t-1} + \\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{g_{t} + \\epsilon}} \\partial_{t}(\\theta)  where g_{t} is the accumulation of the squared gradient for \\theta until time t and \\epsilon is a small value used to prevent the division by 0. 4. RMSprop: RMSprop improves on Adagrad by replacing the accumulation of the past squared gradients with the exponential moving average of the past squared gradients. This can solve the issue of Adagrad that the learning rates are monotonically decreasing.  e_{t} = \\beta e_{t-1} + (1-\\beta)\\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{e_{t} + \\epsilon}} \\partial_{t}(\\theta) where e_{t} is the exponential moving average of the squared gradient for \\theta until time t and \\beta is like momentum that controls degree of weighting decay and is usually set to 0.9. 5. Adam: Adam improves on RMSprop by replacing the raw gradient with the exponential moving average of the past gradients in the update step. It thus combines the benefits of RMSprop and SGD with momentum.  v_{t} = \\beta_{1} v_{t-1} + (1-\\beta_{1})\\partial_{t}(\\theta)   e_{t} = \\beta_{2} e_{t-1} + (1-\\beta_{2})\\partial_{t}^{2}(\\theta)   \\hat{v}_{t} = \\frac{v_{t}}{1-\\beta_{1}^{t}}   \\hat{e}_{t} = \\frac{e_{t}}{1-\\beta_{2}^{t}}   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda \\hat{v}_{t}}{\\sqrt{\\hat{e}_{t} + \\epsilon}}  where \\hat{v}_{t} and \\hat{e}_{t} are the bias corrected versions of v_{t} and e_{t} and \\beta^{t} means the \\beta to the power of t.\n\n\n\n\nVanishing gradient happens to the parameters in the earlier layers of a deep neural network where the gradients are so small in the back-propagation process that the weights are not really changed.\nThe primary reason for this problem is the choice of activation functions such as sigmoid or hyperbolic tangent function, whose gradients are very small and are always much less than 1. If multiple layers with such activations are stacked together, the gradients to the earlier layers of the networks are multiplied lots of times with the loss gradient in the back-propagation process. Each layer reduce the original loss gradient by a fraction and in the end the gradient to the earlier layers are very small. Therefore, the large number of layers are also an important reason for vanishing gradient problem.\nOne effective solution to the problem is to use other activation functions such as ReLU.\n\n\n\n\n\nIt can help solve the vanishing gradient problem. The gradient of the ReLU for inputs larger than 0 is 1 and thus the gradient won’t be reduced in the back-propagation process.\nReLU is computationally more efficient because it only needs to cut the negative input to 0.\nHistorically speaking, ReLU is just good enough for neural network to be trained stably.\n\n\n\n\n\nL_1 regularization is also called Lasso regularization. It adds the sum of the absolute value of all weights in the neural network as a penalty term to the loss function.\nL_2 regularization is also called Ridge regularization. It adds the sum of the squared value of all weights in the neural network as a penalty term to the loss function.\nThey both are used to reduce over-fitting issue of the large neural network.\nThe key difference is the gradient of each penalty. The gradient of Lasso is a 1 or -1 depending on the sign of each weight, while the gradient of Ridge is 2 times the value of the parameter. The weights with Lasso can possibly shrink to exactly 0, while weights with Ridge can only shrink to a very small value instead of exact 0 because the gradients also decrease as the weights decrease. Thus Lasso can be used to train sparse neural network.\n\n\n\n\n\nDropout is to randomly drop neurons of the network in the training process to avoid over-fitting. A neuron is dropped means that the data and the gradient don’t go through that neuron in both forward or backward process.\nTypically dropout is applied per layer and we can set a probability p for each layer to indicate what percentage of neurons we want to drop in that layer. p is usually selected for 0.5 for hidden layer, but a less value for input layer like 0.1 because randomly dropping an entire column of input data is very risky.\nThe dropout neurons are changed every instance or mini-batch depending on what type of gradient descent we are using. We only apply dropout in the training process and we will use all neurons for testing to have consistent output.\n\n\n\n\n\nBatch normalization is to normalize the inputs to each layer for each mini-batch. Batch normalization is proved to help neural network training converges faster.\nBatch normalization first normalize the inputs to each layer by subtracting the mini-batch mean from each value and then divide it by the mini-batch standard deviation. This process will make each input value to be in the range between 0 to 1. Then we need to scale and shift the normalized value into a desirable range. The coefficients for scaling and shifting are also two parameters that are needed to be learned in the backward propagation.\nThe challenge that batch normalization is trying to solve is called internal covariate shift. Since each layer’s output is fed into next layer’s input, the change of the weights in the first layer due to backward process after a mini-batch will cause the change of its output distribution. The internal covariate shift slows down the training process because the learning in the next iteration needs to accommodate for this change.\n\nNotes: 1. [Batch Normalization Layer]: Batch normalization layer can be appended before each layer and outputs the same dimension as the inputs 1. Get the mean \\mu_{B} and standard deviation \\sigma_{B} of the inputs in a mini-batch:  \\mu_{B}=\\frac{1}{m}\\sum_{i=1}^{m}x_i   \\sigma_{B}=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_{B})^{2}}  2. Normalize the input:  \\hat{x}_i=\\frac{x_i-\\mu_{B}}{\\sigma_{B}}  3. Scale and shift back, where \\gamma is the scaling parameter and \\beta is the shifting parameter:  y_{i} = \\gamma\\hat{x}_{i}+\\beta \n\n\n\nhttps://www.deeplearning.ai/ai-notes/initialization/index.html 1. Xavier initialization is a specific way of initialize the weights and bias in the neural network such that the variance of the activations are relatively the same across all layers. 2. If we use Xavier initialization method, the bias will be initialized to 0 and the weights are randomly sampled from a normal distribution that has mean of 0 and variance of 1 over the the number of neurons in the last layer. 3. If we initialize the weights to have the same value, all neurons will have the same activations. Same activations mean that all neurons will have the same gradients and will evolve the same throughout the training. 4. If we initialize the weights to be too small or too large, the activations of each layer in the first several iterations will also be very small or large. Since gradients of weights for each layer are calculated based on the activations, large weights will result in gradient exploding and small weights result in gradient vanishing, both of which prevent the neural from efficiently learning.\n\n\n\n\nhttps://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning#clustering\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html 1. K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed. 2. We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change. 3. K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.\nNotes 1. [K-means algorithm]: 1. Initialize cluster centroids \\mu_{1}, ..., \\mu_{k} randomly. 2. Repeat until convergence: 1. Get the centroid for each instance x_{i}:  c_{i} = \\underset{\\mu_{i}}{\\operatorname{argmin}} \\lVert x_{i}-\\mu_{i} \\rVert^{2}  2. Update the centroid based on the instances:  \\mu_{j} = \\frac{\\sum_{i}^{m}1_{\\{c_i=j\\}}x_{i}}{\\sum_{i}^{m}1_{\\{c_i=j\\}}} \n\n\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c 1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. 2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.\nNotes: 1. [Eigenvectors, Eigenvalues]: Given a matrix A\\in\\mathbb{R}^{n\\times n}, \\lambda is said to be an eigenvalue of A if there exists a eigenvector z\\in\\mathbb{R}^n \\neq 0, such that:  Az = \\lambda z  2. [Eigendecomposition (spectral decomposition]: Let M be a real symmetric d \\times d matrix with eigenvalues \\lambda_{1}, ... , \\lambda_{d} and corresponding orthonormal eigenvectors u_{1}, ..., u_{d}. Then:  M = Q \\Lambda Q^T  where \\Lambda is a diagonal matrix with \\lambda_{1}, ... , \\lambda_{d} in diagonal and 0 elsewhere and Q matrix has u_{1}, ..., u{d} vectors as columns.\n\n\n\n\n\n\n\n[Bayes’ theorem]: the conditional possibility of event A given the event B is true P(A|B) can be computed as:  P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}  which in the Bayesian term is written as:  \\mathrm{Posterior} = \\frac{\\mathrm{Likelihood} \\cdot \\mathrm{Prior}}{\\mathrm{Evidence}}  If we think A as a label and B as a set of features:\n\nP(A|B) is the posterior probability of a label given a set of features.\nP(B|A) is the likelihood which is the probability of a set of features given a label.\nP(A) is the prior probability of a label.\nP(B) is the evidence probaility of a set of features.\n\n[Naive Bayes]: Naive Bayes is a classifier that selects the label \\hat{y} from all possible labels y \\in Y that has maximum conditional possibility given the instance \\mathbf{x}.  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} P(y|\\mathbf{x})  Applying Bayes’ theorem, we have:  P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\cdot P(y)}{P(\\mathbf{x})}  Since P(\\mathbf{x}) is a constant and is independent from P(y_{i}), we can simply drop it:  P(y|\\mathbf{x}) \\propto P(\\mathbf{x}|y) \\cdot P(y)  If we assume that each feature is independent from each other (naive conditional independence assumption), the possibility that the features values are all in \\mathbf{x} is the product of their possibilities:  P(\\mathbf{x}|y) = \\prod_{i}^{m}P(x_{i}|y)  Put them together, we have:  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} \\prod_{i}^{m}P(x_{i}|y) \\cdot P(y) \n\n\n\n\n\nhttps://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html 1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. 2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. 3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model’s output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction’s error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. 4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression. 5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels.\nNotes: 1. [Mean squared Error (MSE)]:\n \\mathrm{MSE} = \\frac{1}{n}\\sum_{i}^{n}(y_{i}-\\hat{y}_{i})^2  where y_{i} is the actual label and \\hat{y}_{i} is the prediction given by the classifier. 1. [Binary cross entropy (BCE)]: only works if the labels y_{i} are 0 or 1 and the values of predictions \\hat{y}_{i} are between 0 and 1.  \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y})))  which can be decomposed to two cases for each prediction and label pair:  -\\log(\\hat{y}_{i}) \\;\\mathrm{if}\\; y=1   -\\log(1 - \\hat{y}_{i}) \\;\\mathrm{if}\\; y=0  1. [Sigmoid (logistic) function and logic function]:  \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}   \\mathrm{logic}(x) = \\log(\\frac{x}{1 - x}) The inverse of sigmoid function is the logic function: \n    \\begin{alignat}{2}\n    x &= \\frac{1}{1 + e^{-y}} \\\\\n    \\frac{1}{x} &= 1 + e^{-y} \\\\\n    e^{-y} &= \\frac{1 - x}{x} \\\\\n    e^{y} &= \\frac{x}{1 - x} \\\\\n    y &= \\log(\\frac{x}{1 - x}) \\\\\n    \\end{alignat}\n     1. [How to solve the parameters for linear regression and logistic regression] 1. Linear regression can be solved mathematically by setting partial derivative of loss w.r.t each weight to 0: (bias is removed for simplification)  \\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{N} \\sum_{i}^{N} x_{i,k} \\bigg(\\sum_{j}^{D}w_{j}x_{i,j} - \\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} - x_{i, k}\\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}   = \\frac{2}{N} \\sum_{j}^{D} w_{j} \\bigg( \\sum_{i}^{N} x_{i,j}x_{i,k} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}  1. Gradient descent can be applied to solve both linear regression and logistic regression. Logistic regression doesn’t have a closed-form solution because of the non-linearity that the sigmoid function imposes.\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/extend-lm.html 1. Generalized linear models build on linear regression models to predict a non-Gaussian distribution. It keeps the weighted sum of the features of the linear regression, but connect the weighted sum and the expected mean of the output distribution through a possibly nonlinear function. 2. For example, the logistic regression is a type of the generalized linear models and it assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logic function. 3. Generalized additive models further relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature. It allows to model the potentially non-linear relations between the features and the output.\nNotes: 1. [Assumptions of linear regression]: 1. The input features are independent from each other (no interactions between the features). 2. The output distribution y given the input features X follows a Gaussian distribution. This follows the following theorem: &gt; Let X_1, ..., X_n be n mutually independent normal random variables, having means \\mu_1, ..., \\mu_n and variances \\sigma_1^2, ... \\sigma_n^2. If the random variable Y is a linear combinations of the X with w_1, ..., w_n coefficients: Y=\\sum_{i=1}^{n}w_iX_i, then Y is a Gaussian distribution with the mean \\mathrm{E}[Y] = \\sum_{i=1}^{n}b_i\\mu_i and variance \\mathrm{Var}[Y] = \\sum_{i=1}^{n}b_i^2\\sigma_i^2. 3. The true relationship between each feature X_i and y is linear. 2. [Components of GLM] 1. Random component: the probability distribution of the output variable Y. It’s expected value (mean value) is \\mathrm{E}(Y). 2. Systematic component: the weighted sum \\sum_{1}^{n}w_ix_i + w_0. 3. Link function: the relation between the random component and the systematic component g.  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}w_ix_i + w_0  3. [Equation of GAM]  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}f(x_i)  where f() can be arbitrarily defined function.\n\n\n\nhttps://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/ https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf 1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters. 1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane. 1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. 1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.\nNotes: 1. [SVM without slacks (hard margin SVM)]: Given a dataset with n instances x_{i} \\in R^{d} and n labels y_{i} \\in \\{-1, 1\\}, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights w \\in R^{d} and a bias b \\in R, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem: \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     1. Solving the above optimization problem will give us two parallel hyperplanes (w x + b = 1 and w x + b = -1) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between. 1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as  \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert w \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert w \\rVert} = \\frac{2}{\\lVert w \\rVert}  1. The constraints specify that the instances must be on the correct side of the two hyperplanes:  w x_{i} + b \\geq 1 \\quad \\mathrm{if} y_{i} = 1   w x_{i} + b \\leq -1 \\quad \\mathrm{if} y_{i} = -1  and y_{i}(w x_{i} + b) \\geq 1 summarizes the above two conditions. 1. [SVM with slacks (soft margin SVM)]: In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} + C \\sum_{i}^{n} \\xi_{i} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots n \\\\\n    \\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     where \\xi_{i} is the slack variable for the instance x_{i} and C is a hyperparameter that penalizes the misclassification of x_{i}. 1. If \\xi_{i} is nonzero for x_{i}, it means that x_{i} is on the misclassified side of w x_{i} + b = 1 (or w x_{i} + b = -1) and the distance is \\xi_{i}. 1. If C = 0, \\xi_{i} can be arbitrary large for each x_{i}. If C \\to \\inf, it is the same as hard margin SVM because any misclassification can induce infinite loss.\n\n[Solving hard margin SVM]\n\nRewrite the primal program for easier Lagrangian computation below: \n\\begin{alignat}{2}\n\\min \\quad & \\frac{1}{2} ww \\\\\n\\text{s.t. } \\quad & -(y_{i}(w x_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots n \\\\\n\\end{alignat}\n\nWe can derive the Lagrangian primal function from the primal program: \n\\begin{alignat}{2}\nL(w, b, \\alpha) & = f(w, b) + \\sum_{i}^{n} \\alpha h_{i}(w, b) \\\\\n& = \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n where \\alpha is a new variable called Lagrangian multiplier.\nThen we can write and solve Lagrangian dual function: \n\\begin{alignat}{2}\ng(\\alpha) & = \\min_{w, b} L(w, b, \\alpha) \\\\\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over w: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} & = 0 \\\\\nw - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} & = 0 \\\\\nw & = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over b: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial b} & = 0 \\\\\n\\sum_{i}^{n} \\alpha_{i}y_{i} & = 0 \\\\\n\\end{alignat}\n Plug in w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} back to g(\\alpha): \n\\begin{alignat}{2}\ng(\\alpha)\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right)\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 \\right) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\  \n\\end{alignat}\n Since we know that \\alpha_{i}y_{i} = 0, then b\\sum_{i}^{n} \\alpha_{i}y_{i} = 0, and thus the final Lagrange dual function is:  g(\\alpha) = \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Notice that \\alpha_{i}y_{i} = 0 is added as part of the constraint.\nSince strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:\n\nThe Lagrange dual problem only involves \\alpha_{i}, but primal problem has w and b, which are much more parameters.\nThe Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.\n\n\n[Solving soft margin SVM]\n\nSimilar as hard margin SVM, we can write Lagrangian dual function as: \n\\begin{alignat}{2}\ng(\\alpha, \\beta) & = \\min_{w, b} \\frac{1}{2} ww\n- \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n\\end{alignat}\n where a new Lagrange multiplier is introduced for the constraint \\xi_{i} \\geq 0.\nSimilar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the w, b, and \\xi_i: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} = 0 & \\Rightarrow w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i}^{n} \\alpha_{i}y_{i} = 0 \\\\\n\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n\\end{alignat}\n and plug the w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} and C = \\alpha_{i} + \\beta_{i} back in g(\\alpha, \\beta). \n\\begin{alignat}{2}\ng(\\alpha, \\beta)\n& = \\min_{w, b} \\frac{1}{2} ww + C\\sum_{i}^{n}\\xi_{i} - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right)\n     + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n} \\alpha_{i}\\xi_{i} + \\sum_{i}^{n} \\beta_{i}\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\sum_{i}^{n}\\alpha_{i}  - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\end{alignat}\n which has exactly the same form as Lagrangian dual function of hard margin SVM.\nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\beta_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Since we know C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}, the constraint \\beta_{i} \\geq 0 can be removed by merging into \\alpha_{i} \\geq 0: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n The only difference with Lagrange dual problem of hard margin SVM is the addition of C \\geq \\alpha_{i}.\n\n[Kernel trick]\n\nKernel trick\n\n[Duality and KKT conditions]\n\nThe Lagrangian dual problem:\n\nGiven a minimization primal problem: \n\\begin{alignat}{2}\n\\min_{x} \\quad & f(x) \\\\\n\\text{s.t. } \\quad & h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\quad & l_{j}(x) = 0, \\quad j = 1, \\dots, m \\\\\n\\end{alignat}\n\nThe Lagrangian is defined as:  L(x, u, v) = f(x) + \\sum_{i}^{n} u_{i}h_{i}(x) + \\sum_{j}^{m} v_{j}l_{j}(x)  where u_{i} and v_{j} are new variables called Lagrangian multipliers.\nThe Lagrange dual function is:  g(u, v) = \\min_{x} L(x, u, v) \nThe Lagrange dual problem is: \n\\begin{alignat}{2}\n\\max_{u, v} \\quad & g(u, v) \\\\\n\\text{s.t. } \\quad & u \\geq 0 \\\\\n\\end{alignat}\n\nThe properties of dual problem:\n\nThe dual problem is always convex even if the primal problem is not convex.\nFor any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).\n\n\nKarush-Kuhn-Tucker (KKT) conditions\n\nGiven the Lagrange dual problem stated above, the KKT conditions are:\n\nStationarity condition:  0 \\in \\partial \\left( f(x) + \\sum_{i=1}^{n} u_{i} h_{i}(x) + \\sum_{j=1}^{m} v_{j}l_{j}(x) \\right) \nComplementary Slackness:  u_{i}h_{i}(x) = 0, \\quad i = 1, \\dots, n \nPrimal feasibility:  h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n   l_{j}(x) = 0,  \\quad j = 1, \\dots, m \nDual feasibility:  u_{i} \\geq 0, \\quad i = 1, \\dots, n \n\nIf a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the x^{*} and u^{*}, v^{*} are primal and dual solutions if and only if x^{*} and u^{*}, v^{*} satisfy the KKT conditions.\n\n\n\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. 2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. 3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.\nNotes 1. [Gini Index and Information Entropy]: Both applies to a dataset (instances with labels) to measure its uncertainty. They both become 0 when there is only one class in the set.\nGini Index (Gini impurity) measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.  G(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)(1-\\textrm{P}(c)) = 1 - \\sum_{c=1}^{C} \\textrm{P}(c)^2  Information Entropy can be roughly thought as the dataset’s variance.  E(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)\\log_2\\textrm{P}(c)  In both cases, \\mathcal{D} is the dataset to be evaluated, C is the total number of classes in \\mathcal{D} and \\textrm{P}(c) is the probability of picking an instance with the class c (fraction of instances with class c in \\mathcal{D}). 2. [Gini Gain and Information Gain]: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset.  G(\\mathcal{D}, S) = M(\\mathcal{D}) - \\sum_{s\\in S}\\frac{\\lvert s \\rvert}{\\lvert D \\rvert} M(s) where \\mathcal{D} is the dataset before splitting, S are subsets of \\mathcal{D} created from all possible splitting of \\mathcal{d}, M is Gini Index (G) or Information Entropy (E), and \\lvert \\cdot \\rvert gives the number of items in a set. 3. [Decision tree training algorithm]: We consider binary classification decision tree. Given a dataset \\mathcal{D}, 1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. 2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. 3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset. 4. The splitting stops when no further splitting can be made (the dataset contains only one class).\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Random Forest contains many decision trees and combine all their outputs to give a final decision. 2. A particular goal in training a random forest is to make each tree in the forest different from each other. First is to use bootstrapping, which means that each decision tree is trained on different dataset that is randomly sampled with replacement from the original dataset. Then to further inject randomness, random subset of the features instead of all features are considered in each split of the decision tree. Then the final output of random forest is to take the majority vote or average each output. 3. The goal of randomize the decision trees and taking the aggregation result is to reduce the variance and thus prevent over-fitting of the single decision tree. By taking an average of the random predictions, some errors can cancel out. Using multiple trees in the prediction make random forest a black box and the explanation for a prediction is hard to be understood by the users.\nNotes: 1. [Bagging]: Bagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output.\n\n\n\nhttps://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html\nhttps://arxiv.org/pdf/1403.1452.pdf 1. Adaboost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (&gt;50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. 2. Adaboost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. 3. Adaboost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner.\nNotes: 1. [Adaboost algorithm] Here we show the adaboost algorithm for binary classification problems (y \\in \\{-1, 1\\}). 1. For the dataset with N instances, initialize the observation weights for each instance w_i=\\frac{1}{N}, i=1,2, ... ,N. 2. For m = 1 ... M, 1. Fit a classifier G_m(x) to the training instances with weights w_i. 2. Compute  E_m=\\frac{\\sum_{i=1}^{N} w_i \\mathcal{1}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^{N}w_i}  3. Compute  \\alpha_m = \\log(\\frac{1-E_m}{E_m})  4. Set  w_i \\gets w_i \\cdot e^{\\alpha_m y_i G_m(x_i)}  3. Final output of Adaboost:  G(x) = \\textrm{sign} (\\sum_{m=1}^M \\alpha_m G_m(x)) \n\n\n\n\nGradient boosting can be seen as the generalized version of boosting, i.e. Adaboost is one special case of gradient boosting. GB can be seen as\nGB is a generalized additive model of n weak learners.  G(x) = g_{1}(x) + \\dots + g_{n}(x)  where G(x) is the final gradient boosting model and g(x) is one type of weak learners.\nThe weak learner g(x) can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting.\n$g_{1}(x) g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets.\n\nGiven a loss function L(\\cdot), a training set X = \\{\\mathbf{x_{i}}\\}, \\mathbf{y} = \\{y_{i}\\}, a learning rate \\alpha, and a number of iterations M, the algorithm to train a GBRT is as follows: 1. Intiailize G(x) by fitting CART on D 1. For m = 1 \\dots M, 1. Evaluate the loss over the current G(x) 1. Calculate the gradient of the loss w.r.t the labels to get the residuals \\tilde{\\mathbf{y}}:  \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}} Note \\tilde{\\mathbf{y}} has the same shape as \\mathbf{y}. 1. Use X and residuals \\tilde{\\mathbf{y}} as the new training set to train a CART g(x). 1. Add the new weak leaner into the current model:  G(x) = G(x) + \\alpha g(x)"
  },
  {
    "objectID": "Notes/ML Q&A.html#ml-basics",
    "href": "Notes/ML Q&A.html#ml-basics",
    "title": "ML Q & A",
    "section": "",
    "text": "We always want the model to have low bias and low variance at the same time, but it is difficult.\nBias is the average difference between the predictions and the label.\nVariance refers to the sensitivity of our model to the fluctuations (noise) in the training set. OR variance describes how much a random variable of your model differs from its expected value.\nSince we only have training set that is a sample from the overall distribution, training our model to reduce bias will generally increase variance at the same time because we don’t know what is the noise.\nHigh bias and low variance lead to under-fitting, where your model is too simple to capture the regularities in your data. This will result in both low training accuracy and testing accuracy.\nLow bias and high variance lead to over-fitting, where your model is too complex that it captures all the patterns in your training data including noise. It will result in high training accuracy, but low testing accuracy.\n\n\n\n\n\nUnder-fitting is when your model is too simple to fit the training set correctly. This will result in low training accuracy and low testing accuracy, assuming that our training set is a reasonable sample of the overall distribution.\nTo combat under-fitting, the first we can do is to choose the model with more complexity. If decision tree under-performs on a dataset, we can try random forest. Second we can increase the parameters that our model have. Like increasing number of trees or the maximum depth of the decision tree. Last, for some models that uses iterative learning procedure such as neural network, we can increase the training time.\nOver-fitting is when your model is too complex such it can fit the training set very accurately. This means that it also fit the noise correctly and will result in high training accuracy and low testing accuracy.\nTo combat over-fitting, we first can reduce the number of parameters. Second, using ensemble of your current models is proven to reduce over-fitting. Third, without changing models, we can do data augmentation such as over-sampling more data to make the training better represent the actual distribution.\n\n\n\n\n\nSupervised learning task is usually provided with labeled dataset and goal is to predict the correct label for a given instance.\nUnsupervised learning only gives the instances themselves, without their labels. The goal is to learn patterns from those unlabeled data.\nSupervised learning include classification and regression. Classification is to predict a categorical label while regression is to predict a continuous value.\nUnsupervised learning include clustering and dimension reduction. Clustering is to group similar instances together and dimension reduction is to select the important features from all features.\n\n\n\n\n\nDimensionality refers to the number of features in your dataset.\nIt is harder for the models to search through a space as the number of features grows. The required number of training instances to achieve the same accuracy grows exponentially with the number of features. Since in practice the number of training instances are fixed, the performance of the models will typically decrease as the dimension increases.\nWe can use feature selection such as manual feature selection by human or feature extraction technique like PCA to reduce the dimensionality. The difference between selection and extraction is that selection selected subset of the original features and extraction extracts a set of new features from data.\n\n\n\n\n\nThe confusion matrix is used to evaluate the performance of an supervised classifier on a dataset. It has N rows and N columns where N is the number of classes in the dataset.\nEach row of the matrix gives the number of predictions from the classifier for a specific label and each column of the matrix gives the number of actual labels. If the classifier is for a binary classification dataset, the first row gives the number of true positives and the number of false positives and the second row will give false negatives and true negatives.\nOther performance evaluation methods such as accuracy or F1-score will be misleading on unbalanced dataset. For example, if we have a dataset that has 95 dogs and 5 cats and a classifier than always predict dog. We have 95% accuracy and 97.5% F1-score, which tells that our classifier is very good. Confusion matrix will give a whole picture of metrics that include true positive, true negative, false positive and false negative. If we add up true positive and false positive in the matrix, we can see that our classifier only predict positive labels.\n\nNotes: 1. For a binary classification problem, we can have several terms: 1. True positive: classifier gives positive class and the prediction is correct. 2. False positive: classifier gives positive class and the prediction is incorrect. 3. True negative: classifier gives negative class and the prediction is correct. 4. False negative: classifier gives negative class and the prediction is incorrect. 2. For a binary classification problem, we have several performance measures: 1. Sensitivity (true positive rate): number of positive predictions correctly labeled / number of instances with positive labels.  \\mathrm{\\frac{TP}{TP + FN}}  False positive rate: number of positive predictions incorrectly labeled / number of instances with negative labels.  \\mathrm{\\frac{FP}{FP + TN}}  2. Specificity (true negative rate): number of negative predictions correctly labeled / number of instances with negative labels.  \\mathrm{\\frac{TN}{TN + FP}}  3. Precision: number of positive instances correctly predicted / number of positive predictions.  \\mathrm{\\frac{TP}{TP + FP}}  4. Recall: same as sensitivity.  \\mathrm{\\frac{TP}{TP + FN}}  5. F1-score: harmonic mean of the precision and recall.  \\mathrm{2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = \\frac{TP}{TP + \\frac{1}{2}(FP + FN)}}\n\n\n\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc 1. An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The x axis is false positive rate and y axis is true positive rate. 2. An ROC curve plots TPR vs. FPR at different classification thresholds. For all models that first produce a score and then thresholded to give the classification, different thresholds mean different number of positive and negative predictions. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives and the curve will always go higher. 3. A perfect model will have a straight horizontal line at y = 1 while a perfectly wrong model will have a horizontal line at y = 0. A random guessing model will have a diagonal line, which means that the model has no class separation capacity. 4. AOC stands for area under the curve and measures the area under the ROC curve. It is used to measure a model’s performance across all possible classification thresholds.\n\n\n\n\nCalculate moving average of a set of data points is creating a series of averages of different subsets of the dataset. The dataset to be analyzed usually contains time series data (data that is indexed by timestamps). Exponential moving average (EMA) is one type of moving average algorithm.\nThe EMA value at timestamp t calculates the average of the data from the beginning to timestamp t, which is calculated by adding up the weighted value of the data at timestamp t and the weighted EMA value on the previous timestamp t-1.\nThe weighted part is called the smoothing factor and is a hyper-parameter that is between 0 and 1. Higher smoothing factor means the current data value is weighted more and the previous EMA value is weighted less in the calculation of the new EMA value.\nThis techniques is usually used by gradient optimizer to calculate new learning rate.\n\nNotes: 1. [EMA equation]: If the current timestamp is t, then the equation for EMA is:  y_{t} = \\alpha x_{t} + (1 - \\alpha)y_{t-1}  where y_{t} is the moving average at timestamp t, x_{t} is the data point at timestamp t, and \\alpha is the smoothing factor, which is the range [0, 1].\n\n\n\nTODO"
  },
  {
    "objectID": "Notes/ML Q&A.html#neural-network",
    "href": "Notes/ML Q&A.html#neural-network",
    "title": "ML Q & A",
    "section": "",
    "text": "Gradient descent is an optimization algorithm that can iteratively minimize the target function to find its local minimum. If the target function is convex, then gradient descent can also find its global minimum.\nFirst we calculate the derivatives of the target function with respect to the parameters of the model. This is the gradient.\nSecond we update the parameters to the opposite direction of the gradients to minimize the value of the target function. This is the descent.\nGradient descent will minimize the target function, while gradient ascent, which updates the parameters to the same direction of the gradients, will maximize the target function.\n\n\n\n\n\nTODO\n\n\n\n\nhttps://ruder.io/optimizing-gradient-descent/index.html#fn4 https://cs231n.github.io/neural-networks-3/#update 1. Normal gradient descent is batch gradient descent. One batch means one complete run of the training set. Thus, we need to evaluate all instances of the training set before one gradient update. Gradient descent is more slow, but guaranteed to converge to the local minimum. 2. Stochastic gradient descent means that one gradient update is performed for each instance evaluated. This approach converges faster and can be used on the fly as the new instance comes in, but can cause target function to fluctuate. 3. Mini-batch is the combination of the two above. Instead of whole batch or single instance, we take subset of training set as the mini-batch and evaluate them to get gradient for a single gradient update. This combines the benefits of two method above.\nNotes: 1. [Different gradient descent optimization algorithms] 1. SGD  \\theta_{t+1} = \\theta_{t} - \\lambda \\partial_{t}(\\theta)  where \\lambda is the learning rate and \\partial_{t}(\\theta) is the gradient of the loss function w.r.t the parameter \\theta at time t. 2. SGD with Momentum: It will help the convergence speed of SGD because it reduces the oscillations of the SGD near the local minimas, which is done by building up the velocity in the correct direction that has consistent gradients  v_{t} = \\mu v_{t-1} - \\lambda\\cdot\\partial_{t}(\\theta)    \\theta_{t+1} = \\theta_{t} + v_{t}  where \\mu is the momentum parameter that is typically 0.9 and v_{t} is the correct accumulated gradient direction at time t. 3. Adagrad: It will make the learning rates of the weights that receive high gradients reduced, and the learning rates of the weights that receive small or infrequent updates increased. Thus we don’t have to manually tune the learning rates in the training progress. However, since there is no way to reduce the accumulated squared gradients in the denominator of the learning rate, the monotonically decreasing learning rate in the training process will eventually stop the training.  g_{t} = g_{t-1} + \\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{g_{t} + \\epsilon}} \\partial_{t}(\\theta)  where g_{t} is the accumulation of the squared gradient for \\theta until time t and \\epsilon is a small value used to prevent the division by 0. 4. RMSprop: RMSprop improves on Adagrad by replacing the accumulation of the past squared gradients with the exponential moving average of the past squared gradients. This can solve the issue of Adagrad that the learning rates are monotonically decreasing.  e_{t} = \\beta e_{t-1} + (1-\\beta)\\partial_{t}^{2}(\\theta)   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda}{\\sqrt{e_{t} + \\epsilon}} \\partial_{t}(\\theta) where e_{t} is the exponential moving average of the squared gradient for \\theta until time t and \\beta is like momentum that controls degree of weighting decay and is usually set to 0.9. 5. Adam: Adam improves on RMSprop by replacing the raw gradient with the exponential moving average of the past gradients in the update step. It thus combines the benefits of RMSprop and SGD with momentum.  v_{t} = \\beta_{1} v_{t-1} + (1-\\beta_{1})\\partial_{t}(\\theta)   e_{t} = \\beta_{2} e_{t-1} + (1-\\beta_{2})\\partial_{t}^{2}(\\theta)   \\hat{v}_{t} = \\frac{v_{t}}{1-\\beta_{1}^{t}}   \\hat{e}_{t} = \\frac{e_{t}}{1-\\beta_{2}^{t}}   \\theta_{t+1} = \\theta_{t} - \\frac{\\lambda \\hat{v}_{t}}{\\sqrt{\\hat{e}_{t} + \\epsilon}}  where \\hat{v}_{t} and \\hat{e}_{t} are the bias corrected versions of v_{t} and e_{t} and \\beta^{t} means the \\beta to the power of t.\n\n\n\n\nVanishing gradient happens to the parameters in the earlier layers of a deep neural network where the gradients are so small in the back-propagation process that the weights are not really changed.\nThe primary reason for this problem is the choice of activation functions such as sigmoid or hyperbolic tangent function, whose gradients are very small and are always much less than 1. If multiple layers with such activations are stacked together, the gradients to the earlier layers of the networks are multiplied lots of times with the loss gradient in the back-propagation process. Each layer reduce the original loss gradient by a fraction and in the end the gradient to the earlier layers are very small. Therefore, the large number of layers are also an important reason for vanishing gradient problem.\nOne effective solution to the problem is to use other activation functions such as ReLU.\n\n\n\n\n\nIt can help solve the vanishing gradient problem. The gradient of the ReLU for inputs larger than 0 is 1 and thus the gradient won’t be reduced in the back-propagation process.\nReLU is computationally more efficient because it only needs to cut the negative input to 0.\nHistorically speaking, ReLU is just good enough for neural network to be trained stably.\n\n\n\n\n\nL_1 regularization is also called Lasso regularization. It adds the sum of the absolute value of all weights in the neural network as a penalty term to the loss function.\nL_2 regularization is also called Ridge regularization. It adds the sum of the squared value of all weights in the neural network as a penalty term to the loss function.\nThey both are used to reduce over-fitting issue of the large neural network.\nThe key difference is the gradient of each penalty. The gradient of Lasso is a 1 or -1 depending on the sign of each weight, while the gradient of Ridge is 2 times the value of the parameter. The weights with Lasso can possibly shrink to exactly 0, while weights with Ridge can only shrink to a very small value instead of exact 0 because the gradients also decrease as the weights decrease. Thus Lasso can be used to train sparse neural network.\n\n\n\n\n\nDropout is to randomly drop neurons of the network in the training process to avoid over-fitting. A neuron is dropped means that the data and the gradient don’t go through that neuron in both forward or backward process.\nTypically dropout is applied per layer and we can set a probability p for each layer to indicate what percentage of neurons we want to drop in that layer. p is usually selected for 0.5 for hidden layer, but a less value for input layer like 0.1 because randomly dropping an entire column of input data is very risky.\nThe dropout neurons are changed every instance or mini-batch depending on what type of gradient descent we are using. We only apply dropout in the training process and we will use all neurons for testing to have consistent output.\n\n\n\n\n\nBatch normalization is to normalize the inputs to each layer for each mini-batch. Batch normalization is proved to help neural network training converges faster.\nBatch normalization first normalize the inputs to each layer by subtracting the mini-batch mean from each value and then divide it by the mini-batch standard deviation. This process will make each input value to be in the range between 0 to 1. Then we need to scale and shift the normalized value into a desirable range. The coefficients for scaling and shifting are also two parameters that are needed to be learned in the backward propagation.\nThe challenge that batch normalization is trying to solve is called internal covariate shift. Since each layer’s output is fed into next layer’s input, the change of the weights in the first layer due to backward process after a mini-batch will cause the change of its output distribution. The internal covariate shift slows down the training process because the learning in the next iteration needs to accommodate for this change.\n\nNotes: 1. [Batch Normalization Layer]: Batch normalization layer can be appended before each layer and outputs the same dimension as the inputs 1. Get the mean \\mu_{B} and standard deviation \\sigma_{B} of the inputs in a mini-batch:  \\mu_{B}=\\frac{1}{m}\\sum_{i=1}^{m}x_i   \\sigma_{B}=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_{B})^{2}}  2. Normalize the input:  \\hat{x}_i=\\frac{x_i-\\mu_{B}}{\\sigma_{B}}  3. Scale and shift back, where \\gamma is the scaling parameter and \\beta is the shifting parameter:  y_{i} = \\gamma\\hat{x}_{i}+\\beta \n\n\n\nhttps://www.deeplearning.ai/ai-notes/initialization/index.html 1. Xavier initialization is a specific way of initialize the weights and bias in the neural network such that the variance of the activations are relatively the same across all layers. 2. If we use Xavier initialization method, the bias will be initialized to 0 and the weights are randomly sampled from a normal distribution that has mean of 0 and variance of 1 over the the number of neurons in the last layer. 3. If we initialize the weights to have the same value, all neurons will have the same activations. Same activations mean that all neurons will have the same gradients and will evolve the same throughout the training. 4. If we initialize the weights to be too small or too large, the activations of each layer in the first several iterations will also be very small or large. Since gradients of weights for each layer are calculated based on the activations, large weights will result in gradient exploding and small weights result in gradient vanishing, both of which prevent the neural from efficiently learning."
  },
  {
    "objectID": "Notes/ML Q&A.html#unsupervised-learning",
    "href": "Notes/ML Q&A.html#unsupervised-learning",
    "title": "ML Q & A",
    "section": "",
    "text": "https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning#clustering\n\n\nhttps://stanford.edu/~cpiech/cs221/handouts/kmeans.html 1. K-means is used to cluster unlabeled instances in dataset into K groups that are defined by their centroids. The points in the same group can be further labeled or analyzed. 2. We first randomly choose K centroids in the space. Then we cluster each data point to its nearest centroids and distance is calculated using sum of the square of the difference. After all data points have been assigned to a cluster, we recompute the centroids of the cluster by taking the average of all the data points that belong to that cluster. Then we cluster the data points again based on the new centroids and we repeat process until centroids don’t really change. 3. K-means is proved to find local minimum instead of global minimum. Thus the initialization of the centroids do matter to the outcome.\nNotes 1. [K-means algorithm]: 1. Initialize cluster centroids \\mu_{1}, ..., \\mu_{k} randomly. 2. Repeat until convergence: 1. Get the centroid for each instance x_{i}:  c_{i} = \\underset{\\mu_{i}}{\\operatorname{argmin}} \\lVert x_{i}-\\mu_{i} \\rVert^{2}  2. Update the centroid based on the instances:  \\mu_{j} = \\frac{\\sum_{i}^{m}1_{\\{c_i=j\\}}x_{i}}{\\sum_{i}^{m}1_{\\{c_i=j\\}}} \n\n\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c 1. Principal component analysis is an unsupervised learning algorithm that is used to reduce dimensionality of the training set. PCA selects multiple orthogonal dimensions that preserve maximum variance in the training set and optionally projects the training instances onto these dimensions. 2. To do PCA, we first need to normalize the training set by subtracting each value in a column by its mean and divide each value by column’s standard deviation. Then we get the covariance matrix of the normalized training set by multiply it with its transposed matrix. The covariance matrix gives us how each variable of the training set relates to each other. We can then use eigendecomposition to decompose the covariance matrix to get the eigenvalues and their corresponding eigenvectors. Here the eigenvectors are orthogonal components and the eigenvalues indicate the importance of the corresponding components. Finally we sort the eigenvalues in decreasing order and select first few eigenvalues and their corresponding eigenvectors as the principle components. We can get the transformed dataset by multiplying the training set with the selected eigenvectors.\nNotes: 1. [Eigenvectors, Eigenvalues]: Given a matrix A\\in\\mathbb{R}^{n\\times n}, \\lambda is said to be an eigenvalue of A if there exists a eigenvector z\\in\\mathbb{R}^n \\neq 0, such that:  Az = \\lambda z  2. [Eigendecomposition (spectral decomposition]: Let M be a real symmetric d \\times d matrix with eigenvalues \\lambda_{1}, ... , \\lambda_{d} and corresponding orthonormal eigenvectors u_{1}, ..., u_{d}. Then:  M = Q \\Lambda Q^T  where \\Lambda is a diagonal matrix with \\lambda_{1}, ... , \\lambda_{d} in diagonal and 0 elsewhere and Q matrix has u_{1}, ..., u{d} vectors as columns."
  },
  {
    "objectID": "Notes/ML Q&A.html#supervised-learning",
    "href": "Notes/ML Q&A.html#supervised-learning",
    "title": "ML Q & A",
    "section": "",
    "text": "[Bayes’ theorem]: the conditional possibility of event A given the event B is true P(A|B) can be computed as:  P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}  which in the Bayesian term is written as:  \\mathrm{Posterior} = \\frac{\\mathrm{Likelihood} \\cdot \\mathrm{Prior}}{\\mathrm{Evidence}}  If we think A as a label and B as a set of features:\n\nP(A|B) is the posterior probability of a label given a set of features.\nP(B|A) is the likelihood which is the probability of a set of features given a label.\nP(A) is the prior probability of a label.\nP(B) is the evidence probaility of a set of features.\n\n[Naive Bayes]: Naive Bayes is a classifier that selects the label \\hat{y} from all possible labels y \\in Y that has maximum conditional possibility given the instance \\mathbf{x}.  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} P(y|\\mathbf{x})  Applying Bayes’ theorem, we have:  P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\cdot P(y)}{P(\\mathbf{x})}  Since P(\\mathbf{x}) is a constant and is independent from P(y_{i}), we can simply drop it:  P(y|\\mathbf{x}) \\propto P(\\mathbf{x}|y) \\cdot P(y)  If we assume that each feature is independent from each other (naive conditional independence assumption), the possibility that the features values are all in \\mathbf{x} is the product of their possibilities:  P(\\mathbf{x}|y) = \\prod_{i}^{m}P(x_{i}|y)  Put them together, we have:  \\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} \\prod_{i}^{m}P(x_{i}|y) \\cdot P(y) \n\n\n\n\n\nhttps://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html 1. Linear regression is a supervised machine learning model that fits a correlation linear line between input and label variables. The output value can be arbitrary continuous value and thus it is used for regression. 2. The model has a weight vector and a bias as parameters. The output of the model is the dot product of the weight vector and the input vector plus a bias value. 3. The linear regression model is trained by solving an optimization problem that is defined by applying a cost function that evaluates the difference between the model’s output and the correct labels. The cost function for linear regression is mean squared error function that takes the mean of the squared value of each prediction’s error. MSE for linear regression is proved to be convex, so solving it using convex optimization or gradient descent will get the global minimum. 4. Logistic regression is similar to linear regression, but the output is a probability value between 0 and 1, so it is used for binary classification instead of regression. 5. A sigmoid (logistic) function is attached after the output of linear regression to output a probability for logistic regression. Instead of using MSE, the cost function is changed to binary cross entropy such that the loss grows exponentially with the difference between outputs and labels.\nNotes: 1. [Mean squared Error (MSE)]:\n \\mathrm{MSE} = \\frac{1}{n}\\sum_{i}^{n}(y_{i}-\\hat{y}_{i})^2  where y_{i} is the actual label and \\hat{y}_{i} is the prediction given by the classifier. 1. [Binary cross entropy (BCE)]: only works if the labels y_{i} are 0 or 1 and the values of predictions \\hat{y}_{i} are between 0 and 1.  \\mathrm{BCE} = -\\frac{1}{n}\\sum_{i}^{n}(y_{i}\\log(\\hat{y}_{i}) + (1-y_{i})\\log(1-\\hat{y})))  which can be decomposed to two cases for each prediction and label pair:  -\\log(\\hat{y}_{i}) \\;\\mathrm{if}\\; y=1   -\\log(1 - \\hat{y}_{i}) \\;\\mathrm{if}\\; y=0  1. [Sigmoid (logistic) function and logic function]:  \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}   \\mathrm{logic}(x) = \\log(\\frac{x}{1 - x}) The inverse of sigmoid function is the logic function: \n    \\begin{alignat}{2}\n    x &= \\frac{1}{1 + e^{-y}} \\\\\n    \\frac{1}{x} &= 1 + e^{-y} \\\\\n    e^{-y} &= \\frac{1 - x}{x} \\\\\n    e^{y} &= \\frac{x}{1 - x} \\\\\n    y &= \\log(\\frac{x}{1 - x}) \\\\\n    \\end{alignat}\n     1. [How to solve the parameters for linear regression and logistic regression] 1. Linear regression can be solved mathematically by setting partial derivative of loss w.r.t each weight to 0: (bias is removed for simplification)  \\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{N} \\sum_{i}^{N} x_{i,k} \\bigg(\\sum_{j}^{D}w_{j}x_{i,j} - \\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} - x_{i, k}\\hat{y}_{i} \\bigg)   = \\frac{2}{N} \\sum_{i}^{N} \\bigg( x_{i, k} \\sum_{j}^{D}w_{j}x_{i,j} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}   = \\frac{2}{N} \\sum_{j}^{D} w_{j} \\bigg( \\sum_{i}^{N} x_{i,j}x_{i,k} \\bigg) - \\frac{2}{N} \\sum_{i}^{N} x_{i,k}\\hat{y}_{i}  1. Gradient descent can be applied to solve both linear regression and logistic regression. Logistic regression doesn’t have a closed-form solution because of the non-linearity that the sigmoid function imposes.\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/extend-lm.html 1. Generalized linear models build on linear regression models to predict a non-Gaussian distribution. It keeps the weighted sum of the features of the linear regression, but connect the weighted sum and the expected mean of the output distribution through a possibly nonlinear function. 2. For example, the logistic regression is a type of the generalized linear models and it assumes a Bernoulli distribution for the outcome and links the expected mean and the weighted sum using the logic function. 3. Generalized additive models further relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modeled by a sum of arbitrary functions of each feature. It allows to model the potentially non-linear relations between the features and the output.\nNotes: 1. [Assumptions of linear regression]: 1. The input features are independent from each other (no interactions between the features). 2. The output distribution y given the input features X follows a Gaussian distribution. This follows the following theorem: &gt; Let X_1, ..., X_n be n mutually independent normal random variables, having means \\mu_1, ..., \\mu_n and variances \\sigma_1^2, ... \\sigma_n^2. If the random variable Y is a linear combinations of the X with w_1, ..., w_n coefficients: Y=\\sum_{i=1}^{n}w_iX_i, then Y is a Gaussian distribution with the mean \\mathrm{E}[Y] = \\sum_{i=1}^{n}b_i\\mu_i and variance \\mathrm{Var}[Y] = \\sum_{i=1}^{n}b_i^2\\sigma_i^2. 3. The true relationship between each feature X_i and y is linear. 2. [Components of GLM] 1. Random component: the probability distribution of the output variable Y. It’s expected value (mean value) is \\mathrm{E}(Y). 2. Systematic component: the weighted sum \\sum_{1}^{n}w_ix_i + w_0. 3. Link function: the relation between the random component and the systematic component g.  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}w_ix_i + w_0  3. [Equation of GAM]  g(\\mathrm{E}(Y)) = \\sum_{1}^{n}f(x_i)  where f() can be arbitrarily defined function.\n\n\n\nhttps://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/ https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf 1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters. 1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane. 1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. 1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process.\nNotes: 1. [SVM without slacks (hard margin SVM)]: Given a dataset with n instances x_{i} \\in R^{d} and n labels y_{i} \\in \\{-1, 1\\}, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights w \\in R^{d} and a bias b \\in R, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem: \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     1. Solving the above optimization problem will give us two parallel hyperplanes (w x + b = 1 and w x + b = -1) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between. 1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as  \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert w \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert w \\rVert} = \\frac{2}{\\lVert w \\rVert}  1. The constraints specify that the instances must be on the correct side of the two hyperplanes:  w x_{i} + b \\geq 1 \\quad \\mathrm{if} y_{i} = 1   w x_{i} + b \\leq -1 \\quad \\mathrm{if} y_{i} = -1  and y_{i}(w x_{i} + b) \\geq 1 summarizes the above two conditions. 1. [SVM with slacks (soft margin SVM)]: In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n    \\begin{alignat}{2}\n    \\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} + C \\sum_{i}^{n} \\xi_{i} \\\\\n    \\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots n \\\\\n    \\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n    \\end{alignat}\n     where \\xi_{i} is the slack variable for the instance x_{i} and C is a hyperparameter that penalizes the misclassification of x_{i}. 1. If \\xi_{i} is nonzero for x_{i}, it means that x_{i} is on the misclassified side of w x_{i} + b = 1 (or w x_{i} + b = -1) and the distance is \\xi_{i}. 1. If C = 0, \\xi_{i} can be arbitrary large for each x_{i}. If C \\to \\inf, it is the same as hard margin SVM because any misclassification can induce infinite loss.\n\n[Solving hard margin SVM]\n\nRewrite the primal program for easier Lagrangian computation below: \n\\begin{alignat}{2}\n\\min \\quad & \\frac{1}{2} ww \\\\\n\\text{s.t. } \\quad & -(y_{i}(w x_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots n \\\\\n\\end{alignat}\n\nWe can derive the Lagrangian primal function from the primal program: \n\\begin{alignat}{2}\nL(w, b, \\alpha) & = f(w, b) + \\sum_{i}^{n} \\alpha h_{i}(w, b) \\\\\n& = \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n where \\alpha is a new variable called Lagrangian multiplier.\nThen we can write and solve Lagrangian dual function: \n\\begin{alignat}{2}\ng(\\alpha) & = \\min_{w, b} L(w, b, \\alpha) \\\\\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over w: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} & = 0 \\\\\nw - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} & = 0 \\\\\nw & = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\end{alignat}\n Taking the derivation of L(w, b, \\alpha) over b: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial b} & = 0 \\\\\n\\sum_{i}^{n} \\alpha_{i}y_{i} & = 0 \\\\\n\\end{alignat}\n Plug in w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} back to g(\\alpha): \n\\begin{alignat}{2}\ng(\\alpha)\n& = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right)\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 \\right) \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\  \n\\end{alignat}\n Since we know that \\alpha_{i}y_{i} = 0, then b\\sum_{i}^{n} \\alpha_{i}y_{i} = 0, and thus the final Lagrange dual function is:  g(\\alpha) = \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Notice that \\alpha_{i}y_{i} = 0 is added as part of the constraint.\nSince strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:\n\nThe Lagrange dual problem only involves \\alpha_{i}, but primal problem has w and b, which are much more parameters.\nThe Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t.\n\n\n[Solving soft margin SVM]\n\nSimilar as hard margin SVM, we can write Lagrangian dual function as: \n\\begin{alignat}{2}\ng(\\alpha, \\beta) & = \\min_{w, b} \\frac{1}{2} ww\n- \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n\\end{alignat}\n where a new Lagrange multiplier is introduced for the constraint \\xi_{i} \\geq 0.\nSimilar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the w, b, and \\xi_i: \n\\begin{alignat}{2}\n\\frac{\\partial L}{\\partial w} = 0 & \\Rightarrow w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i}^{n} \\alpha_{i}y_{i} = 0 \\\\\n\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n\\end{alignat}\n and plug the w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} and C = \\alpha_{i} + \\beta_{i} back in g(\\alpha, \\beta). \n\\begin{alignat}{2}\ng(\\alpha, \\beta)\n& = \\min_{w, b} \\frac{1}{2} ww + C\\sum_{i}^{n}\\xi_{i} - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right)\n     + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right)\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\    \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n} \\alpha_{i}\\xi_{i} + \\sum_{i}^{n} \\beta_{i}\\xi_{i}\n     + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n& = \\min_{w, b} \\sum_{i}^{n}\\alpha_{i}  - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\end{alignat}\n which has exactly the same form as Lagrangian dual function of hard margin SVM.\nThe Lagrange dual problem is written as: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\beta_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n Since we know C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}, the constraint \\beta_{i} \\geq 0 can be removed by merging into \\alpha_{i} \\geq 0: \n\\begin{alignat}{2}\n\\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n\\quad & \\alpha_{i}y_{i} = 0 \\\\\n\\end{alignat}\n The only difference with Lagrange dual problem of hard margin SVM is the addition of C \\geq \\alpha_{i}.\n\n[Kernel trick]\n\nKernel trick\n\n[Duality and KKT conditions]\n\nThe Lagrangian dual problem:\n\nGiven a minimization primal problem: \n\\begin{alignat}{2}\n\\min_{x} \\quad & f(x) \\\\\n\\text{s.t. } \\quad & h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\quad & l_{j}(x) = 0, \\quad j = 1, \\dots, m \\\\\n\\end{alignat}\n\nThe Lagrangian is defined as:  L(x, u, v) = f(x) + \\sum_{i}^{n} u_{i}h_{i}(x) + \\sum_{j}^{m} v_{j}l_{j}(x)  where u_{i} and v_{j} are new variables called Lagrangian multipliers.\nThe Lagrange dual function is:  g(u, v) = \\min_{x} L(x, u, v) \nThe Lagrange dual problem is: \n\\begin{alignat}{2}\n\\max_{u, v} \\quad & g(u, v) \\\\\n\\text{s.t. } \\quad & u \\geq 0 \\\\\n\\end{alignat}\n\nThe properties of dual problem:\n\nThe dual problem is always convex even if the primal problem is not convex.\nFor any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).\n\n\nKarush-Kuhn-Tucker (KKT) conditions\n\nGiven the Lagrange dual problem stated above, the KKT conditions are:\n\nStationarity condition:  0 \\in \\partial \\left( f(x) + \\sum_{i=1}^{n} u_{i} h_{i}(x) + \\sum_{j=1}^{m} v_{j}l_{j}(x) \\right) \nComplementary Slackness:  u_{i}h_{i}(x) = 0, \\quad i = 1, \\dots, n \nPrimal feasibility:  h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n   l_{j}(x) = 0,  \\quad j = 1, \\dots, m \nDual feasibility:  u_{i} \\geq 0, \\quad i = 1, \\dots, n \n\nIf a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the x^{*} and u^{*}, v^{*} are primal and dual solutions if and only if x^{*} and u^{*}, v^{*} satisfy the KKT conditions.\n\n\n\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Decision tree is a tree structure that consists lots of decision nodes and can be used for both classification and regression. Each internal node of the tree splits on certain value of a feature to crate different decision branches and the leaf nodes are the predicted labels. To make a prediction, we start from the root node and follow the path that matches our instance until the leaf node where we are given the label for the instance. 2. To train a classification decision tree, we greedily split on certain feature value that has the max uncertainty gain among all possible splitting choices. The uncertainty gain is calculated using Gini impurity index or information gain that measure how much uncertainty can be reduced in the dataset after the splitting. After the splitting, two or more new child nodes will be created. For each new node, we apply the same algorithm again with the subset of the training instances that follows the decision path. We only stop splitting when we only have one class left in the remaining training instances and that node is a leaf node with the label given by the remaining training instances. 3. Decision tree is interpretable and very efficient to learn, but suffers from over-fitting because tree can be constructed very complex so that a slight difference of the instance will cause the label change. We can apply post pruning or setting the maximum depth to reduce it.\nNotes 1. [Gini Index and Information Entropy]: Both applies to a dataset (instances with labels) to measure its uncertainty. They both become 0 when there is only one class in the set.\nGini Index (Gini impurity) measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.  G(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)(1-\\textrm{P}(c)) = 1 - \\sum_{c=1}^{C} \\textrm{P}(c)^2  Information Entropy can be roughly thought as the dataset’s variance.  E(\\mathcal{D}) = \\sum_{c=1}^{C} \\textrm{P}(c)\\log_2\\textrm{P}(c)  In both cases, \\mathcal{D} is the dataset to be evaluated, C is the total number of classes in \\mathcal{D} and \\textrm{P}(c) is the probability of picking an instance with the class c (fraction of instances with class c in \\mathcal{D}). 2. [Gini Gain and Information Gain]: Both measure the uncertainty (Gini Index and Information Entropy) difference between before and after a splitting on the dataset.  G(\\mathcal{D}, S) = M(\\mathcal{D}) - \\sum_{s\\in S}\\frac{\\lvert s \\rvert}{\\lvert D \\rvert} M(s) where \\mathcal{D} is the dataset before splitting, S are subsets of \\mathcal{D} created from all possible splitting of \\mathcal{d}, M is Gini Index (G) or Information Entropy (E), and \\lvert \\cdot \\rvert gives the number of items in a set. 3. [Decision tree training algorithm]: We consider binary classification decision tree. Given a dataset \\mathcal{D}, 1. Identify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split. 2. Calculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split. 3. Once a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset. 4. The splitting stops when no further splitting can be made (the dataset contains only one class).\n\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/ 1. Random Forest contains many decision trees and combine all their outputs to give a final decision. 2. A particular goal in training a random forest is to make each tree in the forest different from each other. First is to use bootstrapping, which means that each decision tree is trained on different dataset that is randomly sampled with replacement from the original dataset. Then to further inject randomness, random subset of the features instead of all features are considered in each split of the decision tree. Then the final output of random forest is to take the majority vote or average each output. 3. The goal of randomize the decision trees and taking the aggregation result is to reduce the variance and thus prevent over-fitting of the single decision tree. By taking an average of the random predictions, some errors can cancel out. Using multiple trees in the prediction make random forest a black box and the explanation for a prediction is hard to be understood by the users.\nNotes: 1. [Bagging]: Bagging involves two procedures: bootstrapping and Aggregating. Bootstrapping means training each model with sampled with replacement subset of the dataset. Aggregating means combining each model in some specific way to give the final output.\n\n\n\nhttps://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html\nhttps://arxiv.org/pdf/1403.1452.pdf 1. Adaboost, or boosting in general, combines a series of weak learners into a strong learner. A weak learner is defined as any classifier that is slightly better than random guessing (&gt;50%) which means that it has some basic understandings of the underlying distribution of the dataset. The output from the final strong learner is a combination of the weighted outputs of the weak learners. 2. Adaboost works by repeatedly fitting a base model on training instances with different weights. First we initialize a equal weight for each training instance and then we have M iterations. In each iteration, we fit the base model on the training instances with the current weights and get a value called error rate that evaluates what is the percentage of the weights of the incorrectly classified instances. The error rate then is used to compute the classifier coefficient that increases as the error rate decreases. In the end of each iteration, we update the weight of each instance so that misclassified instances get larger weights and correctly classified instances get lower weights. After the iterations, we get M classifiers and their coefficients. To make a prediction for an instance from the strong learner, we get the outputs from the M classifiers, sum up the product of the outputs and their coefficients and take the sign of value as the final output. 3. Adaboost assumes the weak learner to always have training accuracy larger than 50% and the output class to be 1 and -1. A very short decision tree called decision stump is usually used as the weak learner.\nNotes: 1. [Adaboost algorithm] Here we show the adaboost algorithm for binary classification problems (y \\in \\{-1, 1\\}). 1. For the dataset with N instances, initialize the observation weights for each instance w_i=\\frac{1}{N}, i=1,2, ... ,N. 2. For m = 1 ... M, 1. Fit a classifier G_m(x) to the training instances with weights w_i. 2. Compute  E_m=\\frac{\\sum_{i=1}^{N} w_i \\mathcal{1}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^{N}w_i}  3. Compute  \\alpha_m = \\log(\\frac{1-E_m}{E_m})  4. Set  w_i \\gets w_i \\cdot e^{\\alpha_m y_i G_m(x_i)}  3. Final output of Adaboost:  G(x) = \\textrm{sign} (\\sum_{m=1}^M \\alpha_m G_m(x)) \n\n\n\n\nGradient boosting can be seen as the generalized version of boosting, i.e. Adaboost is one special case of gradient boosting. GB can be seen as\nGB is a generalized additive model of n weak learners.  G(x) = g_{1}(x) + \\dots + g_{n}(x)  where G(x) is the final gradient boosting model and g(x) is one type of weak learners.\nThe weak learner g(x) can be any regression model (output a real number). The regression tree is the most commonly used weak leaner in Gradient Boosting.\n$g_{1}(x) g_{n}(x) $ are the same weak leaner (regression tree) trained on different training sets.\n\nGiven a loss function L(\\cdot), a training set X = \\{\\mathbf{x_{i}}\\}, \\mathbf{y} = \\{y_{i}\\}, a learning rate \\alpha, and a number of iterations M, the algorithm to train a GBRT is as follows: 1. Intiailize G(x) by fitting CART on D 1. For m = 1 \\dots M, 1. Evaluate the loss over the current G(x) 1. Calculate the gradient of the loss w.r.t the labels to get the residuals \\tilde{\\mathbf{y}}:  \\tilde{\\mathbf{y}} = \\frac{\\partial L(G(X), \\mathbf{y})}{\\partial \\mathbf{y}} Note \\tilde{\\mathbf{y}} has the same shape as \\mathbf{y}. 1. Use X and residuals \\tilde{\\mathbf{y}} as the new training set to train a CART g(x). 1. Add the new weak leaner into the current model:  G(x) = G(x) + \\alpha g(x)"
  },
  {
    "objectID": "Notes/SSGD.html",
    "href": "Notes/SSGD.html",
    "title": "SSGD",
    "section": "",
    "text": "03-05-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/SSGD.html#proposed-ssgd-framework",
    "href": "Notes/SSGD.html#proposed-ssgd-framework",
    "title": "SSGD",
    "section": "Proposed SSGD framework",
    "text": "Proposed SSGD framework\nTypically, a loss function for a neural network function is as follows:\n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda G(\\boldsymbol{\\theta})\nIn which, J({\\boldsymbol{\\theta}}) is the accuracy loss function and G(\\boldsymbol{\\theta}) is the regularization function.\nIn the SSR literature, G(·) is usually referred to as the general diversity measure that serves as an alternative to the L_0 “norm” for encouraging sparsity.\nWe further define a separable diversity measure that has the form\nG(\\theta) = \\sum g(\\theta_i)\nwhere g(·) has the following properties:\n\ng(u) is symmetric.\ng(\\lvert u \\rvert) is monotonically increasing with \\lvert u \\rvert.\ng(u) is finite.\ng(u) is strictly concave in \\lvert u \\rvert or u^2.\n\nAny function that holds the above properties is a candidate for effective SSR algorithm and thus a candidate for the proposed SSGD framework."
  },
  {
    "objectID": "Notes/SSGD.html#iterative-reweighting-frameworks.",
    "href": "Notes/SSGD.html#iterative-reweighting-frameworks.",
    "title": "SSGD",
    "section": "Iterative reweighting frameworks.",
    "text": "Iterative reweighting frameworks.\nIterative reweighted l_2 and l_1 frameworks are two popular frameworks in SSR literature to solve the above problem.\nFor each timestamp t, we have these two target functions to minimize, respectively for l_2 and l_1 frameworks:\n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_2^2 \n\\underset{\\theta}{\\min} J({\\boldsymbol{\\theta}}) + \\lambda \\lVert \\boldsymbol{\\Omega}\\boldsymbol{\\theta} \\rVert_1 \nwhere \\Omega is a vector of w_i, which is pre-computed based on \\theta_i as discussed below and \\theta_i is the variable that we are solving for.\nIn both cases, they need to satisfy g(u)=f(u^2) and g(u)=f(|u|) respectively, where f(z) is concave for z \\in R_+.\nIn each iteration, after \\theta_i is solved, we can use it to update w_i. Their update rules for l_2 and l_1 frameworks are:\nw_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-\\frac{1}{2}} \\mathrm{where} \\; z=\\theta_i^2\nw_i = \\bigg( \\frac{\\textrm{d}f(z)}{\\textrm{d}z} \\bigg) ^{-1} \\mathrm{where} \\; z=\\lvert \\theta_i \\rvert"
  },
  {
    "objectID": "Notes/SSGD.html#log-sum-as-the-diversity-measure",
    "href": "Notes/SSGD.html#log-sum-as-the-diversity-measure",
    "title": "SSGD",
    "section": "Log-sum as the diversity measure",
    "text": "Log-sum as the diversity measure\nLog-sum penalties for the reweighted l_2 and l_1 frameworks:\ng(\\theta_i) = \\log(\\theta_i^2 + \\epsilon)\ng(\\theta_i) = \\log(\\lvert \\theta_i \\rvert + \\epsilon)\nwhere \\epsilon &gt; 0 and smaller \\epsilon induces stronger sparsity.\nTheir w_i can be computed as:\nw_i = (\\theta_i^2 + \\epsilon) ^ \\frac{1}{2}\nw_i = \\lvert \\theta_i \\rvert + \\epsilon"
  },
  {
    "objectID": "Notes/SSGD.html#ssgd-sparsity-promoting-stochastic-gradient-descent",
    "href": "Notes/SSGD.html#ssgd-sparsity-promoting-stochastic-gradient-descent",
    "title": "SSGD",
    "section": "SSGD: Sparsity-promoting Stochastic Gradient Descent",
    "text": "SSGD: Sparsity-promoting Stochastic Gradient Descent\nSparsity-promoting matrix store all the coefficients that can be multiplied with the gradient for each parameter during gradient descent update to promote sparsity.\nThe coefficient can be obtained from w_i by\ns_i = \\frac{w_i^2}{\\frac{1}{|\\mathcal{I}^k|} \\sum_{j\\in \\mathcal{I}^k} w_j^2} \\mathrm{, for} \\; i \\in \\mathcal{I}^k\nwhere \\mathcal{I}^k is the set of parameters (weights) in layer k and |\\mathcal{I}^k| is the number of parameters in the layer k.\nComplete SSGD training algorithm for DNN:\n\nApply one of the general diversity measure in the loss function.\nAnd for each gradient update: 1. Compute scaling factors: w_i based on \\theta_i. 2. Compute s_i by the equation above. 3. Update parameters: \\theta_i = \\theta_i - \\mathrm{lr} \\times s_i \\times \\nabla_i, where \\nabla_i is the gradient for \\theta_i."
  },
  {
    "objectID": "Notes/RIPPER.html",
    "href": "Notes/RIPPER.html",
    "title": "RIPPER",
    "section": "",
    "text": "This page contains my reading notes on\nSome of the knowledge are also from:"
  },
  {
    "objectID": "Notes/RIPPER.html#introduction",
    "href": "Notes/RIPPER.html#introduction",
    "title": "RIPPER",
    "section": "Introduction",
    "text": "Introduction\n\n\nIn this paper, Cohen first implements his own version of IREP (Incremental Reduced Error Pruning) with some minor differences and has support multi-label problems and missing attributes.\nThen he proposes several major changes to IREP and names the improved version IREP*.\nFinally, based on IREP*, he proposes a new rule mining algorithm called RIPPER (Repeated Incremental Pruning to Produce Error Reduction)."
  },
  {
    "objectID": "Notes/RIPPER.html#irep-cohen-version",
    "href": "Notes/RIPPER.html#irep-cohen-version",
    "title": "RIPPER",
    "section": "## IREP (Cohen version)",
    "text": "## IREP (Cohen version)\n\nIREP algorithm\nThe characteristics of IREP have two fold: 1. Separate and conquer: the covered instances in the training set are removed after a rule is found; thus in the next iteration, a new rule will be learned on the training instances that have not been covered by the previously found rules. 1. Integration of pre-pruning and post-pruning: 1. Pre-pruning: some training examples are deliberately ignored to in the training process (early stopping condition). 1. Post-pruning: first the model is trained to fit the training set perfectly and then some parts of the model are deleted after the training (branch cutting).\n\nFunction: IREP.\nInput: the training set \\mathcal{D} with binary labels and all possible features \\mathcal{F}.\nOutput: the learned rule set \\mathcal{R}. 1. Initialize an empty rule set \\mathcal{R}. 1. While there are still positive instances in \\mathcal{D}: 1. Randomly choose 2/3 from \\mathcal{D} as the growing set \\mathcal{G} and the rest 1/3 becomes the pruning set \\mathcal{P}. 1. R = GrowRule(\\mathcal{G}) 1. R = PruneRule(\\mathcal{P}, R) 1. If the accuracy of R &lt; 0.5 on \\mathcal{P}: break 1. Add R to \\mathcal{R}. 1. Remove instances that are covered by R from \\mathcal{D}. 1. Return \\mathcal{R}\n\nThe minor differences between the Cohen’s implementation of IREP and the original version are: 1. stopping condition. The original IREP stopped when the accuracy of the learned rule is less than the accuracy of the empty rule instead of 50%. 1. PruneRule algorithm, which is to be detailed later.\n\n\nGrow a Rule\nIn each iteration, the feature f with value v that has the maximum FOIL score is selected to the rule and the iterations terminate when the rule doesn’t cover any negative instances in the growing set.\n\nFunction: GrowRule.\nInput: the growing set \\mathcal{G} with binary labels and all possible features \\mathcal{F}.\nOutput: the unpruned rule R. 1. Initialize an empty rule R. 1. Until all instances in \\mathcal{G} that satisfy R are positive (accuracy of R is 1 in \\mathcal{G}) or there is no feature to add: 1. For every feature f \\in \\mathcal{F} not in R and every possible value v \\in \\mathcal{V}(f): 1. Create a temp rule R_{t} by copying current R. 1. Add (f, v) to R_{t}. 1. Calculate FOIL’s information gain of R_{t}: \\mathrm{Foil}(R, R_{t}) based on \\mathcal{G}. 1. Get the R_{t}^{max} with the max value of \\mathrm{Foil}(R_{t}). 1. R=R_{t}^{max}. 1. Return R.\n\n[Support for categorical and continuous features]: the definition of \\mathcal{V}(f) for different feature f is different for categorical and numerical features. 1. For a categorical feature f_{c}, \\mathcal{V}(f) is the collection of all possible values that f_{c} can take. 1. For a numerical feature f_{n}, \\mathcal{V}(f) is the Cartesian product of \\{\\leq, \\geq\\} and all values of f that appear in the training set. For example, if all values that appear in the training set for feature age is \\{10, 20, 30\\}, then \\mathcal{V}(\\text{age}) is \\{\\leq 10, \\geq 10, \\leq 20, \\geq 20, \\leq 30, \\geq 30\\}\n[FOIL’s information gain]: it gives how much information entropy is reduced from R_{old} to R_{new}.\n \\operatorname{Foil}(R_{old}, R_{new}) = P(R_{new}) \\left( \\log_{2} \\left( \\frac{P(R_{new})}{P(R_{new}) + N(R_{new})} \\right) - \\log_{2} \\left( \\frac{P(R_{old})}{P(R_{old}) + N(R_{old})} \\right) \\right) \nwhere P(R) (N(R)) is the number of positive (negative) instances covered by R.\n\n\nPrune a Rule\nPruneRule considers deleting any final sequence of conditions in the order they are grown from the rule and chooses the deletion that maximizes the Rule-Value metric on the pruning set.\n\nFunction: PruneRule.\nInput: the pruning set \\mathcal{P} and the unpruned rule R.\nOutput: the pruned rule R_{p}. 1. Initialize R_{p} = R. 1. For all (f, v)_{i} \\in R starting from the last added one to the first one: 1. Removing (f, v)_{i} from R. 1. If \\operatorname{Value}(R) \\geq \\operatorname{Value}(R_{p}): then R_{p} = R. 1. Return R_{p}\n\nThe original implementation of IREP only considers the “deletions of a single final condition”.\n[IREP Rule-Value metric]:\n \\operatorname{Value}(R) = \\frac{P(R) + (N - N(R))}{P + N} \nwhere P (N) is the total number of positive (negative) instances and P(R) (N(R)) is the number of positive (negative) instances covered by R."
  },
  {
    "objectID": "Notes/RIPPER.html#irep-as-an-improved-version-of-irep",
    "href": "Notes/RIPPER.html#irep-as-an-improved-version-of-irep",
    "title": "RIPPER",
    "section": "IREP* as an improved version of IREP",
    "text": "IREP* as an improved version of IREP\n\nThe support for multi-class and missing value allows IREP to be applied on a wide range of benchmarks and Cohen further improves on his implementation of IREP on the stopping condition and pruning metric.\n\nNew Rule-Value metric\nThe IREP Rule-Value metric sometimes is highly unintuitive. Assuming P and N are fixed to be 3000, the IREP Rule-Value metric prefers R_{1} over R_{2} in the following example, but R_{2} is obviously more predictive. - R_{1}: P(R_{1}) = 2000, N(R_{1}) = 1000, \\operatorname{Value}(R) = \\frac{4000}{6000} - R_{2}: P(R_{2}) = 1000, N(R_{2}) = 1, \\operatorname{Value}(R) = \\frac{3999}{6000}\nCohen’s solution doesn’t have the issue mentioned above.\n[IREP* Rule-Value metric]:\n \\operatorname{Value}(R) = \\frac{P(R) - N(R)}{P(R) + N(R)} \nwhere P(R) is the number of positive instances covered by R and N(R) is the number of negative instances covered by R\n\n\nNew Stopping condition\nThe IREP stops adding rules when the current learned rule has a bad (&lt; 50%) accuracy on the pruning set. This estimate often makes the algorithm stop too early especially if the current learned rule has very low coverage (the algorithm will stop if, for example, the rule only covers 2 instances and 1 instance has negative label).\nIREP* defines the stopping condition based on the total description length value of the currently learned rule set on the pruning set.\n\n\nCalculate the total description length of \\mathcal{R}: \\operatorname{MDL}(\\mathcal{R}).\nIf \\operatorname{MDL}(\\mathcal{R}) &gt; \\operatorname{MDL}_{min} + d: break\nIf \\operatorname{MDL}(\\mathcal{R}) &lt; \\operatorname{MDL}_{min}: \\operatorname{MDL}_{min} = \\operatorname{MDL}(\\mathcal{R})\n\n\nwhere \\operatorname{MDL}(\\mathcal{R}) is the total Description Length of the rule set \\mathcal{R} and d is a hyperparameter with the default value of 64 in the paper’s experiment.\n[MDL Principle (Minimum Description Length Principle)]: From the Machine Learning perspective, each model derived from the dataset can be characterized by a description length, which is defined as the number of bits required to encode the model and the data from which it was learned. MDL Principle states that the model with the minimum description length is generally preferred to avoid over-fitting.\nDescription length consists of model description length (theory cost) and exceptions description length (exceptions cost). - Model description length measures the complexity of the model. Higher model description length means that the model is more complex and thus more prone to over-fitting. - Exceptions description length measures the degree to which the model incorrectly fit to the data. The large the exceptions description length, the more error-prone the model is.\nFor RIPPER, the description length of a rule set is defined as the sum of the model description length of each rule plus the exceptions description length of the whole rule set:\n \\operatorname{MDL}(\\mathcal{R}) = \\sum_{R_{i} \\in \\mathcal{R}} \\operatorname{MDL}_{M}(R_{i}) + \\operatorname{MDL}_{E}(\\mathcal{R}) \nModel description length of each rule calculates how many bits are needed to encode a rule:\n \\operatorname{MDL}_{M}(R) = 0.5(k\\log_{2}\\frac{1}{p} + (n - k) \\log_2\\frac{1}{1 - p} + \\lVert k \\rVert) \nwhere k is the number of features in the rule, n is the number of all features, and p = \\frac{k}{n}. \\lVert k \\rVert = \\log_{2}(k) is the number of bits required to encode the number k. The 0.5 factor is to “account for possible redundancies”.\nExceptions description length evaluates the errors of the rule set on a given dataset:\n \\operatorname{MDL}_{E}(\\mathcal{R}) = \\log_{2}{P(\\mathcal{R}) \\choose \\mathit{FP}(\\mathcal{R})} + \\log_{2}{N(\\mathcal{R}) \\choose \\mathit{FN}(\\mathcal{R})}\nwhere P(\\mathcal{R}) (N(\\mathcal{R})) is the number of positive (negative) instances covered by the rule set \\mathcal{R} and \\mathit{FP}(\\mathcal{R}) (\\mathit{FN}(\\mathcal{R})) is the number of false positives (false negatives) covered by the rule set \\mathcal{R}."
  },
  {
    "objectID": "Notes/RIPPER.html#ripper",
    "href": "Notes/RIPPER.html#ripper",
    "title": "RIPPER",
    "section": "## RIPPER",
    "text": "## RIPPER\nTODO: Many of the implementation details are from this public Github implementation, since the original paper doesn’t elaborate on how exactly they are implemented.\n\nRIPPER = IREP* + a post-processing optimization\nRIPPER further improves on IREP* by post-pruning the rules generated by IREP*.\n\nFunction: RIPPER.\nInput: a training set \\mathcal{D}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Run IREP* on \\mathcal{D} to get \\mathcal{R}. 1. \\mathcal{R}_{o} = Optimize(\\mathcal{D}, \\mathcal{R}). Note that \\mathcal{D} here is a copy of the original training set (no removal from IREP). 1. While there are still positive instances in \\mathcal{D}: 1. R = GrowRule*(\\mathcal{D}). 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. Add R to \\mathcal{R}_{o}. 1. Return \\mathcal{R}_{o}.\n\nThe optimize procedure proposes 2 more versions of the rule for each rule learned from IREP* and select the best version using MDL metric to add to the final rule set.\n\nFunction: Optimize.\nInput: a training set \\mathcal{D} and a rule set \\mathcal{R}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Initialize an empty rule set \\mathcal{R}_{o}. 1. For each R_{i} \\in \\mathcal{R} in the order R_{i} is learned in \\mathcal{R}: 1. Randomly choose 2/3 from \\mathcal{D} as the growing set \\mathcal{G} and the rest 1/3 becomes the pruning set \\mathcal{P}. 1. Grow the replacement rule from scratch using FOIL’s information gain: \\hat{R}_{i} = GrowRule(\\mathcal{G}). 1. Form a new rule set by replacing R_{i} in \\mathcal{R} with \\hat{R}_{i}: \\mathcal{\\hat{R}} = \\{R_{1}, \\dots, \\hat{R}_{i}, \\dots, R_{n}}. 1. Prune the replacement rule: \\hat{R}_{i} = PruneRule(\\mathcal{P}, \\hat{R}_{i}), but using accuracy of the rule set \\mathcal{\\hat{R}} instead of Rule-Value metric as the maximizing objective. 1. Update the \\hat{R}_{i} in \\hat{\\mathcal{R}} with the pruned version. 1. Grow the revision rule from the current rule R_{i} using FOIL’s information gain: \\bar{R}_{i} = GrowRule(\\mathcal{G}). 1. Form a new rule set by replacing R_{i} in \\mathcal{R} with \\bar{R}_{i}: \\mathcal{\\bar{R}} = \\{R_{1}, \\dots, \\bar{R}_{i}, \\dots, R_{n}}. 1. Prune the revision rule: \\bar{R}_{i} = PruneRule(\\mathcal{P}, \\bar{R}_{i}), but using accuracy of the rule set \\mathcal{\\bar{R}} instead of Rule-Value metric as the maximizing objective. 1. Update the \\bar{R}_{i} in \\bar{\\mathcal{R}} with the pruned version. 1. The best rule from the 3 versions is the one whose corresponding rule set has the smallest minimum total description length: R_{i}^{best} = \\arg \\min_{R \\in \\{R_{i}, \\hat{R}_{i}, \\bar{R}_{i}\\}} \\operatorname{MTDL} (\\{R_{1}, \\dots, R, \\dots, R_{n}\\}). 1. Add R_{i}^{best} to \\mathcal{R}_{o}. 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. If there is no positive instances in \\mathcal{D}: break. 1. Return \\mathcal{R}_{o}.\n\n[Minimum total description length]: Given a rule set, the minimum total description length is the description length of the rule set after deleting the rules that increase the total description length of the rule set.\n\n\nRIPPER2 and RIPPERk\nRIPPER2 is just running Optimize again on the output of RIPPER, while RIPPERk is to run RIPPER k times.\n\nFunction: RIPPER2.\nInput: a training set \\mathcal{D}.\nOutput: a optimized rule set \\mathcal{R}_{o}. 1. Run RIPPER on \\mathcal{D} to get \\mathcal{R}. 1. \\mathcal{R}_{o} = Optimize(\\mathcal{D}, \\mathcal{R}). Note that \\mathcal{D} here is a copy of the original training set (no removal from IREP). 1. While there are still positive instances in \\mathcal{D}: 1. R = GrowRule*(\\mathcal{D}). 1. Remove instances that are covered by R_{i}^{best} from \\mathcal{D}. 1. Add R to \\mathcal{R}_{o}. 1. Return \\mathcal{R}_{o}."
  },
  {
    "objectID": "Notes/CG Decision Rules.html",
    "href": "Notes/CG Decision Rules.html",
    "title": "CG Decision Rules",
    "section": "",
    "text": "09-19-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/CG Decision Rules.html#problem-formulation",
    "href": "Notes/CG Decision Rules.html#problem-formulation",
    "title": "CG Decision Rules",
    "section": "Problem formulation",
    "text": "Problem formulation\nThese paper try to find a decision rule set as a binary classifier by fomulating a integer programming problem.\n\nA decision rule set is a binary classifier that only takes binary inputs.\n\nThe decision rule set is in the form of DNF (OR-of-ANDs). Each rule is an AND gate of part of the binary inputs and all rules are connected using an OR gate.\nIf a instance satisfies any of the rules, it is predicted to be positive. Otherwise, the instance is predicted to be negative.\n\nLearning a set of decision rule set can be formulated as a large mixed integer programming (MIP) problem.\n\nA rule is a subset of input features. If there are m binary input features, a rule can be represented by a binary vector of length m (1 means the feature is included in the rule, 0 otherwise).\nLet \\mathcal{K} denotes all possible DNF rules. \\mathcal{K} has in total 2^{m} - 1 of rules (a vector of all 0 is not a rule).\nGiven a training set that has a set of positive instances \\mathcal{P} and a set of negative instances \\mathcal{N}, we can minimize a loss function that characterize the classification error."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#loss",
    "href": "Notes/CG Decision Rules.html#loss",
    "title": "CG Decision Rules",
    "section": "0-1 Loss",
    "text": "0-1 Loss\nThe 0-1 loss is the most direct loss the counts the numbers of false positives and false negatives.\n\n\\begin{aligned}\n\\min \\quad & \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} + \\sum_{i \\in \\mathcal{N}} \\mathcal{L}_{i} \\\\\n\\text{s.t. } \\quad & \\mathcal{L}_{i} + \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\geq 1, && i \\in \\mathcal{P} \\\\\n& w_{k} \\leq \\mathcal{L}_{i}, && i \\in \\mathcal{N}, k \\in \\mathcal{K}_{i} \\\\\n& w_{k}, \\mathcal{L}_{i} \\in \\{0, 1\\}, && k \\in \\mathcal{K}, i \\in \\mathcal{P} \\cup \\mathcal{N} \\\\\n\\end{aligned}\n\nApart from \\mathcal{K}, \\mathcal{P}, \\mathcal{N} defined above, some of new terms are defined:\n\nLet w_{k} \\in \\{0, 1\\} be a variable of if rule k \\in \\mathcal{K} is selected in the rule set.\nLet \\mathcal{L}_{i} \\in \\{0, 1\\} be a variable indicating if an instance i \\in \\mathcal{P} \\cup \\mathcal{N} is misclassified.\nLet \\mathcal{K}_{i} \\subset \\mathcal{K} be the set of rules met by the instance i.\n\nExplanation:\n\nObjective \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} is the number of false positives and \\sum_{i \\in \\mathcal{N}} \\mathcal{L}_{i} is the number of false negatives. Thus the loss function above is directly minimizing the classification errors.\nConstraint 1 defines the misclassification of a positive instance (false negative) by forcing \\mathcal{L}_{i} to take value 1 if no rule that is satisfied by the instance i \\in \\mathcal{P} is in the rule set.\nConstraint 2 defines the misclassification of a negative instance (false positive) by forcing \\mathcal{L}_{i} to take value 1 if any rule that is satisfied by the instance i \\in \\mathcal{N} is in the rule set.\nConstraint 3 defines types of each variables.\n\nThe first constraint has \\lvert \\mathcal{P} \\rvert number of equations while the second one has at most \\lvert \\mathcal{N} \\mathcal{K} \\rvert (assuming each negative instance satisfies all possible rules) equations. The second constraint is very expensive and can be avoided using the hamming loss introduced below."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#hamming-loss",
    "href": "Notes/CG Decision Rules.html#hamming-loss",
    "title": "CG Decision Rules",
    "section": "Hamming Loss",
    "text": "Hamming Loss\nHamming loss also considers characterizing the numbers of false positives and false negatives. While Hamming loss incurs the same loss for each false negative, Hamming loss considers the loss to be the number of selected rules a negative instance satisfy for each false positive.\n\n\\begin{aligned}\n\\min \\quad & \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} + \\sum_{i \\in \\mathcal{N}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\\\\n\\text{s.t. } \\quad & \\mathcal{L}_{i} + \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\geq 1, && i \\in \\mathcal{P} \\\\\n& w_{k}, \\mathcal{L}_{i} \\in \\{0, 1\\}, && k \\in \\mathcal{K}, i \\in \\mathcal{P} \\cup \\mathcal{N} \\\\\n\\end{aligned}\n\nExplanation:\n\nObjective \\sum_{i \\in \\mathcal{P}} \\mathcal{L}_{i} counts the number of false positives (same as that of the 0-1 loss), while \\sum_{i \\in \\mathcal{N}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} counts the number of selected rules of each negative instance.\nConstraint 1 defines a false negative (same as the 1st constraint of 0-1 loss).\nConstraint 2 defines types of each variables (same as the 3rd constraint of 0-1 loss).\n\nHamming loss eliminate the 2nd constraint in 0-1 loss by converting the counting of the false positives to be the hamming distances between true positives and false positives, thus reducing the problem size."
  },
  {
    "objectID": "Notes/CG Decision Rules.html#fairness-metrics-and-constraints",
    "href": "Notes/CG Decision Rules.html#fairness-metrics-and-constraints",
    "title": "CG Decision Rules",
    "section": "Fairness metrics and constraints",
    "text": "Fairness metrics and constraints\n\nGiven a set of protected features (e.g. sex, red), the training instances can be divided into different groups \\mathcal{G} (e.g. 4 groups: \\mathcal{G} = {sex=male, sex=female, red=True, red=False}).\nTwo types of fairness metrics:\n\nEquality of opportunity requires the false negative rate to be equal for all groups.\nEqualized odds requires both the false positive and false negative rate to be equal for all groups. Equalized odds is stricter version of equality of opportunity.\n\nThe maximum violation can be used to measure the unfairness of the classifier:  \\Delta(d) = \\max_{g, g' \\in \\mathcal{G}} \\lvert \\mathbb{P}(d(X)=-1 \\mid Y=1, G=g) - \\mathbb{P}(d(x)=-1 \\mid Y=1, G=g') \\rvert This equation gives the maximum difference of the equality of opportunity (false negative rate) between the instances in any two groups g and g'.\nAssuming \\mathcal{G} = \\{1,2\\} (there are only two groups), we can incorporate the following constraints to the Hamming loss to impose the fairness:\n\nEquality of opportunity (false negative rate):  \\frac{1}{\\lvert \\mathcal{P}_{1} \\rvert} \\sum_{i \\in \\mathcal{P}_{1}} \\mathcal{L}_{i} - \\frac{1}{\\lvert \\mathcal{P}_{2} \\rvert} \\sum_{i \\in \\mathcal{P}_{2}} \\mathcal{L}_{i} \\leq \\epsilon_{1}   \\frac{1}{\\lvert \\mathcal{P}_{2} \\rvert} \\sum_{i \\in \\mathcal{P}_{2}} \\mathcal{L}_{i} - \\frac{1}{\\lvert \\mathcal{P}_{1} \\rvert} \\sum_{i \\in \\mathcal{P}_{1}} \\mathcal{L}_{i} \\leq \\epsilon_{1} \nEquality odds (false positive rate):  \\frac{1}{\\lvert \\mathcal{N}_{1} \\rvert} \\sum_{i \\in \\mathcal{N}_{1}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} - \\frac{1}{\\lvert \\mathcal{N}_{2} \\rvert} \\sum_{i \\in \\mathcal{N}_{2}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\leq \\epsilon_{2}   \\frac{1}{\\lvert \\mathcal{N}_{2} \\rvert} \\sum_{i \\in \\mathcal{N}_{2}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} - \\frac{1}{\\lvert \\mathcal{N}_{1} \\rvert} \\sum_{i \\in \\mathcal{N}_{1}} \\sum_{k \\in \\mathcal{K}_{i}} w_{k} \\leq \\epsilon_{2}  where in both cases, \\mathcal{P}_{1} is the positives instances that are in group 1 (similar for \\mathcal{P}_{2}, \\mathcal{N}_{1}, \\mathcal{N}_{2}) and \\epsilon_{1}, \\epsilon_{2} are two constants that bound the maximum allowed unfairness for false positives and false negatives."
  },
  {
    "objectID": "Notes/L0 Regularization.html",
    "href": "Notes/L0 Regularization.html",
    "title": "L0 Regularization",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\nplt.style.use(\"ggplot\")\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef hard_sigmoid(x):\n    return min(1, max(0, x))\n\ndef logit_dist():\n    u = np.random.random()\n    logit = np.log(u) - np.log(1 - u)\n    \n    return logit\n\ndef binary_concrete(loc, temp):\n    logit = logit_dist()\n    bc = sigmoid((logit + loc) / temp) \n    \n    return bc\n\ndef stretch_binary_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n    bc = binary_concrete(loc, temp)\n    stretch_bc = bc * (zeta - gamma) + gamma\n    \n    return stretch_bc\n\ndef hard_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n    stretch_bc = stretch_binary_concrete(loc, temp, gamma, zeta)\n    hc = hard_sigmoid(stretch_bc) \n\n    return hc\n    \ndef plot_probability(list_samples, bins=100, **kwargs):\n    plt.figure(figsize=(16, 8))\n    for samples in list_samples:\n        weights = np.ones_like(samples) / len(samples)\n        plt.hist(samples, weights=weights, bins=bins, alpha=0.5, **kwargs)\n05-03-2021 (Updated 06-02-2022)\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/L0 Regularization.html#problem-formulation",
    "href": "Notes/L0 Regularization.html#problem-formulation",
    "title": "L0 Regularization",
    "section": "Problem formulation",
    "text": "Problem formulation\nGiven a vector x of length n (matrix can also be seen as a vector by stacking up the rows/cols), the common vector norms are:\n\nL_{0} norm:\n \\sum_{i=1}^{n} \\mathbb{1}[x_{i} \\neq 0] \nL_{1} norm:\n \\sum_{i=1}^{n} \\lvert x_{i} \\rvert \nwhich is also called ridge regularization in neural network.\nL_{2} norm:\n \\sum_{i=1}^{n} x_{i}^2 \nwhich is also called lasso regularization in neural network.\nL_{\\infty} norm:\n \\max_{i=1}^{n} \\lvert x_i \\rvert \n\nThe normal way to prune the edges of the neural network is to use L_{1} or L_{2} regularization to drive weights to near 0 (not exactly 0), and then directly set all weights that are less than threshold to 0.\n\nL_{0} is not used because the operation of counting the number of 0s is not differentiable.\nHowever, L_{0} regularization is still desired because it won’t affect the magnitude of the weights in the pruning process."
  },
  {
    "objectID": "Notes/L0 Regularization.html#general-recipe-of-l_0-regularization",
    "href": "Notes/L0 Regularization.html#general-recipe-of-l_0-regularization",
    "title": "L0 Regularization",
    "section": "General recipe of L_{0} regularization",
    "text": "General recipe of L_{0} regularization\nThe loss function used to train a neural network with L_{0} regularization is:\n \\mathcal{L}(f(x, \\theta), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[\\theta_{i} \\neq 0] \nwhere\n\n\\mathcal{L} is a standard loss function (cross-entropy loss, softmax)\n\\theta are the parameters in the network\nx, y are training instances\n\\lambda is a hyper-parameter that balance loss and the regularization.\n\nIf we attach a trainable binary random variable z_{i} to each element of the model parameter \\theta_{i}, then the weights used in the feed-forward operation of the neural network can be replaced by \\theta \\odot z. The loss function then becomes:\n \\mathcal{L}(f(x, \\theta \\odot z), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[z_{i} \\neq 0]\nwhere\n\nz \\in \\{0, 1\\}^{\\lvert \\theta \\rvert} is randomly sampled in each forward propagation according to some distribution.\n\\odot corresponds to the elementwise product.\n\nIf we assume each z_{i} as a binary random variable with a Bernoulli distribution that has a parameter \\pi_{i}, i.e. z_{i} = \\mathrm{Bern}(\\pi_{i}), then the loss function becomes:\n \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} \nwhere \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} [\\cdot] gives an expectation value of a function that has a Bernoulli distribution z as the input.\nThe reformulation of the above loss function can be established because\n\nSince the minimum of a function is upper bounded by the expectation of the function, minimizing \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] is the same as minimizing the upper bound of \\mathcal{L}(f(x, \\theta \\odot z), y).\nAccording to the definition of the Bernoulli distribution, \\pi gives the probability of z being 1 (non zero). Thus, minimizing \\pi is to increase the probability of z being 0.\n\nIn the equation above,\n\n\\pi_{i} is a parameter that we want to be learned using gradient descent.\nThus, the second term \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} can be directly minimized to regularize \\pi because gradient of the second term w.r.t to pi_{i} can be easily calculated.\nHowever, the first term \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] is still problematic because z as a categorical random variable cannot be differentiated with respect to \\pi."
  },
  {
    "objectID": "Notes/L0 Regularization.html#hard-concrete-distribution",
    "href": "Notes/L0 Regularization.html#hard-concrete-distribution",
    "title": "L0 Regularization",
    "section": "Hard concrete distribution",
    "text": "Hard concrete distribution\n\nBinary Concrete distribution\nThe binary concrete distribution can be seen as a continuous approximation of the Bernoulli distribution. It has 2 parameters:\n\nlocation \\alpha: similar to the probability parameter of the Bernoulli distribution.\ntemperature \\beta: it controls how similar the binary concrete distribution is with the Bernoulli distribution.\n\nUsing the the reparametrization trick, the binary concrete distribution s can be represented as:\n s = \\operatorname{sigmoid} \\left( \\frac{\\alpha + l}{\\beta} \\right) \nwhere l is a sample from the logistic distribution.\n\ndef plot_logit(size=100000, **kwargs):\n    logit_samples = [logit_dist() for _ in range(size)]\n    plot_probability([logit_samples])\n    \ninteract(plot_logit);\n\n\n\n\n\ndef plot_bc(size=100000, **kwargs):\n    ber_samples = np.random.binomial(1, 0.5, size=size)\n    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([ber_samples, bc_samples])\n    \ninteract(plot_bc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\n\n\nFrom binary concrete to hard concrete\nWe cannot use s (binary concrete distribution) to directly replace z (Bernoulli distribution) - The range of s is (0, 1) and never touches 0 or 1. - However, we want z to be either 0 or 1.\nA simple trick to solve this problem is\n\nFirst “stretch” the binary concrete distribution from interval (0, 1) to interval (\\gamma, \\zeta) with \\gamma &lt; 0 and \\zeta &gt; 1\n \\bar{s} = s(\\zeta - \\gamma) + \\gamma \nThen clip the stretch binary concrete distribution into the range [0, 1]\n \\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1) \nwhere\n\n\\mathrm{clip}(x, \\mathrm{min}, \\mathrm{max}) means to clip x between the range [\\mathrm{min}, \\mathrm{max}].\n\\bar{z} is a random variable that follows the hard concrete distribution and it can be used to replace z.\n\n\n\ndef plot_sbc(size=100000, **kwargs):\n    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([bc_samples, sbc_samples])\n    \ninteract(plot_sbc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\n\ndef plot_hc(size=100000, **kwargs):\n    hc_samples = [hard_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n    plot_probability([hc_samples, sbc_samples])\n    \ninteract(plot_hc, loc=(-3, 3), temp=(0.001, 1));\n\n\n\n\nAlso, we cannot use \\alpha (location parameter of binary concrete distribution) to replace \\pi (probability parameter of Bernoulli distribution) in the regularization term of the loss function. - Remember that the regularization term above measures the sum of the probabilities of the z (Bernoulli distribution) being non-zero.\n$$ \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} $$\nMeasuring the probability of \\bar{z} of being non-zero is the same as measuring the probability of \\bar{s} being positive.\n\nBecause all negative values of \\bar{s} is clipped to be 0.\nSince we know that the total probability of a random variable being all values is 1, the probability of \\bar{s} being positive is written as:\n \\textrm{q}(\\bar{s} &gt; 0 | \\phi) = 1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi) \nwhere\n\n\\textrm{Q}(\\cdot) is the cumulative density function (CDF)\n\\mathrm{Q}(s \\leq 0 | \\phi) gives the probability of s being negative.\n\n\nThus, the loss function above can be rewritten using the hard concrete distribution by:\n \\mathbb{E}_{\\bar{s}} \\big[ \\mathcal{L}(f(x, \\theta \\odot \\bar{z}), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} (1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi)) \nThe loss function is “fully” differentiable with respect to \\alpha\n\n\\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1) and \\bar{s} is a differentiable function with respect to \\alpha.\n\\mathrm{Q}(\\bar{s} \\leq 0 | \\phi) can be written as a differentiable function with respect to \\alpha."
  },
  {
    "objectID": "Notes/Confident Learning.html",
    "href": "Notes/Confident Learning.html",
    "title": "Confident Learning",
    "section": "",
    "text": "This page contains my reading notes on\nNotations:\nThe procedure needs 2 inputs:"
  },
  {
    "objectID": "Notes/Confident Learning.html#five-methods-to-identify-instances-with-noisy-labels",
    "href": "Notes/Confident Learning.html#five-methods-to-identify-instances-with-noisy-labels",
    "title": "Confident Learning",
    "section": "Five Methods to identify instances with noisy labels",
    "text": "Five Methods to identify instances with noisy labels\n\n1. CL baseline 1: C_{confusion}\nThe instance is considered to have the noisy label if its given label is different from the label with largest predicted probability.\n\n\n2. CL method 2: C_{\\tilde{y}, y^{*}}\nIn this method, a matrix called confident joint C_{\\tilde{y}, y^{*}} will be calculated using \\hat{\\mathbf{P}} and \\tilde{\\mathbf{y}}.\n\n\n\nC_{\\tilde{y}, y^{*}}\ny^{*} = 0\ny^{*} = 1\ny^{*} = 2\n\n\n\n\n\\tilde{y} = 0\n100\n40\n20\n\n\n\\tilde{y} = 1\n56\n60\n0\n\n\n\\tilde{y} = 2\n32\n12\n80\n\n\n\nTo calculate this matrix:\n\nFor each label j, calculate the average predicted probability t_{j} using \\hat{\\mathbf{P}}.\nFor each instance \\mathbf{x}_{k} with the given label i in the training set, the entry at row i and column j of the confident joint matrix C_{\\tilde{y}=i, y^{*}=j} will be added 1, where the true label j is the one that has the largest predicted probability among all the labels whose predicted probabilities are above the respected t_{j}.\n\nThis basically means that the true label for a given instance is the label whose predicted probability by a model is larger than the average predicted probability.\nIf there are more than one such labels, chose the one that has the largest predicted probability.\nIt is possible that no such label exists, and thus the instance won’t be counted in the matrix.\n\n\nThus, each entry in C_{\\tilde{y}, y^{*}} is corresponding to a set of training instances.\nAll instances that fall in the off-diagonal of the C_{\\tilde{y}, y^{*}} are considered to have noisy labels.\n\n\n3. CL method 3: Prune by Class (PBC)\nIn this method and all methods below, another matrix called Estimate of joint \\hat{Q}_{\\tilde{y}, y^{*}} will be calculated using C_{\\tilde{y}, y^{*}}.\n\n\n\n\n\n\n\n\n\n\\hat{Q}_{\\tilde{y}, y^{*}}\ny^{*} = 0\ny^{*} = 1\ny^{*} = 2\n\n\n\n\n\\tilde{y} = 0\n0.25\n0.1\n0.05\n\n\n\\tilde{y} = 1\n0.14\n0.15\n0\n\n\n\\tilde{y} = 2\n0.08\n0.03\n0.2\n\n\n\n\\hat{Q}_{\\tilde{y}, y^{*}} basically is the normlized C_{\\tilde{y}, y^{*}}: each entry in C_{\\tilde{y}, y^{*}} is divided by the total number of training instances.\nFor each class i, the a number of instances with lowest predicted probabilities for label i are considered to have noisy labels, where a is calculated as the product of n and the sum of off-diagonal entries on row i of \\hat{Q}_{\\tilde{y}, y^{*}}.\n\n\n4. CL method 4: Prune by Noise Rate (PBNR)\nFor each off-diagonal entry in \\hat{Q}_{\\tilde{y}, y^{*}}, the n \\times \\hat{Q}_{\\tilde{y}=i, y^{*}=j} number of instances with largest margin are considered to have noisy labels, where the margin of an instance \\mathbf{x}_{k} with respect to given label i and true label j is \\hat{\\mathbf{P}}_{k, j} - \\hat{\\mathbf{P}}_{k, i}.\n\n\n5. CL method 5: C + NR\nThe instance is considered to have a noisy label if both PBC and PBNR consider it to have a noisy label."
  },
  {
    "objectID": "Notes/MobileNets.html",
    "href": "Notes/MobileNets.html",
    "title": "Mobile Nets",
    "section": "",
    "text": "05-01-2021\nThis page contains my reading notes on"
  },
  {
    "objectID": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary",
    "href": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary",
    "title": "Mobile Nets",
    "section": "Standard 2D convolution layer (Preliminary)",
    "text": "Standard 2D convolution layer (Preliminary)\n\nConvolution layer takes as input M matrices (M input channels) of shape D_i \\times D_i, which can be seen as a 3D volume of size D_i \\times D_i \\times M.\nEach convolution layer consists of N sets of learnable weights (N filters or N output channels), each of which corresponds to a learnable bias. Each set of learnable weights (a filter) is a small 3D volume with size D and same depth as the input volume M, i.e. M matrices of shape D \\times D.\nThe output activation of the convolution layer has the N matrices of shape D_o \\times D_o, which can also be seen as a 3D volume of size D_o \\times D_o \\times M.\n\nD_o is computed as:  D_o = \\frac{D_i - D}{S} + 1 \n\nIn the equation above, S is stride, which tells how many pixels the filter moves each time. The input matrices will usually be zero-padded to control the output size.\nThe animation below shows exactly how the output of a convolution layer is computed."
  },
  {
    "objectID": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary-1",
    "href": "Notes/MobileNets.html#standard-2d-convolution-layer-preliminary-1",
    "title": "Mobile Nets",
    "section": "Standard 2D convolution layer (Preliminary)",
    "text": "Standard 2D convolution layer (Preliminary)\n\nConvolution layer takes as input M matrices (M input channels) of shape D_i \\times D_i, which can be seen as a 3D volume of size D_i \\times D_i \\times M.\nEach convolution layer consists of N sets of learnable weights (N filters or N output channels), each of which corresponds to a learnable bias. Each set of learnable weights (a filter) is a small 3D volume with size D and same depth as the input volume M, i.e. M matrices of shape D \\times D.\nThe output activation of the convolution layer has the N matrices of shape D_o \\times D_o, which can also be seen as a 3D volume of size D_o \\times D_o \\times M.\n\nD_o is computed as:  D_o = \\frac{D_i - D}{S} + 1 \n\nIn the equation above, S is stride, which tells how many pixels the filter moves each time. The input matrices will usually be zero-padded to control the output size.\nThe animation below shows exactly how the output of a convolution layer is computed.\n\n\nfrom IPython.display import IFrame\nIFrame('https://cs231n.github.io/assets/conv-demo/index.html', width=792, height=700)"
  },
  {
    "objectID": "Notes/MobileNets.html#depthwise-separable-convolution-mobilenetv1",
    "href": "Notes/MobileNets.html#depthwise-separable-convolution-mobilenetv1",
    "title": "Mobile Nets",
    "section": "Depthwise separable convolution (MobileNetV1)",
    "text": "Depthwise separable convolution (MobileNetV1)\n\nInstead of using the traditional convolution layers as the building blocks, MobileNetV1 uses layers called depthwise separable convolutions.\nEach depthwise separable convolution consists of two layers:\n\nDepthwise convolution layer: it has M separate filters of shape D \\times D \\times 1, where the mth filter is applied to the mth input channel. This layer can also be thought to have 1 filter of shape D \\times D \\times M, but the output channels will not be added together. In this way, the output size is D_o \\times D_o \\times M.\nPointwise convolution layer: it has N separate filters of shape 1 \\times 1 \\times M. Each filter is doing a linear combination of the matrices (input channels) of the input volume, and thus N filters will have N matrices (output channels). It takes the output of depthwise convolution as the input, and the output of pointwise convolution has the shape of D_o \\times D_o \\times N, just like the output of a standard convolution layer.\n\nEach filter in a convolution layer both filters and combines inputs in one step. Depthwise separable convolution splits this into two layers: depthwise convolution does filtering while pointwise convolution combines.\nComputational cost comparison:\n\nStandard convolution layer (assuming stride is 1): D_i \\times D_i \\times D \\times D \\times M \\times N\nDepthwise convolution layer (assuming stride is 1): D_i \\times D_i \\times D \\times D \\times M\nPointwise convolution layer: D \\times D \\times M \\times N\nThus the computation cost of depthwise separable convolution is less than that of the standard convolution layer:  \\frac{D_i \\times D_i \\times D \\times D \\times M + D \\times D \\times M \\times N}{D_i \\times D_i \\times D \\times D \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_i^2} \n\n\n\nfrom IPython.display import Image\nImage(filename='./1.png', width=480)"
  },
  {
    "objectID": "Notes/MobileNets.html#inverted-residual-and-linear-bottleneck-block-mobilenetv2",
    "href": "Notes/MobileNets.html#inverted-residual-and-linear-bottleneck-block-mobilenetv2",
    "title": "Mobile Nets",
    "section": "Inverted residual and linear bottleneck block (MobileNetV2)",
    "text": "Inverted residual and linear bottleneck block (MobileNetV2)\n\nThe improvement of this layer over the depthwise separable convolution is based on the following two assumptions:\n\nAlthough Non-linear activations (Relu) can increase representational complexity, it will result in information loss if the input doesn’t have lots of channels.\nActivation maps are able to be encoded in low-dimensional subspaces (less channels).\n\nAn expansion layer is appended to the front of the depthwise separable convolution. If the input has the shape of D_i \\times D_i \\times M, the expansion layer is a pointwise convolution layer that has tM filters of size 1 \\times 1 \\times M, where t is called expansion factor/ratio and should be larger than 1. The output will have the shape D_i \\times D_i \\times tM. The layer expands the low-dimensional (less channels) input activation map to a higher-dimensional (more channels) space suited to non-linear activation functions.\nNon-linear activation functions (Relu6) can only be applied to the high-dimensional activation maps. The last pointwise convolution layer projects the high-dimensional space back into low-dimensional space, so it can only have linear activation functions. Therefore, linear bottleneck block has three layers:\n\nExpansion layer: tM filters of 1 \\times 1 \\times M, Relu6.\nDepthwise convolution layer: 1 filter of D \\times D \\times tM (Dwise), Relu6.\nPointwise convolution layer: M' filters of 1 \\times 1 \\times tM, Linear.\n\nIf the initial and final activation maps of the block are of the same dimensions, a residual connection between the input and output of the block (between expansion layers from two consecutive blocks) is added to aid gradient flow during back-propagation, which is achieved by:\n\nDepthwise convolution stride equals (S = 1).\nInput and output channels are equal (M' = M).\n\nThe paper emphasizes that the connection is added between the expansion layers (inverted residual connection) instead of between depthwise convolution layer (residual connection), so that the connections are created between the low-dimensional activation maps.\n\n\nfrom IPython.display import Image\nImage(filename='./2.png', width=480)\n\n\n\n\n\nfrom IPython.display import Image\ni = Image(filename='./3.png', width=480)\ni"
  },
  {
    "objectID": "Notes/MobileNets.html#squeeze-and-excite-mobilenetv3",
    "href": "Notes/MobileNets.html#squeeze-and-excite-mobilenetv3",
    "title": "Mobile Nets",
    "section": "Squeeze-and-Excite (MobileNetV3)",
    "text": "Squeeze-and-Excite (MobileNetV3)\n\nMobileNetV3 uses the block that modifies the inverted residual block by adding a Squeeze-and-Excite block between the depthwise convolution layer and pointwise convolution layer.\nGiven an input that has the shape D_i \\times D_i \\times M, Squeeze-and-Excite block in the original paper consists of three parts:\n\nSqueeze: apply global average pooling to the input, so that all information in each input channel is “squeezed” into a single channel descriptor. Each descriptor is a single value that summarizes each channel of the input. The output shape of this layer is 1 \\times 1 \\times M.\nExcite: apply two fully connected layers to the output of the Squeeze layer. Use Relu activation function for the first layer and Sigmoid activation function for the second layer. The first layer contains \\frac{M}{r} number of neurons, and thus the output of the first layer reduces the dimension by a reduction ratio r. The second layer contains M number of neurons to restore the output dimension to be the same as the output of the Squeeze layer.\nScale: the M coefficients of the output of the Excite layer are timed with M channels of input of the block to form the output of the block.\n\n\n\nfrom IPython.display import Image\nImage(filename='./4.png', width=960)\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='./5.png', width=480)"
  },
  {
    "objectID": "Notes/index.html",
    "href": "Notes/index.html",
    "title": "Research notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nBirkhoff+\n\n\n\n\n\n\n\nCG Decision Rules\n\n\n\n\n\n\n\nConfident Learning\n\n\n\n\n\n\n\nL0 Regularization\n\n\n\n\n\n\n\nML Q & A\n\n\n\n\n\n\n\nMLIC IMLI\n\n\n\n\n\n\n\nMobile Nets\n\n\n\n\n\n\n\nQuantization Survey\n\n\n\n\n\n\n\nRIPPER\n\n\n\n\n\n\n\nSGD Warm Restarts\n\n\n\n\n\n\n\nSSGD\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Discrete Distributions.html",
    "href": "Knowledge/Probability and Statistics/Discrete Distributions.html",
    "title": "Discrete Distributions",
    "section": "",
    "text": "A discrete distribution is a function of a discrete random variable, which specifies the probability of the values that the random variable can take."
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Discrete Distributions.html#probability-mass-function-pmf",
    "href": "Knowledge/Probability and Statistics/Discrete Distributions.html#probability-mass-function-pmf",
    "title": "Discrete Distributions",
    "section": "Probability mass function (PMF)",
    "text": "Probability mass function (PMF)\n(bernoulli-distribution)= ## Bernoulli distribution\nA random variable X \\in \\{0, 1\\} follows the Bernoulli distribution\n X \\sim \\mathrm{Ber}(p) \\quad p \\in [0, 1], \nif X takes value 1 (success) with probability p and 0 (failure) with probability 1 - p.\n \\mathbb{P}_{X}(x) = p^{x} (1 - p)^{1 - x} \n \\mathbb{E}_{X}[x] = p \n \\mathbb{V}_{X}(x) = p (1 - p) \n(binomial-distribution)= ## Binomial distribution\nA random variable $X {0, , n} $ follows the binomial distribution\n X \\sim \\mathrm{Bin}(n, p) \\quad n \\in \\mathbb{N} \\quad p \\in [0, 1], \nif X is the sum of the results of (or number of successes in) n independent and identically distributed Bernoulli trials with probability p.\n \\mathbb{P}_{X}(x) = {n \\choose x} p^{x} (1 - p)^{n - x} \n \\mathbb{E}_{X}[x] = np \n \\mathbb{V}_{X}(x) = np(1 - p) \n(geometric-distribution)= ## Geometric distribution\nA random variable X \\in \\{ 1, 2, \\dots\\} follows the geometric distribution\n X \\sim \\mathrm{Geo}(p) \\quad p \\in [0, 1], \nif X is the number of independent Bernoulli trials with parameter p up to and including first success.\n \\mathbb{P}_{X}(x) = p (1 - p)^{x - 1} \n \\mathbb{E}_{X}[x] = \\frac{1}{p} \n \\mathbb{V}_{X}(x) = \\frac{1}{p^2} \n(negative-binomial-distribution)= ## Negative binomial distribution\nA random variable X \\in \\{ r, r + 1, \\dots \\} follows the negative binomial distribution\n X \\sim \\mathrm{NegBio}(r, p) \\quad r \\in \\mathbb{N} \\quad p \\in [0, 1], \nif X is the number of independent Bernoulli trials with parameter p up to and including the rth success.\n \\mathbb{P}_{X}(x) = {x - 1 \\choose r - 1} p^{r} (1 - p)^{x - r} \n \\mathbb{E}_{X}[x] = \\frac{r}{p} \n \\mathbb{V}_{X}(x) = \\frac{r (1 - p)}{(1 - p)^2} \n(poisson-distribution)= ## Poisson distribution\nA random variable X \\in \\mathbb{N} follows the Poisson distribution\n X \\sim \\mathrm{Poi}(\\lambda) \\quad \\lambda &gt; 0, \nif X is the number of events that occur in one unit of time independently with rate \\lambda per unit time.\n \\mathbb{P}_{X}(x) = e^{-\\lambda} \\frac{\\lambda^{x}}{x!} \n \\mathbb{E}_{X}[x] = \\lambda \n \\mathbb{V}_{X}(x) = \\lambda"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html",
    "title": "Joeyonng",
    "section": "",
    "text": "A random variable X with mean \\mu is called Sub-Gaussian with parameter \\sigma (X \\sim SubGau (\\sigma)) if\n\n\\mathbb{E}_{X} \\left[\n    e^{s (X - \\mu)}\n\\right] \\leq \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 }\n\\right].\n\nThe definition can equivalently be expressed in terms of bounds on the tail of X. Let X \\sim SubGau (\\sigma) and \\mu = \\mathbb{E}_{X} [X]. Then for any t &gt; 0,\n\n\\mathbb{P}_{X} \\left(\n    X - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\n:::{prf:proof} Bound of Sub-Gaussian :label: bound-of-sub-gaussian :class:dropdown\nFirst use Chernoff bound to derive\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s t}\n}\n\\\\\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right].\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t \\right] is a convex function\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n& = 0\n\\\\\n(\\sigma^{2} s - t) \\exp \\left[\n    \\frac{ \\sigma^{2} s^{2} }{ 2 } - t s\n\\right]\n& = 0\n\\\\\ns\n& = \\frac{ t }{ \\sigma^{2} }.\n\\end{aligned}\n\nPlug s = \\frac{ t }{ \\sigma^{2} } back and we get derive the result\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ t^{2} }{ 2 \\sigma^{2} } - \\frac{ t^{2} }{ \\sigma^{2} }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html#sub-gaussian-random-variables",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html#sub-gaussian-random-variables",
    "title": "Joeyonng",
    "section": "",
    "text": "A random variable X with mean \\mu is called Sub-Gaussian with parameter \\sigma (X \\sim SubGau (\\sigma)) if\n\n\\mathbb{E}_{X} \\left[\n    e^{s (X - \\mu)}\n\\right] \\leq \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 }\n\\right].\n\nThe definition can equivalently be expressed in terms of bounds on the tail of X. Let X \\sim SubGau (\\sigma) and \\mu = \\mathbb{E}_{X} [X]. Then for any t &gt; 0,\n\n\\mathbb{P}_{X} \\left(\n    X - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\n:::{prf:proof} Bound of Sub-Gaussian :label: bound-of-sub-gaussian :class:dropdown\nFirst use Chernoff bound to derive\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{X} [e^{s x}]\n}{\n    e^{s t}\n}\n\\\\\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right].\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t \\right] is a convex function\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n& = 0\n\\\\\n(\\sigma^{2} s - t) \\exp \\left[\n    \\frac{ \\sigma^{2} s^{2} }{ 2 } - t s\n\\right]\n& = 0\n\\\\\ns\n& = \\frac{ t }{ \\sigma^{2} }.\n\\end{aligned}\n\nPlug s = \\frac{ t }{ \\sigma^{2} } back and we get derive the result\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x \\geq t)\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sigma^{2} }{ 2 } - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ t^{2} }{ 2 \\sigma^{2} } - \\frac{ t^{2} }{ \\sigma^{2} }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - t^{2} }{ 2 \\sigma^{2} }\n\\right].\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html#hoeffdings-inequality",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html#hoeffdings-inequality",
    "title": "Joeyonng",
    "section": "Hoeffding’s inequality",
    "text": "Hoeffding’s inequality\nHoeffding’s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount.\n\nHoeffding’s Lemma\nLet X be a bounded random variable with a \\leq X \\leq b and \\mu = \\mathbb{E}_{X} [x]. Then for all s &gt; 0,\n\n\\mathbb{E}_{X} [e^{s (x - \\mu)}] \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right].\n\n:::{prf:proof} Hoeffding’s Lemma :label: hoeffding’s-lemma :class:dropdown\nSince f (x) = e^{a x} is a convex function, the definition of the convex function states that\n\ne^{s x} \\leq \\frac{ (x - a) e^{s b} }{ b - a } + \\frac{ (b - x) e^{s a} }{ b - a }.\n\nTaking the expectation on the both ends and the fact that \\mathbb{E}_{X} (x) = 0,\n\n\\begin{aligned}\n\\mathbb{E}_{X} [e^{s x}]\n& \\leq \\mathbb{E}_{X} \\left[\n    \\frac{ (x - a) e^{s b} }{ b - a } + \\frac{ (b - x) e^{s a} }{ b - a }\n\\right]\n\\\\\n& = \\frac{\n    e^{s b} \\mathbb{E}_{X} [x] - a e^{s b} + b e^{s a} - e^{s a} \\mathbb{E}_{X} [x]\n}{\n    b - a\n}\n\\\\\n& = \\frac{ b e^{s a} - a e^{s b} }{ b - a }.\n\\end{aligned}\n\nNow we will prove that\n\n\\frac{ b e^{s a} - a e^{s b} }{ b - a } \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right]\n\nwhich is same as proving\n\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right] \\leq \\frac{ s^{2} (b - a)^{2} }{ 8 }.\n\nBy taking p = \\frac{ b }{ b - a } and 1 - p = -\\frac{ a }{ b - a }\n\n\\begin{aligned}\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right]\n& = \\log \\left[\n    \\frac{ b e^{s a} }{ b - a } - \\frac{ a e^{s b} }{ b - a }\n\\right]\n\\\\\n& = \\log \\left[\n    p e^{s a} + (1 - p) e^{s b}\n\\right]\n\\\\\n& = \\log \\left[\n    e^{s a} e^{-s a} (p e^{s a} + (1 - p) e^{s b})\n\\right]\n\\\\\n& = s a + \\log \\left[\n    (p + (1 - p) e^{s b - s a})\n\\right]\n\\end{aligned}\n\nand taking u = (b - a) s, we can get a function\n\n\\begin{aligned}\n\\phi (u)\n& = s a + \\log \\left[\n    (p + (1 - p) e^{s b - s a})\n\\right]\n\\\\\n& = (p - 1) u + \\log \\left[\n    (p + (1 - p) e^{u})\n\\right].\n\\end{aligned}\n\nThis function can be approximated using Taylor series until the second order term at point a = 0,\n\n\\phi (u) = \\phi (0) + \\phi' (0) x + \\frac{ \\phi' (0) }{ 2 } x^{2},\n\nwhere\n\n\\phi' (u) = (p - 1) + \\frac{ (1 - p) e^{u} }{ p + (1 - p) e^{u} } = 0,\n\nand\n\n\\phi'' (u) = \\frac{ p (1 - p) e^{u} }{ (p + (1 - p) e^{u})^{2} } = 0.\n\nSince \\phi (0) = 0, \\phi' (0) = 0 and\n\n\\phi'' (0) = \\frac{ p (1 - p) }{ (p + (1 - p))^{2} }\n\nreaches its maximum of 0.25 at p = 0.5, we can derive\n\n\\log \\left[\n    \\frac{ b e^{s a} - a e^{s b} }{ b - a }\n\\right] = \\phi (u) \\leq \\frac{ u^{2} }{ 8 } = \\frac{ s^{2} (b - a)^{2} }{ 8 }.\n\nTherefore, we can prove the lemma\n\n\\mathbb{E}_{X} [e^{s x}] \\leq \\frac{ b e^{s a} - a e^{s b} }{ b - a } \\leq \\exp \\left[\n    \\frac{ s^{2} (b - a)^{2} }{ 8 }\n\\right].\n\n:::\nEquivalently, Hoeffding’s lemma states that any random variable X \\in [a, b] is a subGaussian variable\n\nX \\in SubGau \\left(\n    \\frac{ (b - a)^{2} }{ 4 }\n\\right).\n\n\n\nHoeffding’s inequality\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a_{i}, b_{i}], \\forall i. Let X = \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{X} (x). Then for t &gt; 0\n\n\\mathbb{P}_{X} \\left(\n    x - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    \\frac{\n        - 2 t^{2}\n    }{\n        \\sum_{i=1}^{n} (b_{i} - a_{i})^{2}\n    }\n\\right].\n\n:::{prf:proof} Hoeffding’s inequality :label: hoeffding’s-inequality :class:dropdown\nWe can first apply Chernoff’s bounding method\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{ M_{X} (s) }{ e^{s t} }\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} M_{X_{i}} (s)\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\mathbb{E}_{X} \\left[\n        e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n    \\right]\n}{\n    e^{s t}\n}.\n\\end{aligned}\n\nThen by applying the Hoeffding’s Lemma,\n\n\\mathbb{E}_{X_{i}} \\left[\n    e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n\\right] \\leq \\exp \\left[\n    \\frac{ s^{2} (b_{i} - a_{i})^{2} }{ 8 }\n\\right]\n\nwe can have\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\mathbb{E}_{X_{i}} \\left[\n        e^{s (X_{i} - \\mathbb{E}_{X_{i}} [x_{i}])}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\prod_{i = 1}^{n} \\exp\\left[\n        \\frac{ s^{2} (b_{i} - a_{i})^{2} }{ 8 }\n    \\right]\n}{e^{s t}}\n\\\\\n& = \\inf_{s &gt; 0} \\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 } - s t\n\\right].\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t \\right] is a convex function,\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n& = 0\n\\\\\n\\left(\n    \\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n\\right)\n\\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 }  - s t\n\\right]\n& = 0\n\\\\\n\\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n& = 0\n\\\\\ns\n& = \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\end{aligned}\n\nTherefore, we have proved Hoeffding’s inequality\n\n\\begin{aligned}\n\\mathbb{P}_{X} (x - \\mu \\geq t)\n& \\leq \\inf_{s &gt; 0} \\exp\\left[\n    \\frac{ s^{2} }{ 8 } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{\n        \\left(\n            \\frac{ 4 t }{ \\sum_{i}^{n} (b_{i} - a_{i})^{2} }\n        \\right)^{2}\n    }{\n        8\n    } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} } t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\\end{aligned}\n\n:::\nAnother version is to bound the estimated mean.\nLet X_{1}, \\dots, X_{n} be bounded independent random variables such that X_{i} \\in [a_{i}, b_{i}], \\forall i. Let \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} and \\mu = \\mathbb{E}_{\\bar{X}} (x). Then for t &gt; 0\n\n\\mathbb{P}_{\\bar{X}} \\left(\n    x - \\mu \\geq t\n\\right) \\leq \\exp \\left[\n    -\\frac{ 2 n^{2} t^{2} }{ \\sum_{i=1}^{n} (b_{i} - a_{i})^{2} }\n\\right]"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html#martingales",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html#martingales",
    "title": "Joeyonng",
    "section": "Martingales",
    "text": "Martingales\n\nMartingale sequence\nGiven a sequence of random variables X_{1}, \\dots, X_{\\infty}, define another sequence of random variables Y_{1}, \\dots, Y_{\\infty} where Y_{n} is a measurable function of X_{1}, \\dots X_{n}\n\nY_{n} = f (X_{1}, \\dots, X_{n}).\n\nFor all Y_{n}, if \\mathbb{E}_{Y_{n}} [y_{n}] is finite and\n\n\\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}} [Y_{n + 1} \\mid X_{1}, \\dots, X_{n}] = Y_{n},\n\nthen Y_{1}, \\dots, Y_{\\infty} is a martingale sequence with respect to X_{1}, \\dots, X_{\\infty}.\n\n\nMartingale difference sequence\nGiven a sequence of random variables X_{1}, \\dots, X_{\\infty}, define another sequence of random variables Z_{1}, \\dots, Z_{\\infty} where Z_{n} is a measurable function of X_{1}, \\dots X_{n}\n\nZ_{n} = f (X_{1}, \\dots, X_{n}).\n\nFor all Z_{n}, if E_{Z_{n}} [z_{n}] is finite and\n\n\\mathbb{E}_{Z_{n + 1} \\mid X_{1}, \\dots, X_{n}} [Z_{n + 1} \\mid X_{1}, \\dots, X_{n}] = 0\n\nthen Z_{1}, \\dots, Z_{\\infty} is a martingale difference sequence with respect to X_{1}, \\dots, X_{\\infty}.\n\n\nA common Martingale sequence\nIf X_{1}, \\dots, X_{\\infty} is a sequence of independent random variables where \\mathbb{E}_{X_{i}} [x_{i}] = 0, \\forall X_{i}, then Y_{1}, \\dots, X_{\\infty} with Y_{n} = \\sum_{i = 1}^{n} X_{i} is martingale sequence.\n\n\\begin{aligned}\n\\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n& = \\mathbb{E}_{Y_{n} + X_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = \\mathbb{E}_{Y_{n} \\mid X_{1}, \\dots, X_{n}} + \\mathbb{E}_{X_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = Y_{n} + \\mathbb{E}_{X_{n + 1}}\n\\\\\n& = Y_{n}\n\\end{aligned}\n\nSuppose Y_{1}, \\dots, Y_{n} is a martingale sequence with respect to X_{1}, \\dots, X_{n}. then the sequence Z_{1}, \\dots, Z_{\\infty} with Z_{n} = Y_{n} - Y_{n - 1} is a martingale difference sequence.\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{n + 1} \\mid X_{1}, \\dots, X_{n}}\n& = \\mathbb{E}_{Y_{n + 1} - Y_{n} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = \\mathbb{E}_{Y_{n + 1} \\mid X_{1}, \\dots, X_{n}} - \\mathbb{E}_{Y_{n} \\mid X_{1}, \\dots, X_{n}}\n\\\\\n& = Y_{n} - Y_{n}\n\\\\\n& = 0\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html#azuma-inequality",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html#azuma-inequality",
    "title": "Joeyonng",
    "section": "Azuma Inequality",
    "text": "Azuma Inequality\nFor a sequence of Martingale difference sequence of bounded random variable Z_{1}, \\dots, Z_{\\infty} with Z_{i} \\in [a_{i}, b_{i}], \\forall i, then:\n\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right] \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\n:::{prf:proof} Azuma Inequality :label: azuma-inequality :class:dropdown\nFirst we can apply Chernoff bound to derive\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        e^{s \\sum_{i = 1}^{n} z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        \\prod_{i = 1}^{n} e^{s z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\end{aligned}\n\nThen we can use the law of total expectation to derive\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n} \\mid Z_{1}, \\dots Z_{n - 1}} \\left[\n        \\prod_{i = 1}^{n} e^{s z_{i}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n} \\mid Z_{1}, \\dots Z_{n - 1}} \\left[\n        e^{s z_{n}} \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\mathbb{E}_{Z_{n} \\mid Z_{1}, \\dots, Z_{n - 1}} \\left[\n        e^{s z_{n}} \\mid z_{1}, \\dots, z_{n - 1}\n    \\right]\n\\right],\n\\end{aligned}\n\nwhere the last equality is derived because \\prod_{i = 1}^{n - 1} e^{s z_{i}} is a constant given z_{1}, \\dots, z_{n - 1}.\nWe can derive a upper bound by applying the Hoeffding’s lemma\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& \\leq \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}}\\ \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}} \\exp \\left[\n        \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n    \\right]\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n\\right] \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}}\\ \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}}\n\\right]\n\\end{aligned}\n\nWe can apply the same procedure for \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} [ \\prod_{i = 1}^{n - 1} e^{s z_{i}} ]\n\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n - 1}} \\left[\n    \\prod_{i = 1}^{n - 1} e^{s z_{i}}\n\\right] = \\exp \\left[\n    \\frac{ s^{2} (a_{n - 1} - b_{n - 1})^{2} }{ 8 }\n\\right] \\mathbb{E}_{Z_{1}, \\dots, Z_{n - 2}}\\ \\left[\n    \\prod_{i = 1}^{n - 2} e^{s z_{i}}\n\\right]\n\nand do this iteratively to derive\n\n\\begin{aligned}\n\\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\prod_{i = 1}^{n} e^{s z_{i}}\n\\right]\n& = \\exp \\left[\n    \\frac{ s^{2} (a_{n} - b_{n})^{2} }{ 8 }\n\\right] \\times \\dots \\times \\exp \\left[\n    \\frac{ s^{2} (a_{1} - b_{1})^{2} }{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 }\n\\right].\n\\end{aligned}\n\nTherefore,\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\frac{\n    \\mathbb{E}_{Z_{1}, \\dots, Z_{n}} \\left[\n        e^{s \\sum_{i = 1}^{n} z_{i}}\n    \\right]\n}{\n    e^{s t}\n}\n\\\\\n& = \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n\\end{aligned}\n\nSince \\exp \\left[ \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t \\right] is a convex function,\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n& = 0\n\\\\\n\\left(\n    \\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n\\right)\n\\exp\\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 8 }  - s t\n\\right]\n& = 0\n\\\\\n\\frac{ s \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }{ 4 } - t\n& = 0\n\\\\\ns\n& = \\frac{ 4 t }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& \\leq \\inf_{s &gt; 0} \\exp \\left[\n    \\frac{ s^{2} \\sum_{i = 1}^{n} (a_{n} - b_{n})^{2} }{ 8 } - s t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{\n        \\left(\n            \\frac{ 4 t }{ \\sum_{i}^{n} (b_{i} - a_{i})^{2} }\n        \\right)^{2}\n    }{\n        8\n    } \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} - \\frac{\n        4 t\n    }{\n        \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2}\n    } t\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right].\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Probability and Statistics/Inequalities2.html#mcdiarmids-inequality",
    "href": "Knowledge/Probability and Statistics/Inequalities2.html#mcdiarmids-inequality",
    "title": "Joeyonng",
    "section": "McDiarmid’s Inequality",
    "text": "McDiarmid’s Inequality\nGiven a function f: \\mathbb{R}^{n} \\to \\mathbb{R} with the bounded difference property, that is, the maximum change of the function output induced by replacing any x_{i} with x'_{i} is bounded by c_{i},\n\n\\lvert f (x_{1}, \\dots, x_{i}, \\dots, x_{n}) - f (x_{1}, \\dots, x'_{i}, \\dots, x_{n}) \\rvert \\leq c_{i},\n\nthen the function f of independent random variables X_{1}, \\dots, X_{n} satisfies\n\n\\mathbb{P}_{X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n}) - \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n        f (X_{1}, \\dots, X_{n})\n    \\right] \\geq t\n\\right] \\leq \\exp \\left[\n    \\frac{\n        - 2 t^{2}\n    }{\n        \\sum_{i=1}^{n} c_{i}^{2}\n    }\n\\right].\n\n:::{prf:proof} McDiarmid’s Inequality :label: micdiarmid’s-inequality :class:dropdown\nLet X_{1}, \\dots, X_{n} be a sequence of independent random variable, and Y_{1}, \\dots, Y_{n} be a Martingale sequence with respect to X_{1}, \\dots, X_{n} where\n\nY_{i} = g (X_{1}, \\dots, X_{i}) = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i}\n\\right].\n\nNote that we have\n\n\\begin{aligned}\nY_{0}\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n})\n\\right],\n\\\\\nY_{n}\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{n}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{n}\n\\right] = f (X_{1}, \\dots, X_{n}).\n\\end{aligned}\n\nLet Z_{1}, \\dots, Z_{n} be an independent random variable with Z_{i} = Y_{i} - Y_{i - 1}, which is proved to be a Martingale difference sequence.\nNote that\n\n\\sum_{i = 1}^{n} Z_{i} = Y_{n} - Y_{0} = f (X_{1}, \\dots, X_{n}) - \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n    X_{1}, \\dots, X_{n}\n\\right]\n\nFor each Z_{i}, its upper bound U_{i} and lower bound L_{i} can be written as\n\n\\begin{aligned}\nU_{i}\n& = \\sup_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i - 1}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\nL_{i}\n& = \\inf_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i - 1}} \\left[\n    f (X_{1}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\end{aligned}\n\nThe difference between U_{i} and L_{i} is less than c_{i} because of the bounded property property of f,\n\n\\begin{aligned}\nU_{i} - L_{i}\n& = \\sup_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, \\dots X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right] - \\inf_{x_{i} \\in X_{i}} \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    f (X_{1}, \\dots, x_{i}, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& = \\sup_{x_{i}, x'_{i} \\in X_{i}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right] - \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right]\n\\right]\n\\\\\n& = \\sup_{x_{i}, x'_{i} \\in X_{i}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n        f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) -\n        f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    \\sup_{x_{i}, x'_{i} \\in X_{i}} f (X_{1}, \\dots, x_{i}, \\dots, X_{n}) -\n    f (X_{1}, \\dots, x'_{i}, \\dots, X_{n}) \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& \\leq \\mathbb{E}_{X_{1}, \\dots, X_{n} \\mid X_{1}, \\dots, X_{i}} \\left[\n    c_{i}\n    \\mid X_{1}, \\dots, X_{i - 1}\n\\right]\n\\\\\n& \\leq c_{i}.\n\\end{aligned}\n\nTherefore, we have a Martingale difference sequence of bounded random variable Z_{1}, \\dots, Z_{n}, where Z_{i} \\in [a_{i}, b_{i}], \\forall i and b_{i} - a_{i} \\leq c_{i}. We can apply Azuma inequality to obtain McDiarmid’s inequality.\n\n\\begin{aligned}\n\\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\sum_{i = 1}^{n} z_{i} \\geq t\n\\right]\n& = \\mathbb{P}_{Z_{1}, \\dots, Z_{n}} \\left[\n    \\mathbb{E}_{X_{1}, \\dots, X_{n}} \\left[\n        X_{1}, \\dots, X_{n}\n    \\right]\n\\right]\n\\\\\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} (b_{i} - a_{i})^{2} }\n\\right]\n\\\\\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} c_{i}^{2} }\n\\right].\n\\end{aligned}\n\n:::"
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html",
    "title": "Expectation-maximization",
    "section": "",
    "text": "Updated 01-11-2023 (First commited 01-08-2023)\n(expectation-maximization)=\nExpectation-maximization (EM) is an iterative algorithm that solves maximum likelihood (ML) problems of estimating parameters in case there are missing or latent (unobserved) variables (features or labels). EM iterates between E-step and M-step until the convergence criterion is met. In E-step, the expected value of the complete data log-likelihood with respect to the probability of the missing variables is computed as a function of the unknown parameters. In M-step, we choose the parameters that maximize the expected value derived in the E-step. EM is proved to converge to a local minimum while the global minimum is not guaranteed."
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#preliminary",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#preliminary",
    "title": "Expectation-maximization",
    "section": "Preliminary",
    "text": "Preliminary\n\nStatistics\n\nKL-divergence\nEntropy\n\n\n\nStatistical Learning\n\nMaximum Likelihood Estimation"
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#the-em-algorithm",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#the-em-algorithm",
    "title": "Expectation-maximization",
    "section": "The EM algorithm",
    "text": "The EM algorithm\n\nProblem statements\nIn EM problems, we have two types of variables:\n\nObserved variables: a set of random variables \\mathbf{X} from which we can draw samples.\nHidden variables: another set of random variables \\mathbf{Z} from which we cannot draw samples.\n\nAlthough we cannot draw samples from \\mathbf{Z}, we assume there exists a \\mathbf{z} for each \\mathbf{x} observed. Thus we define two types of datasets.\n\nIncomplete data: samples drawn from the observed variables\n\n  \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\}.\n  \nComplete data: the sample pairs of the observed and hidden variables that we never be able to observe\n\n  \\mathcal{X}, \\mathcal{Z} = \\{ (\\mathbf{x}_{1}, \\mathbf{z}_{1}), \\dots, (\\mathbf{x}_{n}, \\mathbf{z}_{n}) \\}.\n  \n\nGiven the incomplete data \\mathcal{X}, the goal is to find the maximum likelihood estimate of parameters \\Psi for the log-likelihood of the incomplete data \\mathcal{X}\n\n\\Psi^{*} = \\arg\\max_{\\Psi} \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi).\n\n\nIf we knew what the model \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) is without introducing other variables, the problem is a standard MLE problem and usually can be easily solved.\nIf instead the joint probability \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathbf{x}, \\mathbf{z}) is known or can be derived, we can still solve the problem by maximizing the equation that marginalizes out the latent variables from the joint probability\n\n  \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n      \\mathcal{X}, \\mathcal{Z}; \\Psi\n  ) \\partial \\mathbf{z},\n  \nwhich in most of the cases doesn’t have a closed-form solution and thus needs to be solved using iterative algorithms.\nEM is one of such algorithm with some unique properties that other algorithms don’t have.\n\n\n\nEM procedures\n\n1. E-Step\nGiven parameters \\hat{\\Psi} estimated at the current iteration and incomplete data \\mathcal{X}, compute the Q function, which is the expected value of the log likelihood of the complete data \\mathcal{X}, \\mathcal{Z} over the distribution of \\mathbf{Z}\n\nQ_{\\hat{\\Psi}} (\\Psi) = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi) \\mid \\mathcal{X}\n\\right].\n\n\nSince \\mathcal{X} is given but we never be able to see \\mathcal{Z}, the log-likelihood is a function of both \\Psi and \\mathbf{Z},\n\n  L(\\mathcal{Z}, \\Psi) = \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) \\mid \\mathcal{X}.\n  \nThe Q function only depend on \\Psi but not \\mathcal{Z} because taking the expected value of L(\\mathcal{Z}, \\Psi) over the distribution of \\mathbf{Z} eliminates \\mathcal{Z} in the expression.\nThe estimate of the parameter \\hat{\\Psi} in the current iteration is used in calculating the probability of \\mathcal{Z} given \\mathcal{X}, which is needed in the expectation calculation process.\nDepending the problem setups, the expressions for both \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi) and \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}) (used in \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} [\\cdot]) should be already known or can be derived from the known probabilistic models between \\mathbf{X} and \\mathbf{Z}.\n\n\n\n2. M-Step\nFind the parameter \\hat{\\Psi} that maximizes the expected value derived in the E-step.\n\n\\hat{\\Psi} = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi)\n\nRepeat procedure 1 and 2 until convergence."
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#derivation-of-em",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#derivation-of-em",
    "title": "Expectation-maximization",
    "section": "Derivation of EM",
    "text": "Derivation of EM\nThe log-likelihood of the incomplete data can be decomposed into 2 components\n\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = G (\\Psi, q) +  D (\\Psi, q)\n\nwhere q is an arbitrary distribution of the latent variables.\n\nThe lower bound of EM\n\n  G (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log \\frac{\n          \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n      }{\n          q (\\mathcal{Z})\n      }\n  \\right]\n  \nThe KL-divergence of between two distributions of \\mathbf{Z}, i.e. q(\\mathcal{Z}) and \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n\n  D (\\Psi, q) = \\mathrm{KL} \\left[\n      q \\mathrel{\\Vert} \\mathbb{P}_{\\mathbf{X} \\mid \\mathbf{Z}}\n  \\right] = \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log \\frac{\n          q (\\mathcal{Z})\n      }{\n          \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n      }\n  \\right].\n  \n\n:::{admonition} Proof: \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) = Q (\\Psi, q) + \\mathrm{H} (q) + D (\\Psi, q) :class: dropdown\nSince \\log \\mathbb{P}_\\mathbf{X} (\\mathbf{x}) is not dependent on \\mathbf{Z}, taking its expectation over any distribution of \\mathbf{Z} is equal to itself. Thus,\n\n\\begin{aligned}\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        q (\\mathcal{Z})\n    } \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        q (\\mathcal{Z})\n    } \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n    }{\n        q (\\mathcal{Z})\n    }\n\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = G (\\Psi, q) + D (\\Psi, q)\n\\\\\n& = \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n\\right] - \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log q (\\mathcal{Z})\n\\right] + \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\Psi)\n    }\n\\right]\n\\\\\n& = Q (\\Psi, q) + \\mathrm{H} (q) + D (\\Psi, q)\n\\end{aligned}\n\n:::\n\nLower bound\nSince KL-divergence D (\\Psi, q) is proved to be non-negative, G(\\Psi, q) can be seen as a lower bound of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \\geq G(\\Psi, q),\n\nwhich can also be proven using Jensen’s inequality.\n:::{admonition} Proof: \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) \\geq G(\\Psi, q) using Jensen’s inequality. :class: dropdown\n\n\\begin{aligned}\n\\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi)\n& = \\log \\int \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n    \\mathcal{X}, \\mathcal{Z}; \\Psi\n) \\partial \\mathbf{z}\n\\\\\n& = \\log \\int q (\\mathcal{Z}) \\frac{\n    \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n        \\mathcal{X}, \\mathcal{Z}; \\Psi\n    )\n}{\n    q (\\mathcal{Z})\n} \\partial \\mathbf{z}\n\\\\\n& = \\log \\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n            \\mathcal{X}, \\mathcal{Z}; \\Psi\n    )\n    }{\n        q (\\mathcal{Z})\n    }\n\\right]\n\\\\\n& \\geq \\mathbb{E}_{\\mathbf{Z}} \\left[ \\log\n    \\frac{\n        \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\n            \\mathcal{X}, \\mathcal{Z}; \\Psi\n        )\n    }{\n        q (\\mathcal{Z})\n    }\n\\right]\n\\\\\n& \\geq G (\\Psi, q)\n\\\\\n\\end{aligned}\n\n:::\n\n\nEM as coordinate ascent on lower bound\nThe EM is essentially doing coordinate ascent on G (\\Psi, q), which is believed to be easier to optimize than directly optimizing \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi), while guaranteeing the \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) is non-decreasing as G (\\Psi, q) is optimized.\nCoordinate ascent is a optimization method that optimize a single variable or 1 dimension of the variable at a time, while fixing the values of the rest of the variables from the last iteration unchanged. In the case of applying to G (\\Psi, q) function, \\Psi and q are separately maximized in different steps of each iteration.\n\nE-step: given the parameter \\hat{\\Psi} estimated in the last iteration, the choice of the q function is optimized to maximize the value of G (\\Psi, q).\nM-step: given the \\hat{q} function selected in E-step, \\Psi is optimized to maximize the value of G (\\Psi, \\hat{q}) and will be used in the E-step of the next iteration.\n\n\n\nE-step\nSince the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) doesn’t depend on the choice of q, the choice of q only affect the balance between G (\\Psi, q) and D (\\Psi, q) when the \\Psi is fixed.\nThus, given the parameters \\hat{\\Psi} estimated in the last iteration, G (\\hat{\\Psi}, q) is maximized with respect to q when D (\\hat{\\Psi}, q) is minimized. Since the minimized value of D (\\Psi, q) is 0, we have\n\n\\begin{aligned}\nD (\\hat{\\Psi}, q)\n& = 0\n\\\\\n\\mathbb{E}_{\\mathbf{Z}} \\left[\n    \\log \\frac{\n        q (\\mathcal{Z})\n    }{\n        \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi})\n    }\n\\right]\n& = 0\n\\\\\nq (\\mathcal{Z})\n& = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}),\n\\end{aligned}\n\nwhich shows that G (\\hat{\\Psi}) is maximized when the distribution of latent variables is chosen to be the probability of the latent variables given the observed data and the current estimate of the parameters.\nA nice property of optimizing q, even though it doesn’t affect the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) at all, is that the value of KL-divergence D (\\Psi, \\hat{q}) will also be non-decreasing no matter what \\Psi is selected in the M-step by maximizing G (\\Psi, \\hat{q}).\n\nThis can be seen from the fact that D (\\Psi, \\hat{q}) = 0 when \\hat{p} is selected to maximize G (\\hat{\\Psi}, q), and thus any \\Psi will guarantee that the value of D (\\Psi, \\hat{q}) is larger or equal to 0.\nThis property implicitly prove the convergence of the EM algorithm in that both G (\\Psi, q) and D (\\Psi, q) will be non-decreasing during each iteration, and therefore, the value of \\log \\mathbb{P}_{\\mathbf{X}} (\\mathcal{X}; \\Psi) is non-decreasing in each iteration.\n\n\n\nM-step\nG (\\Psi, q) can be further decomposed into two components\n\nG (\\Psi, q) = Q (\\Psi, q) + \\mathrm{H} (q).\n\n\nThe expected value of the complete data with respect to the distribution q\n\n  Q (\\Psi, q) = \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n  \\right].\n  \nThe entropy of the latent variables\n\n  \\mathrm{H} (q) = - \\mathbb{E}_{\\mathbf{Z}} \\left[\n      \\log q (\\mathcal{Z})\n  \\right].\n  \n\nSince \\mathrm{H} (q) doesn’t depend on \\Psi, it will stay as a constant in the process of maximizing G (\\Psi, q) with respect to \\Psi.\nGiven \\hat{q} = \\mathbb{P}_{\\mathbf{Z} \\mid \\mathbf{X}} (\\mathcal{Z} \\mid \\mathcal{X}; \\hat{\\Psi}), maximizing G (\\Psi, \\hat{q}) is the same as maximizing Q function we defined above:\n\n\\begin{aligned}\n\\arg\\max_{\\Psi} G (\\Psi, \\hat{q})\n& = \\arg\\max_{\\Psi} Q_{\\hat{\\Psi}} (\\Psi)\n\\\\\n& = \\arg\\max_{\\Psi} \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\log \\mathbb{P}_{\\mathbf{X}, \\mathbf{Z}} (\\mathcal{X}, \\mathcal{Z} ; \\Psi)\n\\right].\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#example-mixture-model",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#example-mixture-model",
    "title": "Expectation-maximization",
    "section": "Example: mixture model",
    "text": "Example: mixture model\nOne application of EM algorithm is to obtain MLE of the parameters in a mixture models.\n\nMixture model\nWe say random variable \\mathbf{X} follows a mixture model if its distribution is a weighted combination of multiple components, where each component has a simple parametric distributions. Thus mixture model can represent distributions that cannot be expressed using a single parametric distribution.\nEach sample \\mathbf{x} is associated with a latent random variable z that indicates which component (parametric distribution) that \\mathbf{x} should be drawn. Thus the sample \\mathbf{x} has the conditional probability in a parametric form with parameters \\boldsymbol{\\theta}\n \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z ; \\boldsymbol{\\theta}) \nif we know the latent variable z for the sample.\nAssuming in total we have c latent variables for all samples and each latent variable has the probability \\mathbb{P}_{Z} (z), the probability of the sample is\n \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}) = \\sum_{z=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z) \\mathbb{P}_{Z} (z). \nThe Gaussian mixture model is simply a mixture model in which all components are Gaussian distributions.\n\n\nEM for mixture model\nIf we knew what z is for each \\mathbf{x}, the estimate of parameters for each component can be easily derived by sampling a dataset \\mathcal{X}_{z} from the conditional distribution and applying MLE.\nHowever, in practice, we never know which component each sample belongs to, and thus we apply EM by treating \\mathbf{X} as observed variables and Z as the hidden variable.\nThe goal of applying EM is to find the parameters \\Psi in the mixture model including:\n\nThe parameters for the parametric distribution of each component \\{ \\boldsymbol{\\theta}_{1}, \\dots, \\boldsymbol{\\theta}_{c} \\}.\nThe probability of each component \\{ \\pi_{1}, \\dots, \\pi_{c} \\}.\n\n\nE-step: complete data likelihood\nTo derive the EM procedure, we first need to write out the log-likelihood of the complete data in terms of the known parametric distributions\n\n\\begin{aligned}\nL(\\mathcal{Z}, \\Psi)\n& = \\log \\mathbb{P}_{\\mathbf{X}, Z} (\\mathcal{X}, \\mathcal{Z}; \\Psi)\n\\\\\n& = \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathcal{X} \\mid \\mathcal{Z} ; \\boldsymbol{\\theta}) \\mathbb{P}_{Z} (\\mathcal{Z})\n\\\\\n& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n& [\\text{i.i.d assumption}],\n\\\\\n\\end{aligned}\n\nwhere x_{i} is a sample, z_{i} indicates the component that x_{i} belongs to, \\boldsymbol{\\theta}_{z_{i}} is the parameters of the component z_{i}, and \\pi_{z_{i}} is the probability of the component z_{i}.\nSince z is discrete and range from 1 to c, any function of z can be written as\n f(z) = \\prod_{i=1}^{c} f(i)^{\\mathbb{1}(z = i)}, \nwhere z is extracted out from the function to the power of the function.\nThus, the complete data likelihood can be further simplified to \n\\begin{aligned}\nL(\\mathcal{Z}, \\Psi)\n& = \\log \\prod_{i=1}^{n} \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}\n\\\\\n& = \\log \\prod_{i=1}^{n} \\prod_{j=1}^{c} \\left[\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n\\right]^{\\mathbb{1}(z_{i} = j)}\n& [f(z_{i}) = \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid z_{i} ; \\boldsymbol{\\theta}_{z_{i}}) \\pi_{z_{i}}]\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\log \\left[\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}\n\\right]^{\\mathbb{1}(z_{i} = j)}\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}) \\pi_{j}.\n\\end{aligned}\n\n\n\nE-step: Q function\nTaking the expectation of complete data log-likelihood over Z gives us the Q function that doesn’t depend on Z\n\n\\begin{aligned}\nQ_{\\hat{\\Psi}} (\\Psi)\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    L(\\mathcal{Z}, \\Psi)\n\\right]\n\\\\\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{1}(z_{i} = j) \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n    \\right) \\pi_{j}\n\\right]\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\mathbb{1}(z_{i} = j)\n\\right] \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j}\n\\\\\n& = \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j},\n\\\\\n\\end{aligned}\n\nwhere $h_{i, j} = _{Z ; } $ is a constant value that can be computed given that we have parameter \\hat{\\Psi} estimated in the last iteration.\n%%markdown\n\n\\begin{aligned}\nh_{i, j}\n& = \\mathbb{E}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left[\n    \\mathbb{1} (z_{i} = j)\n\\right]\n\\\\\n& = \\sum_{k=1}^{c} \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n    k \\mid \\mathbf{x}_{i}\n\\right)\n\\mathbb{1} (k = j)\n\\\\\n& = \\mathbb{P}_{Z \\mid \\mathbf{X}; \\hat{\\Psi}} \\left(\n    j \\mid \\mathbf{x}_{i}\n\\right)\n& [\\mathbb{1} (k = j) = 0 \\text{ for } k \\neq j]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x}_{i})\n}\n& [\\text{Bayes' Theorem}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j ; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X}, Z} \\left(\n        \\mathbf{x}_{i}, k\n    \\right)\n}\n& [\\text{marginalization}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid j; \\hat{\\boldsymbol{\\theta}}_{j}\n    \\right) \\hat{\\pi}_{j}\n}{\n    \\sum_{k=1}^{c} \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n        \\mathbf{x}_{i} \\mid k; \\hat{\\boldsymbol{\\theta}}_{k}\n    \\right) \\hat{\\pi}_{k}\n}\n\\end{aligned}\n\n\n\nM-step: maximizes Q function\nComputing \\hat{\\Psi}_{\\text{new}} that maximizes Q function for the next iteration is an optimization problem\n\n\\hat{\\Psi}_{\\text{new}} = \\arg\\max_{\\Psi} \\sum_{i=1}^{n} \\sum_{j=1}^{c} h_{i, j} \\log \\mathbb{P}_{\\mathbf{X} \\mid Z} \\left(\n    \\mathbf{x}_{i} \\mid j ; \\boldsymbol{\\theta}_{j}\n\\right) \\pi_{j},\n\nwhich can usually be solved analytically depending on the mathematical form of the parametric distribution of the component \\mathbb{P}_{\\mathbf{X} \\mid Z} (\\mathbf{x} \\mid z).\nAgain, \\hat{\\Psi}_{\\text{new}} is the estimated parameters that include\n\nparameters \\{ \\hat{\\boldsymbol{\\theta}}_{1}, \\dots, \\hat{\\boldsymbol{\\theta}}_{c} \\} in the c components of the mixture model.\nprobability parameters \\{ \\hat{\\pi}_{1}, \\dots, \\hat{\\pi}_{c} \\} of c components."
  },
  {
    "objectID": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#reference",
    "href": "Knowledge/Statistical Learning/4_Expectation_Maximization.html#reference",
    "title": "Expectation-maximization",
    "section": "Reference",
    "text": "Reference\n\nhttp://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf\nhttps://gregorygundersen.com/blog/2019/11/10/em/\nhttps://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1268&context=gc_cs_tr\nhttps://mbernste.github.io/posts/em/"
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html",
    "title": "Bayesian Estimation",
    "section": "",
    "text": "Updated 01-11-2023 (First commited 01-08-2023)\n(bayesian-estimation)="
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#preliminary",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#preliminary",
    "title": "Bayesian Estimation",
    "section": "Preliminary",
    "text": "Preliminary\n\nStatistical Learning\n\nBayesian Decision Theory (BDT)\nMaximum Likelihood Estimation (MLE)"
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#views-on-parameter-estimation",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#views-on-parameter-estimation",
    "title": "Bayesian Estimation",
    "section": "Views on parameter estimation",
    "text": "Views on parameter estimation\nThere are two different frameworks on statistical inferences: frequentist view and Bayesian view. Both views have the same definition of the probability, but they have different views on how probability of an event should be calculated or accessed. Thus, the way to do parameter estimation is different under the two frameworks.\n\nFrequentist view\nFrequentists believe that the probability of an event is a measure of relative frequency and should be calculated by observing how many times the event happens in a large number of trials.\n\nProbability: the probability of any event is objective and doesn’t change with different beliefs to the event.\nParameters: if the parameters of the distribution are unknown, the parameters must be fixed constants. That is, they must be certain determined values.\nEstimation: the single best estimation of the parameter can be derived using single dataset and its goodness (bias and variance) can be measured by sampling different datasets.\n\n\n\nBayesian view\nBayesian approach believes that a probability of an event includes not only the relative frequency, but also the subjective beliefs. That is, the degree of belief on the outcomes of the experiment.\n\nProbability: the subjective beliefs can be very different from person to person, and thus the probability of any event is very subjective.\nParameters: the unknown parameters of the distribution are viewed as random variables, and thus include subjective beliefs.\nEstimation: the subjective beliefs of the parameters are specified using a prior distribution, which is then updated using the single dataset observed. The result of the estimation is a posterior probabilities of a range of parameter values, which include both prior beliefs and relative frequency."
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#bayesian-estimation",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#bayesian-estimation",
    "title": "Bayesian Estimation",
    "section": "Bayesian estimation",
    "text": "Bayesian estimation\nIn Bayesian estimation, the unknown parameters are treated as random variables.\n\nThe subjective beliefs about the parameters that we want to estimate before observing any dataset are encoded using a distribution\n\n  \\mathbb{P}_{\\boldsymbol{\\Theta}} (\\boldsymbol{\\theta}),\n  \nwhich specifies the prior probabilities of all possible values of the parameters.\nThe likelihood of a dataset \\mathcal{X} = \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n} \\} given parameters \\boldsymbol{\\theta} is a conditional probability\n\n  \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left(\n      \\mathcal{X} \\mid \\boldsymbol{\\theta}\n  \\right).\n  \n\nUnlike maximum likelihood estimation, which is under frequentist view and gives only the single best parameter, the result of Bayesian estimation is a posterior distribution that informs us how the observed data update the prior. According to Bayes Theorem, the posterior distribution can be calculated by:\n\n\\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n\\right) = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid \\boldsymbol{\\Theta}} \\left (\n        \\mathcal{X} \\mid \\boldsymbol{\\theta}\n    \\right) \\mathbb{P}_{\\boldsymbol{\\Theta}} \\left(\n        \\boldsymbol{\\theta}\n    \\right)\n}{\n    \\mathbb{P}_{\\mathbf{X}} \\left(\n        \\mathcal{X}\n    \\right)\n}.\n\n\nMaximum a posteriori (MAP) estimation\nIn the case where we want a single estimate for the parameter using Bayesian estimation, MAP estimation chooses the value of the parameter that has the largest probability in the posterior distribution\n\n\\boldsymbol{\\theta}_{MAP} = \\arg\\max_{\\boldsymbol{\\theta}} \\mathbb{P}_{\\boldsymbol{\\Theta} \\mid \\mathbf{X}} \\left(\n    \\boldsymbol{\\theta} \\mid \\mathcal{X}\n\\right)."
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#bayesian-bdr",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#bayesian-bdr",
    "title": "Bayesian Estimation",
    "section": "Bayesian BDR",
    "text": "Bayesian BDR\n\nTODO"
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#example-mean-of-the-univariate-gaussian",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#example-mean-of-the-univariate-gaussian",
    "title": "Bayesian Estimation",
    "section": "Example: mean of the univariate Gaussian",
    "text": "Example: mean of the univariate Gaussian\nHere we shows an example of estimating the posterior probability of the mean parameter of a univariate normal distribution using Bayesian estimation.\nConsider the univariate case where the probability of the instance x follows a normal distribution with unknown mean \\mu and known variance \\sigma^{2}:\n\n\\mathbb{P}_{X \\mid \\mu} (x \\mid \\mu) \\sim \\mathcal{N} (\\mu, \\sigma^{2}) = \\mathcal{G} (x, \\mu, \\sigma^{2}),\n\nand we assume whatever prior knowledge we have about \\mu can be expressed by another normal distribution with known mean \\mu_{0} and known variance \\sigma_{0}^{2}:\n\n\\mathbb{P}_{\\mu} (\\mu) \\sim \\mathcal{N} (\\mu_{0}, \\sigma_{0}^{2}) = \\mathcal{G} (\\mu, \\mu_{0}, \\sigma_{0}^{2}).\n\n\nPosterior distribution of mean parameter\nSuppose now that n samples \\mathcal{X} = \\{x_{1}, \\dots, x_{n}\\} are independently sampled. We can use Bayes formula to obtain the posterior probability:\n\n\\mathbb{P}_{\\mu \\mid \\mathcal{X}} (\\mu \\mid \\mathcal{X}) = \\frac{\n    \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n}{\n    \\mathbb{P}_{X} (\\mathcal{X})\n}.\n\nSince \\mathbb{P}_{X}(\\mathcal{X}) is a normalization factor that doesn’t depend on \\mu, we now omit it for simplicity:\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X})\n& \\propto \\mathbb{P}_{X \\mid \\mu} (\\mathcal{X} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n\\\\\n& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu} (x_{i} \\mid \\mu) \\mathbb{P}_{\\mu} (\\mu)\n& [\\text{independent assumption}].\n\\\\\n\\end{aligned}\n\nAfter expanding the normal distribution definition and some simplifications, we can see that \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) also follows a normal distribution:\n\n\\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\sim \\mathcal{N} (\\mu_{n}, \\sigma_{n}^{2})\n\nwhere\n\n\\mu_{n} = \\frac{\n    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n},\n\n\n\\sigma_{n}^{2} = \\frac{\n    \\sigma_{0}^{2} \\sigma^{2}\n}{\n    n \\sigma_{0}^{2} + \\sigma^{2}\n}.\n\n:::{admonition} Proof: \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\sim \\mathcal{N} (\\mu_{n}, \\sigma_{n}^{2}) :class: dropdown\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid \\mathcal{D}}(\\mu \\mid \\mathcal{D})\n& \\propto \\mathbb{P}_{\\mathcal{D} \\mid \\mu}(\\mathcal{D} \\mid \\mu) \\mathbb{P}_{\\mu}(\\mu) \\\\\n& = \\prod_{i=1}^{n} \\mathbb{P}_{X \\mid \\mu}(x_{i} \\mid \\mu) \\mathbb{P}_{\\mu}(\\mu) \\\\\n& = \\prod_{i=1}^{n} \\frac{ 1 }{\\sqrt{2 \\pi \\sigma^{2}}} \\exp{ - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2} } } \\frac{ 1 }{ \\sqrt{2 \\pi \\sigma_{0}^{2}} } \\exp{ - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } } \\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\prod_{i=1}^{n} \\exp{ \\left[ - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2}} - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{merging constants and exponentials}] \\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ \\sum_{i=1}^{n} - \\frac{ (x_{i} - \\mu)^{2} }{ 2 \\sigma^{2} } - \\frac{ (\\mu - \\mu_{0})^{2} }{ 2 \\sigma_{0}^{2} } \\right] } \\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ \\sum_{i=1}^{n} - \\frac{ x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2}}{2 \\sigma^{2} } - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{expanding squares}] \\\\\n& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} \\left[ x_{i}^{2} - 2 x_{i} \\mu + \\mu^{2} \\right] }{ 2 \\sigma^{2} } - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2}} \\right] } \\\\\n& = \\frac{ 1 }{\\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} - \\sum_{i=1}^{n} 2 x_{i} \\mu + n \\mu^{2} }{2 \\sigma^{2}} - \\frac{ \\mu^{2} - 2 \\mu \\mu_{0} + \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{reordering sums}] \\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } + \\frac{ \\sum_{i=1}^{n} 2 x_{i} \\mu }{ 2 \\sigma^{2} } - \\frac{ n \\mu^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu^{2} }{ 2 \\sigma_{0}^{2} } + \\frac{ 2 \\mu \\mu_{0} }{ 2 \\sigma_{0}^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } \\\\\n& = \\frac{ 1 }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } & [\\text{grouping } \\mu] \\\\\n& = \\frac{ \\exp{ \\left[ - \\frac{ \\sum_{i=1}^{n} x_{i}^{2} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0}^{2} }{ 2 \\sigma_{0}^{2} } \\right] } }{ \\sqrt{4 \\pi^{2} \\sigma^{2} \\sigma_{0}^{2}} } \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu \\right] } & [\\text{extracting out terms without } \\mu] \\\\\n& \\propto \\exp{ \\left[ - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right) \\mu^{2} + 2 \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right) \\mu \\right] } & [\\text{removing terms without } \\mu] \\\\\n\\end{aligned}\n\nUsing the completing the squares trick\n\n\\begin{aligned}\nax^{2} + 2bx + c  \n& = a \\left( x^{2} + 2 \\frac{ b }{ a } x + \\frac{ c }{ a } \\right) \\\\\n& = a \\left( x^{2} + 2 \\frac{ b }{ a } x + \\left( \\frac{ b }{ a } \\right)^{2} - \\left( \\frac{ b }{ a } \\right)^{2} + \\frac{ c }{ a } \\right) \\\\\n& = a \\left( x + \\frac{ b }{ a } \\right)^{2} + c - \\frac{ b^{2} }{ a }, \\\\\n\\end{aligned}\n\nand treating\n a = - \\left( \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} } \\right), \n b = \\left( \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} } \\right), \nwe can have\n\n\\begin{aligned}\n\\mathbb{P}_{\\mu \\mid \\mathcal{D}}(\\mu \\mid \\mathcal{D})\n& \\propto \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu^{2}\n    + 2 \\left(\n        \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n    \\right) \\mu\n\\right] }\n\\\\\n& \\propto \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right)\n    \\left(\n        \\mu - \\frac{\n            \\frac{ \\sum_{i=1}^{n} x_{i} }{ 2 \\sigma^{2} } - \\frac{ \\mu_{0} }{ 2 \\sigma_{0}^{2} }\n        }{\n            \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n        }\n    \\right)^{2}\n\\right] }\n& [\\text{remove } \\frac{ b^{2} }{ a } \\text{ as it doesn't depend on } \\mu]\n\\\\\n& = \\exp{ \\left[\n    - \\left(\n        \\frac{ n }{ 2 \\sigma^{2} } + \\frac{ 1 }{ 2 \\sigma_{0}^{2} }\n    \\right)\n    \\left(\n        \\mu - \\frac{\n            \\frac{\n                \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n            }{\n                2 \\sigma^{2} \\sigma_{0}^{2}\n            }\n        }{\n            \\frac{\n                \\sigma^{2} + n \\sigma_{0}^{2}\n            }{\n                2 \\sigma^{2} \\sigma_{0}^{2}\n            }\n        }\n    \\right)^{2}\n\\right] }\n\\\\\n& = \\exp{ \\left[\n    - \\left(\n        \\frac{ 2 \\sigma^{2} \\sigma_{0}^{2} }{ \\sigma^{2} + n \\sigma_{0}^{2} }\n    \\right)^{-1}\n    \\left(\n        \\mu - \\frac{\n            \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n        }{\n            \\sigma^{2} + n \\sigma_{0}^{2}\n        }\n    \\right)^{2}\n\\right] }\n.\n\\\\\n\\end{aligned}\n\n:::\nThis means that the unknown parameter \\mu estimated by a set of instances \\mathcal{X} using Bayesian estimation has a probability \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) that follows a normal distribution that has mean \\mu_{n} and variance \\sigma_{n}^{2}.\n\nSince \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) is a normal distribution, \\mathbb{P}_{\\mu \\mid X} (\\mu_{n} \\mid \\mathcal{X}) is largest and thus we can view \\mu_{n} as our best guess for \\mu after observing \\mathcal{X}.\nThen we can view the variance \\sigma_{n}^{2} as the uncertainty about this best guess.\nSince \\sigma_{n}^{2} decreases monotonically with n, each additional observation decreases our uncertainty of the best guess.\n\n%%markdown\nSince \\mu_{n} can be further rewritten as\n\n\\begin{aligned}\n\\mu_{n}\n& = \\frac{\n    \\sigma_{0}^{2} \\sum_{i=1}^{n} x_{i} + \\mu_{0} \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n}\n\\\\\n& = \\frac{\n    \\sigma_{0}^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n} \\sum_{i=1}^{n} x_{i} + \\frac{\n    \\sigma^{2}\n}{\n    \\sigma^{2} + n \\sigma_{0}^{2}\n} \\mu_{0}\n\\\\\n& = \\frac{\n    n \\sigma_{0}^{2}\n}{\n    n \\sigma_{0}^{2} + \\sigma^{2}\n} \\bar{x}_{n} + \\left(\n    1 - \\frac{\n        n \\sigma_{0}^{2}\n    }{\n        n \\sigma_{0}^{2} + \\sigma^{2}\n    }\n\\right) \\mu_{0}\n& [\\bar{x}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}]\n\\\\\n& = \\alpha_{n} \\bar{x}_{n} + (1 - \\alpha_{n}) \\mu_{0}\n& [\\alpha_{n} = \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}}]\n,\n\\\\\n\\end{aligned}\n\nthe final equation shows that \\mu_{n} is a combination of the maximum likelihood estimate \\bar{x}_{n} and the prior information \\mu_{0}.\nSince\n  \n\\begin{aligned}\n\\lim_{n \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n}\n& [\\lim_{n \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 1]\n\\\\\n\\lim_{n \\to 0} \\mu_{n}\n& = \\mu_{0}\n& [\\lim_{n \\to 0} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 0],\n\\end{aligned}\n\n\nIf there is large amount of data, \\mu_{n} converges to maximum likelihood estimate.\nIf there is no observed data, \\mu_{n} converges to the mean of the prior knowledge.\n\nIf the number of sampled data n is fixed,\n  \n\\begin{aligned}\n\\lim_{\\sigma_{0} \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n}\n& [\\lim_{\\sigma_{0} \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 1]\n\\\\\n\\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n& = \\mu_{0}\n& [\\lim_{\\sigma \\to \\infty} \\frac{n \\sigma_{0}^{2}}{n \\sigma_{0}^{2} + \\sigma^{2}} = 0],\n\\end{aligned}\n\nwhich means\n\n\\mu_{n} will converge to the maximum likelihood estimate \\bar{x}_{n} if our prior knowledge of the \\mu indicates that we have no certainty about \\mu_{n} (infinite variance).\n\\mu_{n} will converge to the mean of our prior knowledge \\mu_{0} if our prior knowledge of the \\mu indicates that we are very certain about \\mu_{0} (zero variance).\n\n\n\nPredictive distribution function\n\n\\begin{aligned}\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X})\n& = \\int \\mathbb{P}_{X \\mid \\mu}(x \\mid \\mu) \\mathbb{P}_{\\mu \\mid X} (\\mu \\mid \\mathcal{X}) \\mathop{d \\mu}\n\\\\\n& \\sim \\mathcal{N}(\\mu_{n}, \\sigma^{2} + \\sigma_{n}^{2})\n\\end{aligned}\n\n\n\nSelecting parameter priors\nWhen the number of samples n is large, the predictive distribution will not change much if we select different mean parameter priors. Consider the 2 extreme cases of the mean parameter priors.\n\nUniform prior (normal distribution with infinite variance).\nSince\n\n\\begin{aligned}\n\\lim_{\\sigma_{0}^{2} \\to \\infty} \\mu_{n}\n& = \\bar{x}_{n},\n\\\\\n\\lim_{\\sigma_{0}^{2} \\to \\infty} \\sigma_{n}\n& = \\frac{\\sigma^{2}}{n},\n\\end{aligned}\n\nthe predictive distribution is\n\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left(\n     \\bar{x}_{n}, \\sigma^{2} + \\frac{\\sigma^{2}}{n}\n\\right).\n\nDirac delta prior (normal distribution with zero variance).\nSince\n\n\\begin{aligned}\n\\lim_{\\sigma_{0} \\to 0} \\mu_{n}\n& = \\mu_{0},\n\\\\\n\\lim_{\\sigma_{0} \\to 0} \\sigma_{n}\n& = 0,\n\\end{aligned}\n\nthe predictive distribution is\n\n\\mathbb{P}_{X \\mid X}(x \\mid \\mathcal{X}) \\sim \\mathcal{N} \\left(\n     \\mu_{0}, \\sigma^{2} \\right\n).\n\nSince \\mu_{0} is \\bar{x}_{n} with extra points, \\mu_{0} = \\bar{x}_{n} when n is large."
  },
  {
    "objectID": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#references",
    "href": "Knowledge/Statistical Learning/3_Bayesian_Estimation.html#references",
    "title": "Bayesian Estimation",
    "section": "References",
    "text": "References\n\nhttps://www.bu.edu/sph/files/2014/05/Bayesian-Statistics_final_20140416.pdf"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html",
    "title": "Principle Component Analysis (PCA)",
    "section": "",
    "text": "Updated 01-13-2023 (First commited 02-27-2022)"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#preliminary",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#preliminary",
    "title": "Principle Component Analysis (PCA)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nExpected value (Expectation)\nGiven a random variable X, its expected value (expectation) is its mean value.\n \\mathbb{E}[X] = \\mu \nwhere \\mu is the mean value of the random variable X.\n\nLinearity of Expectation\n \\mathbb{E}[X + Y] = \\mathbb{E}[X] + E [Y]   \\mathbb{E}[aX + b] = \\mathbb{E}[a] \\mathbb{E}[X] +\\mathbb{E}[b] = a \\mathbb{E}[X] +b \nIf X is a discrete random variable with a finite number of values x_{1}, x_{2}, \\dots, x_{k} occurring with probabilities p_{1}, p_{2}, \\dots, p_{k}, respectively, the expected value of X is defined as\n \\mathbb{E}[X] = \\sum_{i=1}^{k} x_{i}p_{i} \nIf the possibilities are the same (p_{1} = p_{2} = \\dots = p_{n} = \\frac{1}{n}), then\n \\mathbb{E}[X] = \\frac{1}{n} \\sum_{i=1}^{k} x_{i} \nIn general, given a discrete random variable X with probability mass function (PMF) P_{X}(x), the expected value of a function f(x) that takes X as the inputs is\n \\mathbb{E}_{X}[f(X)] = \\sum_{x \\in X} f(x)P_{X}(x) \n\n\n\nVariance\nThe variance of a random variable X is the expected value of the squared deviation from the mean of X:\n \\operatorname{var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}] \nwhich can also be expressed as:\n\n\\begin{align}\n\\operatorname{var}(X) & = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}] \\\\\n& = \\mathbb{E}[X^{2} - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^{2}] \\\\\n& = \\mathbb{E}[X^{2}] - 2\\mathbb{E}[X]\\mathbb{E}[X] + \\mathbb{E}[X]^{2} & \\text{[Linearity of Expectation]}\\\\\n& = \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n\\end{align}\n\n\n\nCovariance\nThe covariance of two random variables X and Y measures the strength of the correlation between them:\n\n\\begin{align}\n\\operatorname{cov}(X, Y) & = \\mathbb{E}[(X - \\mathbb{E}[X]) (Y - \\mathbb{E}[Y])] \\\\\n& = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n\\end{align}\n\n\nCovariance tells the relation between how Y changes from \\mathbf{E}(Y) when X changes from \\mathbf{E}(X).\n\nIf \\operatorname{cov}(X, Y) &gt; 0, then Y tends to increase as X increases (positive correlation).\nIf \\operatorname{cov}(X, Y) &lt; 0, then Y tends to decrease as X increases (negative correlation).\nIf \\operatorname{cov}(X, Y) = 0, then X and Y are uncorrelated.\n\nIf we sample n observations from X and Y to get vectors \\mathbf{x} and \\mathbf{y}, then we can actually compute \\operatorname{cov}(X, Y) based on the values in \\mathbf{x} and \\mathbf{y}, since the expected value of a random variable is just the mean of the its observations:\n \\operatorname{cov}(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{x}_{i} - \\bar{\\mathbf{x}}) (\\mathbf{y}_{i} - \\bar{\\mathbf{y}}) \nwhere \\bar{\\mathbf{x}} and \\bar{\\mathbf{y}} are the mean of \\mathbf{x} and \\mathbf{y}, respectively.\nIn the special case of Y = X, the covariance reduces to variance.\n\n\\begin{align}\n\\operatorname{cov}(X, X) & = \\mathbb{E}[(X - \\mathbb{E}[X]) (X - \\mathbb{E}[X])] \\\\\n& = \\mathbb{E}[(X - \\mathbb{E}[X])^{2}] \\\\\n& = \\operatorname{var}(X) \\\\\n\\end{align}\n\n\n\n\nCovariance matrix (variance-covariance matrix)\nGiven n random variables X_{1}, X_{2}, \\dots, X_{n}, the covariance matrix \\mathbf{V} is a square matrix of the size n \\times n giving the covariance between every pair of random variables, where\n \\mathbf{V}_{i, j} = \\operatorname{cov}(X_{i}, X_{j}) \n\n\\mathbf{V} is always symmetric, since \\operatorname{cov}(X_{i}, X_{j}) = \\operatorname{cov}(X_{j}, X_{i}).\nThe diagonal elements of \\mathbf{V} are the variances of the random variables (\\mathbf{V}_{i, i} = \\operatorname{cov}(X_{i}, X_{i}) = \\operatorname{var}(X_{i})).\nGiven a matrix \\mathbf{X} that has n observations (rows) and d variables (columns), let\n \\mathbf{A} = \\mathbf{X}^{T}\\mathbf{X} \nThen, each element of the matrix \\mathbf{A} is the dot product of each pair of the columns in \\mathbf{X}.\n\n\\begin{align}\n\\mathbf{A}_{i, j} & = \\mathbf{X}_{*, i} \\cdot \\mathbf{X}_{*, j} \\\\\n& = \\sum_{k=1}^{n} \\mathbf{X}_{k, i}\\mathbf{X}_{k, j} \\\\\n\\end{align}\n\nNow let’s look at the covariance between each pair of the variables (columns) in \\mathbf{X},\n \\operatorname{cov}(\\mathbf{X}_{*, i}, \\mathbf{X}_{*, j}) = \\frac{1}{n} \\sum_{k=1}^{n} (\\mathbf{X}_{k, i} - \\bar{\\mathbf{X}}_{k, i}) (\\mathbf{X}_{k, j} - \\bar{\\mathbf{X}}_{k, j}) \nAssuming that the each variable (column) of \\mathbf{X} is zero-centered (the means of the columns in \\mathbf{X} are 0):\n\n\\begin{align}\n\\operatorname{cov}(\\mathbf{X}_{*, i}, \\mathbf{X}_{*, j}) & = \\frac{1}{n} \\sum_{k=1}^{n} (\\mathbf{X}_{k, i} - \\bar{\\mathbf{X}}_{k, i}) (\\mathbf{X}_{k, j} - \\bar{\\mathbf{X}}_{k, j}) \\\\\n& = \\frac{1}{n} \\sum_{k=1}^{n} \\mathbf{X}_{k, i} \\mathbf{X}_{k, j} \\\\\n& = \\frac{1}{n} \\mathbf{A}_{i, j}\n\\end{align}\n\nThus, the covariance matrix \\mathbf{V} between each pair of the variables in zero-centered matrix \\mathbf{X} is\n \\mathbf{V} = \\frac{1}{n} \\mathbf{X}^{T}\\mathbf{X} \n\n\n\n\nLinear algebra\n\nOrthogonal\nTwo vectors \\mathbf{a} and \\mathbf{b} are orthogonal if their inner product is 0:\n \\mathbf{a} \\cdot \\mathbf{b} = 0 \n\n\nVector projection\nThe vector projection of a vector \\mathbf{a} on a vector \\mathbf{b} is the orthogonal projection of \\mathbf{a} onto \\mathbf{b}:\n\n\\begin{align}\n\\operatorname{proj}_{\\mathbf{b}}\\mathbf{a} & = (\\lvert \\mathbf{a} \\rvert \\cos(\\theta)) \\hat{\\mathbf{b}} \\\\\n& = (\\mathbf{a} \\cdot \\hat{\\mathbf{b}}) \\hat{\\mathbf{b}} \\\\\n\\end{align}\n\nwhere \\lvert \\mathbf{a} \\rvert is the length of \\mathbf{a}, \\theta is the angle between \\mathbf{a} and \\mathbf{b}, and \\hat{\\mathbf{b}} is the unit vector that has the same direction with \\mathbf{b}.\n\n\nOrthonormal basis, orthonormal matrix\nThe vectors \\hat{\\mathbf{v}}_{1}, \\hat{\\mathbf{v}}_{2}, \\dots, \\hat{\\mathbf{v}}_{d} \\in \\mathbb{R}^{d} form an orthonormal basis for the space V with d dimensions if the vectors \\hat{\\mathbf{v}}_{1}, \\hat{\\mathbf{v}}_{2}, \\dots, \\hat{\\mathbf{v}}_{d} are unit vectors and orthogonal to each other.\nGiven that the vectors \\hat{\\mathbf{v}}_{1}, \\hat{\\mathbf{v}}_{2}, \\dots, \\hat{\\mathbf{v}}_{d} form an orthonormal basis for the space V, the projection of a vector \\mathbf{w} onto V is the sum of the projections of \\mathbf{w} onto the individual basis vectors:\n \\tilde{\\mathbf{w}} = \\sum_{i=1}^{d} (\\mathbf{w} \\cdot \\hat{\\mathbf{v}}_{i}) \\hat{\\mathbf{v}}_{i} \n\n\nEigenvectors, Eigenvalues\nGiven a matrix \\mathbf{A} \\in \\mathbb{R}^{n \\times n}, \\lambda is said to be an eigenvalue of \\mathbf{A} if there exists a eigenvector \\mathbf{z} \\in \\mathbb{R}^n \\neq 0, such that:\n \\mathbf{A}\\mathbf{z} = \\lambda\\mathbf{z} \n\nIt can interpreted as: the application of \\mathbf{A} to \\mathbf{z} is the same as changing the length of \\mathbf{z} by a factor of \\lambda without changing \\mathbf{z}’s direction\n\n\n\nEigendecomposition (spectral decomposition)\nLet \\mathbf{M} be a real symmetric d \\times d matrix with eigenvalues \\lambda_{1}, \\lambda_{2}, \\dots , \\lambda_{d} and corresponding orthonormal eigenvectors \\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d}. Then:\n \\mathbf{M} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T \nwhere \\mathbf{\\Lambda} is a diagonal matrix with \\lambda_{1}, \\lambda_{2}, \\dots , \\lambda_{d} in diagonal and 0 elsewhere and \\mathbf{Q} matrix has \\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d} vectors as columns."
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#pca-algorithm",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#pca-algorithm",
    "title": "Principle Component Analysis (PCA)",
    "section": "## PCA algorithm",
    "text": "## PCA algorithm\nFunction: PCA.\nInput: A matrix \\mathbf{X} \\in \\mathbb{R}^{n \\times d} and an integer value indicating the objective dimension m.\nOutput: a transformed matrix in low dimension \\hat{\\mathbf{X}} \\in \\mathbb{R}^{n \\times m}.\n1. Standardize the input. For j in [1, 2, \\dots, d]\n$$ \\mathbf{X}_{*, j} = \\frac{\\mathbf{X}_{*, j} - \\operatorname{mean}(\\mathbf{X}_{*, j})}{\\operatorname{std}(\\mathbf{X}_{*, j})} $$.\n\nCalculate the covariance matrix between columns.\n \\mathbf{V} = \\frac{1}{n} \\mathbf{X}^{T}\\mathbf{X} .\nUse eigendecomposition to decompose \\mathbf{V} to get a list of eigenvalues \\lambda_{1}, \\lambda_{2}, \\dots, \\lambda_{d} and corresponding eigenvectors \\mathbf{u}_{1}, \\mathbf{u}_{1}, \\dots \\mathbf{u}_{d}.\nSort eigenvalues in the decreasing order and select the eigenvectors with m largest eigenvalues. View the m eigenvectors as m columns of the matrix \\mathbf{E} \\in \\mathbb{R}^{d \\times m}.\nGet the transformed matrix \\hat{\\mathbf{X}} in m dimensions.\n \\hat{\\mathbf{X}} = \\mathbf{X}\\mathbf{E} .\nReturn \\hat{\\mathbf{X}}."
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#pca-objective-derivation",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#pca-objective-derivation",
    "title": "Principle Component Analysis (PCA)",
    "section": "PCA objective derivation",
    "text": "PCA objective derivation\n\n\nProjection of a single instance (vector) to the subspace\nAn instance \\mathbf{x} \\in \\mathbb{R}^{d} can be projected to any given orthonormal basis \\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{d} \\in \\mathbb{R}^{d} of dimension d without any error:\n \\mathbf{x} = \\tilde{\\mathbf{x}} = \\sum_{i=1}^{d} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \nHowever, there is a inevitable reconstruction error if \\mathbf{x} is projected to only m (m &lt; d) dimensions \\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{m}:\n \\mathbf{x} \\neq \\tilde{\\mathbf{x}} = \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \nWe can measure the error by the squared distance:\n\n\\begin{align}\n\\operatorname{err} & = \\lVert \\mathbf{x} - \\tilde{\\mathbf{x}} \\rVert^{2} \\\\\n& = \\big\\lVert \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\big\\rVert^{2} \\\\\n& = \\left( \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\right) \\cdot \\left( \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) \\hat{\\mathbf{w}}_{i} \\right) \\\\\n& = \\mathbf{x} \\cdot \\mathbf{x} - \\mathbf{x} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} - \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\cdot \\mathbf{x} + \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\\\\n& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\left( \\mathbf{x} \\cdot \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i} \\right) + \\sum_{i=1}^{m} ((\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i}) ((\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})\\hat{\\mathbf{w}}_{i}) & [\\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{j} = 0 \\text{ if } i \\neq j] \\\\\n& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) + \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i}) (\\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i}) \\\\\n& = \\mathbf{x} \\cdot \\mathbf{x} - 2 \\sum_{i=1}^{m}(\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} + \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} & [\\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1] \\\\\n& = \\mathbf{x} \\cdot \\mathbf{x} - \\sum_{i=1}^{m} (\\mathbf{x} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\end{align}\n\n\n\nPCA as minimizing projection error\nGiven a dataset of n instances and d variables, we can use mean squared error to measure the reconstruction error of all instances in the dataset projected to m dimensions:\n\n\\begin{align}\n\\operatorname{MSE} & = \\frac{1}{n} \\sum_{j=1}^{n} \\lVert \\mathbf{x}_{j} - \\tilde{\\mathbf{x}}_{j}\\rVert^{2} \\\\\n& = \\frac{1}{n} \\sum_{j=1}^{n} \\left( \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j} - \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\right) \\\\\n& = \\frac{1}{n} \\sum_{j=1}^{n} \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j} - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\end{align}\n\nThe goal of PCA is to get a particular orthonormal basis of only m dimensions such that the mean squared error of projecting all instances on to it is minimized:\n\n\\begin{align}\n\\min \\quad &  - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n\\end{align}\n\n\nThe constraint is added so that the \\hat{\\mathbf{w}}_{1}, \\hat{\\mathbf{w}}_{2}, \\dots, \\hat{\\mathbf{w}}_{m} are all unit vectors.\nThe first term \\frac{1}{n} \\sum_{j=1}^{n} \\mathbf{x}_{j} \\cdot \\mathbf{x}_{j} is omitted in the objective because it is a fixed value once the dataset is provided.\n\n\n\nPCA as maximizing projection variance\nThe objective of minimizing projection error defined above is the same as maximizing its negation.\n\n\\begin{align}\n\\min \\quad & - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\max \\quad & \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\end{align}\n\nThe variance of the projection of all instances to the dimension \\hat{\\mathbf{w}}_{i} is:\n\n\\begin{align}\n\\operatorname{var}(X) & = \\mathbb{E}[X^{2}] - \\mathbb{E}[X]^{2} \\\\\n\\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) & = \\mathbb{E}[(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2}] - \\mathbb{E}[\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}]^{2} \\\\\n\\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) & = \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 - \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 \\\\\n\\end{align}\n\nThe sum of the variances of the projections to all m dimensions is:\n \\sum_{i=1}^{m} \\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) = \\sum_{i=1}^{m} \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 - \\sum_{i=1}^{m} \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 \nSince the dataset is preprocessed to be zero-centered (each variable has mean 0), the last term of the equation above becomes 0:\n \\sum_{i=1}^{m} \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 = \\sum_{i=1}^{m} \\left( \\left( \\frac{1}{n}\\sum_{j=1}^{n}\\mathbf{x}_{j} \\right) \\cdot \\hat{\\mathbf{w}}_{i} \\right)^2 = \\sum_{i=1}^{m} (0 \\cdot \\hat{\\mathbf{w}}_{i})^{2} = 0 \nThus, we can see that minimizing projection error is the same as maximizing the sum of the projection variance:\n \\sum_{i=1}^{m} \\operatorname{var}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i}) = \\sum_{i=1}^{m} \\frac{1}{n}\\sum_{j=1}^{n}(\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^2 = \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2}"
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#solving-pca-objective",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#solving-pca-objective",
    "title": "Principle Component Analysis (PCA)",
    "section": "## Solving PCA objective",
    "text": "## Solving PCA objective\nGiven the minimization problem:\n\n\\begin{align}\n\\min \\quad & - \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n\\end{align}\n we can first rewrite the objective in the matrix form:\n\n\\begin{align}\n& \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{i=1}^{m} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n= & \\sum_{i=1}^{m} \\frac{1}{n} \\sum_{j=1}^{n} (\\mathbf{x}_{j} \\cdot \\hat{\\mathbf{w}}_{i})^{2} \\\\\n= & \\sum_{i=1}^{m} \\frac{1}{n} (\\mathbf{X}\\hat{\\mathbf{w}}_{i})^{T} (\\mathbf{X}\\hat{\\mathbf{w}}_{i}) \\\\\n= & \\sum_{i=1}^{m} \\frac{1}{n} \\hat{\\mathbf{w}}_{i}^{T}\\mathbf{X}^{T} \\mathbf{X}\\hat{\\mathbf{w}}_{i} \\\\\n= & \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\frac{\\mathbf{X}^{T}\\mathbf{X}}{n} \\hat{\\mathbf{w}}_{i} \\\\\n\\end{align}\n\nSince we have already zero-centered the dataset, we can replace \\frac{\\mathbf{X}^{T}\\mathbf{X}}{n} with the covariance matrix \\mathbf{V}. Thus, the minimization problem in the matrix form is:\n\n\\begin{align}\n\\min \\quad & - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} \\\\\n\\text{s.t. } \\quad & \\hat{\\mathbf{w}}_{i} \\cdot \\hat{\\mathbf{w}}_{i} = 1, \\quad i = 1, \\dots m \\\\\n\\end{align}\n\nThe Lagrangian of the optimization problem is:\n L(\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{m}, \\lambda_{1} \\dots, \\lambda_{m}) = - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} + \\sum_{i=1}^{m} \\lambda_{i}(\\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} - 1) \nSolving L by 1. Setting the derivative of L w.r.t \\hat{\\mathbf{w}}_{i} to be 0:\n$$\n\\begin{align}\n\\frac{\\partial L}{\\partial \\hat{\\mathbf{w}}}_{i} & = 0 \\\\\n-\\sum_{i=1}^{m} 2\\mathbf{V}\\hat{\\mathbf{w}}_{i} + \\sum_{i=1}^{m} 2\\lambda_{i}\\hat{\\mathbf{w}}_{i} & = 0 \\\\\n\\sum_{i=1}^{m} 2\\mathbf{V}\\hat{\\mathbf{w}}_{i} & = \\sum_{i=1}^{m} 2\\lambda_{i}\\hat{\\mathbf{w}}_{i} \\\\\n\\mathbf{V}\\hat{\\mathbf{w}}_{i} & = \\lambda_{i}\\hat{\\mathbf{w}}_{i}, \\quad i = 1, \\dots m \\\\\n\\end{align}\n$$\n\nThe results show that the results we want are the eigenvectors $\\hat{\\mathbf{w}}_{i}$ and eigenvalues $\\lambda_{i}$ of $\\mathbf{V}$. \n\nSetting the derivative of L w.r.t \\lambda_{i} to be 0:\n\n\\begin{align}\n\\frac{\\partial L}{\\partial \\lambda_{i}} & = 0 \\\\\n\\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} - 1 & = 0 \\\\\n\\hat{\\mathbf{w}}_{i}^{T}\\hat{\\mathbf{w}}_{i} & = 1, \\quad i = 1, \\dots m \\\\\n\\end{align}\n\nThe constraints show that the eigenvectors must also be unit vectors.\nPlug in the results back to the objective:\n -\\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\mathbf{V} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\hat{\\mathbf{w}}_{i}^{T} \\lambda_{i} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\lambda_{i} \\hat{\\mathbf{w}}_{i}^{T} \\hat{\\mathbf{w}}_{i} = - \\sum_{i=1}^{m} \\lambda_{i} \nThe last equation shows that we need to select the m largest eigenvalues to minimize the objective."
  },
  {
    "objectID": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#reference",
    "href": "Knowledge/Unsupervised Learning/Principle Component Analysis (PCA).html#reference",
    "title": "Principle Component Analysis (PCA)",
    "section": "Reference",
    "text": "Reference\n\n\nhttps://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\nhttps://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\nhttps://towardsdatascience.com/dimensionality-reduction-with-pca-from-basic-ideas-to-full-derivation-37921e13cae7"
  },
  {
    "objectID": "Knowledge/Learning Theory/1_Statistical_Learning.html",
    "href": "Knowledge/Learning Theory/1_Statistical_Learning.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "The goal of machine learning is to use algorithms to learn from the data. The data here refers to labeled instances (x, y) \\in \\mathcal{X} \\times \\mathcal{Y}, where\n\nThe instance x is usually a vector that belongs to an instance space \\mathcal{X};\nThe label y is usually a scalar that belongs to a label space \\mathcal{Y}.\n\nFor simplicity, we will write the labeled instances z \\coloneqq (x, y) and the space of labeled instances as \\mathcal{Z} \\coloneqq \\mathcal{X} \\times \\mathcal{Z}.\nMachine learning problems usually have two sets of data:\n\nTraining set: the training set consists of a finite number of labeled instances from which the algorithms can learn.\nTest set: the test set may consist of an infinite number of labeled instances that are used to evaluate the performance of the algorithm in a real-world setting.\n\n\n\n\n\nA decision function is a function f: \\mathcal{X} \\to \\mathcal{Y} whose domain is \\mathcal{X} and the range is \\mathcal{Y}\n\n\\hat{y} = f (x)\n\nthat maps each input instance x \\in \\mathcal{X} to a label y \\in \\mathcal{Y}.\nHere we have two types of decision functions that have slightly different meanings in the context of machine learning\n\nConcept c and concept class C: a concept from a concept class c \\in \\mathcal{C} is the decision function that the algorithm wants to learn, which assigns all correct labels for given instances.\nHypothesis h and hypothesis class H: a hypothesis from a hypothesis class h \\in \\mathcal{H} is the decision function that the algorithm actually learns from the hypothesis class.\n\n\n\n\nThe way we evaluate a function f on a labeled instance (x, y) is determined by a loss function L: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}^{+}\n\nL (z) = L (f (x), y)\n\nwhich calculates some notion of discrepancy between the true label y and the predicated label \\hat{y} = f (x).\nAll the loss functions used in this note are 0-1 loss\n\nL (z) = L (f (x), y) = \\mathbb{1} \\left[\n    f (x) \\neq y\n\\right],\n\nwhich incurs a loss of 1 if the predicated label is the same as the true label and 0 if they are the same.\n\n\n\n\nIn a statistical learning problem, each labeled instance is an independent and identically distributed (i.i.d.) draw from some fixed but unknown joint distribution \\mathbb{P}_{X, Y} over \\mathcal{X} \\times \\mathcal{Y} that describes the probability that both x and y happens in the real world.\nThis means that there is always a probability associated with each term:\n\nthe distribution \\mathbb{P}_{X} for a multivariate random variable X that describes the probability of an instance x\nthe distribution \\mathbb{P}_{Y} for a random variable Y that describes the probability of a label y.\n\nWe can decompose the joint probability according to the chain rule:\n\n\\mathbb{P}_{X, Y}(x, y) = \\mathbb{P}_{X \\mid Y}(x \\mid y) \\mathbb{P}_{Y}(y),\n\nwhere \\mathbb{P}_{X \\mid Y}(x \\mid y) is called class conditional probability, which gives the probability of the instance if we know the label is y.\nFor simplicity, sometimes we will write \\mathbb{P}_{Z} \\coloneqq \\mathbb{P}_{X, Y} to denote the probability of the labeled instance.\n\n\nThe true risk of the hypothesis h is defined as the expectation of the loss function over the joint probability\n\nR (h) =  \\mathbb{E}_{X, Y} [L (h (X), Y)] = \\mathbb{E}_{Z} [L (Z)]\n\nwhich is the probability that h makes a mistake if the loss function is 0-1 loss\n\nR (h) = \\mathbb{P}_{X, Y} \\left[\n    \\mathbb{1} \\left[\n        h (x) \\neq y\n    \\right]\n\\right].\n\n\nLemma 1 Apart from the expectation with respect to the join probability of \\mathbb{P}_{X, Y}, the true risk of any hypothesis h can also be written as the expectation of the conditional expectation of the loss function\n\nR(h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is based on the definition of the expectation and the chain rules of the probability\n\n\\begin{aligned}\nR(h)\n& = \\mathbb{E}_{X, Y} [L (h (X), Y)]\n\\\\\n& = \\int \\int \\mathbb{P}_{X, Y} (x, y) L (h (x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{definition of } \\mathbb{E} [\\cdot]]\n\\\\\n& = \\int \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) \\mathbb{P}_{X} (x) L (g(x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{probability chain rule}]\n\\\\\n& = \\int \\mathbb{P}_{X} (x) \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) L (g(x), y) \\mathop{dy} \\mathop{d x}\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\\end{aligned}\n\n\n\n\n\n\n\nThe empirical risk function is used with the past data of n labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} as a surrogate function for the risk function\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (x_{i}), y) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i}),\n\nwhich is the average number of mistakes h made in \\mathcal{D}^{n} if the loss is 0-1 loss\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{1} \\left[\n    h (x_{i}) \\neq y_{i}\n\\right].\n\nThe idea is that if the past data we have is representative of the actual distribution, then it will be the case that the empirical risk will be close to the true risk.\n\n\n\nThe empirical risk is an unbiased estimator of the true risk. That is, the expectation of the empirical risk over all samples is the true risk\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right] = R (h).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right]\n& = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i})\n\\right]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{Z} [L (z_{i})]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} R (h)\n\\\\\n& = R (h).\n\\end{aligned}\n\n\n\n\nAlso, by the law of large numbers, we have R_{n} (h) \\to R(h) as n \\to \\infty, almost surely, which means the empirical risk is closed to the true risk if the sample size is large enough.\n\n\n\n\n\n\n\nComplement rule\n\n\n\\mathbb{P} (A &gt; t) &lt; \\delta \\implies \\mathbb{P} (A \\leq t) \\geq 1 - \\delta"
  },
  {
    "objectID": "Knowledge/Learning Theory/1_Statistical_Learning.html#functions",
    "href": "Knowledge/Learning Theory/1_Statistical_Learning.html#functions",
    "title": "Statistical Learning",
    "section": "",
    "text": "A decision function is a function f: \\mathcal{X} \\to \\mathcal{Y} whose domain is \\mathcal{X} and the range is \\mathcal{Y}\n\n\\hat{y} = f (x)\n\nthat maps each input instance x \\in \\mathcal{X} to a label y \\in \\mathcal{Y}.\nHere we have two types of decision functions that have slightly different meanings in the context of machine learning\n\nConcept c and concept class C: a concept from a concept class c \\in \\mathcal{C} is the decision function that the algorithm wants to learn, which assigns all correct labels for given instances.\nHypothesis h and hypothesis class H: a hypothesis from a hypothesis class h \\in \\mathcal{H} is the decision function that the algorithm actually learns from the hypothesis class.\n\n\n\n\nThe way we evaluate a function f on a labeled instance (x, y) is determined by a loss function L: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}^{+}\n\nL (z) = L (f (x), y)\n\nwhich calculates some notion of discrepancy between the true label y and the predicated label \\hat{y} = f (x).\nAll the loss functions used in this note are 0-1 loss\n\nL (z) = L (f (x), y) = \\mathbb{1} \\left[\n    f (x) \\neq y\n\\right],\n\nwhich incurs a loss of 1 if the predicated label is the same as the true label and 0 if they are the same."
  },
  {
    "objectID": "Knowledge/Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "href": "Knowledge/Learning Theory/1_Statistical_Learning.html#learning-in-a-probability-setting",
    "title": "Statistical Learning",
    "section": "",
    "text": "In a statistical learning problem, each labeled instance is an independent and identically distributed (i.i.d.) draw from some fixed but unknown joint distribution \\mathbb{P}_{X, Y} over \\mathcal{X} \\times \\mathcal{Y} that describes the probability that both x and y happens in the real world.\nThis means that there is always a probability associated with each term:\n\nthe distribution \\mathbb{P}_{X} for a multivariate random variable X that describes the probability of an instance x\nthe distribution \\mathbb{P}_{Y} for a random variable Y that describes the probability of a label y.\n\nWe can decompose the joint probability according to the chain rule:\n\n\\mathbb{P}_{X, Y}(x, y) = \\mathbb{P}_{X \\mid Y}(x \\mid y) \\mathbb{P}_{Y}(y),\n\nwhere \\mathbb{P}_{X \\mid Y}(x \\mid y) is called class conditional probability, which gives the probability of the instance if we know the label is y.\nFor simplicity, sometimes we will write \\mathbb{P}_{Z} \\coloneqq \\mathbb{P}_{X, Y} to denote the probability of the labeled instance.\n\n\nThe true risk of the hypothesis h is defined as the expectation of the loss function over the joint probability\n\nR (h) =  \\mathbb{E}_{X, Y} [L (h (X), Y)] = \\mathbb{E}_{Z} [L (Z)]\n\nwhich is the probability that h makes a mistake if the loss function is 0-1 loss\n\nR (h) = \\mathbb{P}_{X, Y} \\left[\n    \\mathbb{1} \\left[\n        h (x) \\neq y\n    \\right]\n\\right].\n\n\nLemma 1 Apart from the expectation with respect to the join probability of \\mathbb{P}_{X, Y}, the true risk of any hypothesis h can also be written as the expectation of the conditional expectation of the loss function\n\nR(h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is based on the definition of the expectation and the chain rules of the probability\n\n\\begin{aligned}\nR(h)\n& = \\mathbb{E}_{X, Y} [L (h (X), Y)]\n\\\\\n& = \\int \\int \\mathbb{P}_{X, Y} (x, y) L (h (x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{definition of } \\mathbb{E} [\\cdot]]\n\\\\\n& = \\int \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) \\mathbb{P}_{X} (x) L (g(x), y) \\mathop{d x} \\mathop{dy}\n& [\\text{probability chain rule}]\n\\\\\n& = \\int \\mathbb{P}_{X} (x) \\int \\mathbb{P}_{Y \\mid X} (y \\mid x) L (g(x), y) \\mathop{dy} \\mathop{d x}\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\\end{aligned}\n\n\n\n\n\n\n\nThe empirical risk function is used with the past data of n labeled instances \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} as a surrogate function for the risk function\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (h (x_{i}), y) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i}),\n\nwhich is the average number of mistakes h made in \\mathcal{D}^{n} if the loss is 0-1 loss\n\nR_{\\mathcal{S}} (h) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{1} \\left[\n    h (x_{i}) \\neq y_{i}\n\\right].\n\nThe idea is that if the past data we have is representative of the actual distribution, then it will be the case that the empirical risk will be close to the true risk.\n\n\n\nThe empirical risk is an unbiased estimator of the true risk. That is, the expectation of the empirical risk over all samples is the true risk\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right] = R (h).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    R_{\\mathcal{S}} (h)\n\\right]\n& = \\mathbb{E}_{Z} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} L (z_{i})\n\\right]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{Z} [L (z_{i})]\n\\\\\n& = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} R (h)\n\\\\\n& = R (h).\n\\end{aligned}\n\n\n\n\nAlso, by the law of large numbers, we have R_{n} (h) \\to R(h) as n \\to \\infty, almost surely, which means the empirical risk is closed to the true risk if the sample size is large enough."
  },
  {
    "objectID": "Knowledge/Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "href": "Knowledge/Learning Theory/1_Statistical_Learning.html#some-probability-facts",
    "title": "Statistical Learning",
    "section": "",
    "text": "Complement rule\n\n\n\\mathbb{P} (A &gt; t) &lt; \\delta \\implies \\mathbb{P} (A \\leq t) \\geq 1 - \\delta"
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "Rademacher complexity measures the complexity of a function class in a sense that if \\mathcal{F} contains so many functions such that there exists some functions in \\mathcal{F} that can always output the same signs with the random generated Rademacher random variables, then \\mathcal{F} will have a high Rademacher complexity.\n\n\n\nDefinition 1 A Rademacher variable has a discrete probability distribution where X has the equal probability of being +1 and -1.\n\nThe empirical Rademacher complexity measures the ability of the functions in a function class \\mathcal{F} to fit the random noise for a fixed sample \\mathcal{S}, which is described by the maximum correlation over all f \\in \\mathcal{F} between f (z_{i}) and \\sigma_{i}.\n\nDefinition 2 Given an i.i.d sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} from the distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the empirical Rademacher complexity of a class of binary function \\mathcal{F} is defined as\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}) = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right],\n\nwhich is a function of the random variable \\mathcal{S} and therefore is a random variable.\n\nTherefore, the Rademacher complexity of \\mathcal{F} measures the expected noise-fitting-ability of \\mathcal{F} over all data sets \\mathcal{S} \\in \\mathcal{Z}^{n} that could be drawn according to the distribution \\mathbb{P}_{\\mathcal{Z}^{n}}.\n\nDefinition 3 Then the Rademacher complexity is defined as the expectation of the empirical Rademacher complexity over all i.i.d samples of size n\n\n\\mathrm{Rad}_{n} (\\mathcal{F}) = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\\right] = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right]\n\\right].\n\n\nFor completeness, we include the definition of Rademacher average of a set of vectors.\n\nDefinition 4 Given n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the Rademacher average of a set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} is\n\n\\mathrm{Rad}_{\\mathcal{A}} = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right].\n\n\n\n\n\n\n\nThe empirical Rademacher complexity and Rademacher complexity are non-negative.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& \\stackrel{(1)}{\\geq} \\sup_{f \\in \\mathcal{F}} \\mathbb{E}_{\\sigma} \\left[\n     \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& = \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{\\sigma} [ \\sigma_{i} ] f (z_{i})\n\\\\\n& \\stackrel{(2)}{=} 0.\n\\end{aligned}\n\nExplanations in the derivations\n\nSince \\sup is a convex function, (1) follows because of the Jensen’s inequality.\n\nfollows because of the definition of Rademacher variable.\n\n\n\n\n\n\n\n\nGiven any function class \\mathcal{F} and constants a, b \\in \\mathbb{R}, denote the function class \\mathcal{G} = \\{ g (x) = a f (x) + b \\}.\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\n\n\\mathrm{Rad}_{n} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{n} (\\mathcal{F}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition of \\mathcal{G} and the empirical Rademacher complexity,\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (a f (z_{i}) + b)\n    \\right)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n        + \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n+ \\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(3)}{=} \\lvert a \\rvert \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right)\n\\right]\n\\\\\n& = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\nSince the term \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b does not depend on f, it can be pulled out of \\sup_{f \\in \\mathcal{F}}. Then (1) follows because of the linearity of expectation.\n\nfollows because of the linearity of expectation and \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nWhen a &lt; 0, \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i}) = \\lvert a \\rvert \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}). Then (3) follows since \\sigma_{i} and -\\sigma_{i} have the same distribution.\n\n\n\n\n\n\n\n\nHere we proved an important result with the Rademacher complexity using so called symmetrization technique, which involves creating a “ghost” sample as a hypothetical independent copy of the original sample.\n\nTheorem 1 For any class of measurable functions \\mathcal{F}, the expectation of the maximum error in estimating the mean of any function f \\in \\mathcal{F} is bounded by 2 times of the Rademacher complexity\n\n\\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n= \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right] \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n\nwhere E_{\\mathcal{S}} (f) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i}) is the estimated expectation of f on the sample \\mathcal{S}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy using the symmetrization technique, we introduce a ghost sample \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} that is also i.i.d drawn from \\mathbb{P}_{\\mathcal{Z}^{n}}, which means\n\n\\mathbb{E}_{\\mathcal{S}'} \\left[\n    E_{\\mathcal{S}'} (f)\n\\right] = \\mathbb{E}_{Z} [f (z_{i})].\n\nTherefore, we can get the following results\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i})\n        - \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{z_{i}' \\in \\hat{\\mathcal{S}}} f (z_{i}')\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nuses the linearity of expectation and \\frac{ 1 }{ n } \\sum_{z_{i} \\in \\mathcal{S} f (z_{i})} is a constant.\n\n\nuses Jensen’s inequality since \\sup is a convex operator.\n\n\nSince f (z_{i}) - f (z_{i}') is invariant of sign change, we get\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}', \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right\\rvert\n\\right]\n+ \\mathbb{E}_{\\hat{\\mathcal{S}}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}')\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{=} 2 \\mathrm{Rad}_{n} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows because of the \\sup_{f \\in \\mathcal{F}} operator,\n\n\nfollows because \\sigma_{i} = - \\sigma_{i} by the definition of Rademacher variable.\n\n\nTherefore we have reached our conclusion\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n\\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F}).\n\n\n\n\n\n\n\n\nTheorem 2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, if the function class \\mathcal{F} only contains the functions f such that f (x) \\in [a, a + 1], then for every f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall f \\in \\mathcal{F}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{F}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven a function f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation on a sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} is less than the maximum difference of the expectations among all functions in \\mathcal{F}, which is denoted by \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert  \\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert] = \\phi (\\mathcal{S}).\n\nFirst we will prove the following property so that we can use McDiarmid’s inequality on \\phi (\\mathcal{S})\n\n\\sup_{\\mathcal{S}, \\mathcal{S}'} \\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert \\leq \\frac{ 1 }{ n }\n\nwhere \\mathcal{S}' = \\{ z_{1}, \\dots, z_{j}', \\dots, z_{n} \\} has z_{j}' different from z_{j} in \\mathcal{S}.\nLet f^{*} \\in \\mathcal{F} be the function that maximizes \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n= \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S})\n\nand by definition\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert\n\\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}'} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S}').\n\nTherefore,\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\lvert \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n- \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert \\rvert\n\\\\\n& = \\lvert E_{\\mathcal{S}'} (f^{*}) - E_{\\mathcal{S}} (f^{*}) \\rvert\n\\\\\n& = \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert.\n\\end{aligned}\n\nSince \\mathcal{S} and \\mathcal{S}' only differ on two elements z_{j}, z_{j}', this becomes\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j})\n    \\right) - \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j}')\n    \\right)\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert f^{*} (z_{j}) - f^{*} (z_{j}') \\right\\rvert\n\\\\\n& \\leq \\frac{ 1 }{ n }.\n\\end{aligned}\n\nThis results show that the function \\phi follows the bounded difference property, so we can apply the McDiarmid’s inequality on \\phi,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (\\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq t)\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} \\left(\n        \\frac{ 1 }{ n }\n    \\right)^{2} }\n\\right]\n\\\\\n& \\leq e^{- 2 m t^{2}}.\n\\end{aligned}\n\nBy setting \\delta = e^{-2 n t^{2}}, we can derive that t = \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}, so\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\leq \\delta\n\\\\\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\leq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nwhich means we have the following fact with the probability larger than 1 - \\delta,\n\n\\phi (\\mathcal{S}) \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}.\n\nBy plugging back the result to the equation that we want to prove, we have the final results\n\n\\begin{aligned}\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f)] \\rvert\n& \\leq \\phi (\\mathcal{S})\n\\\\\n& \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\\\\n& \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\end{aligned}\n\nwith the probability larger than 1 - \\delta.\n\n\n\n\n\nGiven a hypothesis class \\mathcal{H}, a corresponding loss class with the 0-1 loss can be defined as\n\n\\mathcal{L} = \\{ l_{h} \\mid l_{h} (z) = L (h (\\mathbf{x}), y), h \\in \\mathcal{H}, z \\sim \\mathcal{Z} \\}\n\nand therefore the empirical risk and true risk can be defined as\n\nR_{\\mathcal{S}} (h) = E_{\\mathcal{S}} (l_{h}), R (h) = \\mathbb{E}_{\\mathcal{Z}} [l_{h} (z)].\n\nSince all the loss functions l_{h} \\in \\mathcal{L} have output range [0, 1], we can apply the uniform theorem above to the loss class \\mathcal{L} to derive the uniform convergence results for risks.\n\nCorollary 1 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, a hypothesis class \\mathcal{H}, and the corresponding 0-1 loss class \\mathcal{L}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{L}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\nBy using the following lemma, we can write the results in terms of the Rademacher complexity the hypothesis class \\mathcal{H} instead of the loss class \\mathcal{L}.\n\nLemma 1 Given a hypothesis class \\mathcal{H} and the corresponding loss class \\mathcal{L}, we have\n\n\\mathrm{Rad}_{n} (\\mathcal{H}) = 2 \\mathrm{Rad}_{n} (\\mathcal{L}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the definition of Rademacher complexity and 0-1 loss, we have\n$$\n\\begin{aligned}\n\\mathrm{Rad}_{S} (\\mathcal{H})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\mathbb{1} (h(x_{i}) \\neq y_{i})\n    \n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\left(\n        \\frac{ 1 }{ 2 } - y_{i} h (x_{i})\n    \\right)\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\left[\n        \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i}\n        + \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (-y_{i} h (x_{i}))\n    \\right]\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i}\n    + \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (- y_{i} h (x_{i}))\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} h (x_{i})\n\\right]\n& \\quad [\\mathbb{E}\\left[\\sum_{i=1}^{m}\\sigma_{i}\\right] = 0]\n\\\\\n& = \\frac{ 1 }{ 2 }\\text{Rad}_{S}(\\mathcal{H}).\n\\end{aligned}\n$$\n\n\n\n\nCorollary 2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and a hypothesis class \\mathcal{H}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{H}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\n\n\n\n\nThe Rademacher complexity can be upper bounded for any function class with a finite VC dimension.\n\n\n\nLemma 2 (Massart’s lemma) Given any set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} the empirical Rademacher average is upper-bounded\n\n\\mathrm{Rad} (\\mathcal{A}) \\leq \\frac{ R \\sqrt{2 \\log \\lvert \\mathcal{A} \\rvert} }{ n }\n\nwhere R = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\lVert \\mathbf{a} \\rVert_{2}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Jensen’s inequality, the following quantity can be upper-bounded\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\sigma} \\left[\n    \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\mathbb{E}_{\\sigma} \\left[\n    \\prod_{i = 1}^{n} \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows since \\exp is non-negative and therefore \\sup \\leq \\sum.\n\n\nfollows because of the independence between \\sigma_{i}.\n\n\nSince \\mathbb{E}_{\\sigma_{i} a_{i}} = 0, we can apply Hoeffding’s lemma with \\mu = 0\n\n\\begin{aligned}\n\\exp \\left[\n    s \\sigma_{i} a_{i}\n\\right]\n& \\leq \\exp \\left[\n    \\frac{ s^{2} (2 a_{i})^{2}}{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right],\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right].\n\\end{aligned}\n\nwhere the last inequality follows because \\sum_{i = 1}^{n} f (a_{i}) \\leq n \\sup_{a_{i}} f (a_{i}), \\forall f.\nCombining all pieces together,\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\end{aligned}\n\nwhere R^{2} = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}.\nSince \\log \\frac{ \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 } is a convex function, we can minimize it with respect to s\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n& = 0\n\\\\\n- \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s^{2} } + \\frac{ R^{2} }{ 2 }\n& = 0\n\\\\\ns\n& = \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }.\n\\end{aligned}\n\nPlugging it back\n\n\\begin{aligned}\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\\\\n& = \\frac{\n    \\log \\lvert \\mathcal{A} \\rvert\n}{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }\n} + \\frac{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R } R^{2}\n}{\n    2\n}\n\\\\\n& = R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert }\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ n },\n\\end{aligned}\n\nwhere the last equation is derived by dividing both sides by n.\n\n\n\n\n\n\n\nLemma 3 The Rademacher complexity of the hypothesis class \\mathcal{H} with the finite VC dimension d is upper-bounded\n\n\\mathrm{Rad}_{n} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\mathcal{H} has a finite VC dimension, its projection to any sample \\mathcal{S} with size n is finite. Since each \\mathcal{H} (\\mathcal{S}) can be seen as a set of vectors of length n, we can replace the set of vectors \\mathcal{A} in Lemma 2 with \\mathcal{H} (\\mathcal{S}).\nSince h^{2} (z_{i}) = 1, \\forall z_{i}, \\forall h \\in \\mathcal{H},\n\nR = \\sup_{h \\in \\mathcal{H}} \\sqrt{\\sum_{i = 1}^{n} h^{2} (z_{i})} = \\sqrt{n}.\n\nso we have\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H} (\\mathcal{S}))\n\\leq \\frac{ \\sqrt{n} \\sqrt{2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert} }{ n }\n= \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}.\n\nBy the definition of growth function and Sauer’s lemma \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (\\mathcal{S}) \\leq \\left( \\frac{ e }{ d } n \\right)^{d}, where d is the VC dimension of \\mathcal{H}, we can derive\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}\n\\leq \\sqrt{\\frac{ 2 \\log \\Pi_{\\mathcal{H}} (\\mathcal{n}) }{ n }}\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}."
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#definitions",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#definitions",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "Definition 1 A Rademacher variable has a discrete probability distribution where X has the equal probability of being +1 and -1.\n\nThe empirical Rademacher complexity measures the ability of the functions in a function class \\mathcal{F} to fit the random noise for a fixed sample \\mathcal{S}, which is described by the maximum correlation over all f \\in \\mathcal{F} between f (z_{i}) and \\sigma_{i}.\n\nDefinition 2 Given an i.i.d sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} from the distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the empirical Rademacher complexity of a class of binary function \\mathcal{F} is defined as\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}) = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right],\n\nwhich is a function of the random variable \\mathcal{S} and therefore is a random variable.\n\nTherefore, the Rademacher complexity of \\mathcal{F} measures the expected noise-fitting-ability of \\mathcal{F} over all data sets \\mathcal{S} \\in \\mathcal{Z}^{n} that could be drawn according to the distribution \\mathbb{P}_{\\mathcal{Z}^{n}}.\n\nDefinition 3 Then the Rademacher complexity is defined as the expectation of the empirical Rademacher complexity over all i.i.d samples of size n\n\n\\mathrm{Rad}_{n} (\\mathcal{F}) = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\\right] = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right]\n\\right].\n\n\nFor completeness, we include the definition of Rademacher average of a set of vectors.\n\nDefinition 4 Given n independent Rademacher random variables \\sigma = \\{ \\sigma_{1}, \\dots, \\sigma_{n} \\}, the Rademacher average of a set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} is\n\n\\mathrm{Rad}_{\\mathcal{A}} = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]."
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#rademacher-complexity-properties",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "The empirical Rademacher complexity and Rademacher complexity are non-negative.\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& \\stackrel{(1)}{\\geq} \\sup_{f \\in \\mathcal{F}} \\mathbb{E}_{\\sigma} \\left[\n     \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n\\right]\n\\\\\n& = \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\mathbb{E}_{\\sigma} [ \\sigma_{i} ] f (z_{i})\n\\\\\n& \\stackrel{(2)}{=} 0.\n\\end{aligned}\n\nExplanations in the derivations\n\nSince \\sup is a convex function, (1) follows because of the Jensen’s inequality.\n\nfollows because of the definition of Rademacher variable.\n\n\n\n\n\n\n\n\nGiven any function class \\mathcal{F} and constants a, b \\in \\mathbb{R}, denote the function class \\mathcal{G} = \\{ g (x) = a f (x) + b \\}.\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F})\n\n\n\\mathrm{Rad}_{n} (\\mathcal{G}) = \\lvert a \\rvert \\mathrm{Rad}_{n} (\\mathcal{F}).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy definition of \\mathcal{G} and the empirical Rademacher complexity,\n\n\\begin{aligned}\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{G})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (a f (z_{i}) + b)\n    \\right)\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n        + \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n+ \\mathbb{E}_{\\sigma} \\left[\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i})\n    \\right)\n\\right]\n\\\\\n& \\stackrel{(3)}{=} \\lvert a \\rvert \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left(\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right)\n\\right]\n\\\\\n& = \\lvert a \\rvert \\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\nSince the term \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} b does not depend on f, it can be pulled out of \\sup_{f \\in \\mathcal{F}}. Then (1) follows because of the linearity of expectation.\n\nfollows because of the linearity of expectation and \\mathbb{E}_{\\sigma} [\\sigma_{i}] = 0.\n\nWhen a &lt; 0, \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a f (z_{i}) = \\lvert a \\rvert \\sup_{f \\in \\mathcal{F}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}). Then (3) follows since \\sigma_{i} and -\\sigma_{i} have the same distribution."
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#symmetrization-lemma",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "Here we proved an important result with the Rademacher complexity using so called symmetrization technique, which involves creating a “ghost” sample as a hypothetical independent copy of the original sample.\n\nTheorem 1 For any class of measurable functions \\mathcal{F}, the expectation of the maximum error in estimating the mean of any function f \\in \\mathcal{F} is bounded by 2 times of the Rademacher complexity\n\n\\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n= \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right] \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n\nwhere E_{\\mathcal{S}} (f) = \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i}) is the estimated expectation of f on the sample \\mathcal{S}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy using the symmetrization technique, we introduce a ghost sample \\mathcal{S}' = \\{ z_{1}', \\dots, z_{n}' \\} that is also i.i.d drawn from \\mathbb{P}_{\\mathcal{Z}^{n}}, which means\n\n\\mathbb{E}_{\\mathcal{S}'} \\left[\n    E_{\\mathcal{S}'} (f)\n\\right] = \\mathbb{E}_{Z} [f (z_{i})].\n\nTherefore, we can get the following results\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f (z_{i})\n        - \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{z_{i}' \\in \\hat{\\mathcal{S}}} f (z_{i}')\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{=} \\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\mathbb{E}_{\\mathcal{S}'} \\left[\n            \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n        \\right]\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nuses the linearity of expectation and \\frac{ 1 }{ n } \\sum_{z_{i} \\in \\mathcal{S} f (z_{i})} is a constant.\n\n\nuses Jensen’s inequality since \\sup is a convex operator.\n\n\nSince f (z_{i}) - f (z_{i}') is invariant of sign change, we get\n\n\\begin{aligned}\n\\mathbb{E}_{\\mathcal{S}, \\mathcal{S}'} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n& = \\mathbb{E}_{\\mathcal{S}, \\mathcal{S}', \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} (f (z_{i}) - f (z_{i}'))\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\mathcal{S}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} f (z_{i})\n    \\right\\rvert\n\\right]\n+ \\mathbb{E}_{\\hat{\\mathcal{S}}, \\sigma} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        \\frac{ 1 }{ n } \\sum_{i = 1}^{n} - \\sigma_{i} f (z_{i}')\n    \\right\\rvert\n\\right]\n\\\\\n& \\stackrel{(2)}{=} 2 \\mathrm{Rad}_{n} (\\mathcal{F}).\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows because of the \\sup_{f \\in \\mathcal{F}} operator,\n\n\nfollows because \\sigma_{i} = - \\sigma_{i} by the definition of Rademacher variable.\n\n\nTherefore we have reached our conclusion\n\n\\mathbb{E}_{\\mathcal{S}} \\left[\n    \\sup_{f \\in \\mathcal{F}} \\left\\lvert\n        E_{\\mathcal{S}} (f) - \\mathbb{E}_{Z} [f (z_{i})]\n    \\right\\rvert\n\\right]\n\\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})."
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#rademacher-based-uniform-convergence",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "Theorem 2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, if the function class \\mathcal{F} only contains the functions f such that f (x) \\in [a, a + 1], then for every f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall f \\in \\mathcal{F}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{F}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven a function f \\in \\mathcal{F}, the difference between its true expectation and estimated expectation on a sample \\mathcal{S} = \\{ z_{1}, \\dots, z_{n} \\} is less than the maximum difference of the expectations among all functions in \\mathcal{F}, which is denoted by \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f) \\rvert  \\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert] = \\phi (\\mathcal{S}).\n\nFirst we will prove the following property so that we can use McDiarmid’s inequality on \\phi (\\mathcal{S})\n\n\\sup_{\\mathcal{S}, \\mathcal{S}'} \\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert \\leq \\frac{ 1 }{ n }\n\nwhere \\mathcal{S}' = \\{ z_{1}, \\dots, z_{j}', \\dots, z_{n} \\} has z_{j}' different from z_{j} in \\mathcal{S}.\nLet f^{*} \\in \\mathcal{F} be the function that maximizes \\phi (\\mathcal{S})\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n= \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S})\n\nand by definition\n\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert\n\\leq \\sup_{\\hat{f} \\in \\mathcal{F}} [\\lvert \\mathbb{E}_{\\mathcal{Z}} [\\hat{f} (z_{i})] - E_{\\mathcal{S}'} (\\hat{f}) \\rvert]\n= \\phi (\\mathcal{S}').\n\nTherefore,\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\lvert \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}} (f^{*}) \\rvert\n- \\lvert \\mathbb{E}_{\\mathcal{Z}} [f^{*} (z_{i})] - E_{\\mathcal{S}'} (f^{*}) \\rvert \\rvert\n\\\\\n& = \\lvert E_{\\mathcal{S}'} (f^{*}) - E_{\\mathcal{S}} (f^{*}) \\rvert\n\\\\\n& = \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert.\n\\end{aligned}\n\nSince \\mathcal{S} and \\mathcal{S}' only differ on two elements z_{j}, z_{j}', this becomes\n\n\\begin{aligned}\n\\lvert \\phi (\\mathcal{S}) - \\phi (\\mathcal{S}') \\rvert\n& \\leq \\left\\lvert\n    \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}) - \\frac{ 1 }{ n } \\sum_{i = 1}^{n} f^{*} (z_{i}')\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j})\n    \\right) - \\left(\n        \\sum_{i \\neq j} f^{*} (z_{i}) + f^{*} (z_{j}')\n    \\right)\n\\right\\rvert\n\\\\\n& = \\frac{ 1 }{ n } \\left\\lvert f^{*} (z_{j}) - f^{*} (z_{j}') \\right\\rvert\n\\\\\n& \\leq \\frac{ 1 }{ n }.\n\\end{aligned}\n\nThis results show that the function \\phi follows the bounded difference property, so we can apply the McDiarmid’s inequality on \\phi,\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} (\\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq t)\n& \\leq \\exp \\left[\n    \\frac{ - 2 t^{2} }{ \\sum_{i = 1}^{n} \\left(\n        \\frac{ 1 }{ n }\n    \\right)^{2} }\n\\right]\n\\\\\n& \\leq e^{- 2 m t^{2}}.\n\\end{aligned}\n\nBy setting \\delta = e^{-2 n t^{2}}, we can derive that t = \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}, so\n\n\\begin{aligned}\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\geq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\leq \\delta\n\\\\\n\\mathbb{P}_{\\mathcal{S}} \\left(\n    \\phi (\\mathcal{S}) - \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})] \\leq\n    \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\right)\n& \\geq 1 - \\delta.\n\\end{aligned}\n\nwhich means we have the following fact with the probability larger than 1 - \\delta,\n\n\\phi (\\mathcal{S}) \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}.\n\nBy plugging back the result to the equation that we want to prove, we have the final results\n\n\\begin{aligned}\n\\lvert \\mathbb{E}_{\\mathcal{Z}} [f (z_{i})] - E_{\\mathcal{S}} (f)] \\rvert\n& \\leq \\phi (\\mathcal{S})\n\\\\\n& \\leq \\mathbb{E}_{\\mathcal{S}} [\\phi (\\mathcal{S})]\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\\\\n& \\leq 2 \\mathrm{Rad}_{n} (\\mathcal{F})\n+ \\sqrt{\\frac{ \\log{\\frac{ 1 }{ \\delta }} }{ 2 n }}\n\\end{aligned}\n\nwith the probability larger than 1 - \\delta.\n\n\n\n\n\nGiven a hypothesis class \\mathcal{H}, a corresponding loss class with the 0-1 loss can be defined as\n\n\\mathcal{L} = \\{ l_{h} \\mid l_{h} (z) = L (h (\\mathbf{x}), y), h \\in \\mathcal{H}, z \\sim \\mathcal{Z} \\}\n\nand therefore the empirical risk and true risk can be defined as\n\nR_{\\mathcal{S}} (h) = E_{\\mathcal{S}} (l_{h}), R (h) = \\mathbb{E}_{\\mathcal{Z}} [l_{h} (z)].\n\nSince all the loss functions l_{h} \\in \\mathcal{L} have output range [0, 1], we can apply the uniform theorem above to the loss class \\mathcal{L} to derive the uniform convergence results for risks.\n\nCorollary 1 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}}, a hypothesis class \\mathcal{H}, and the corresponding 0-1 loss class \\mathcal{L}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{L}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}.\n\n\nBy using the following lemma, we can write the results in terms of the Rademacher complexity the hypothesis class \\mathcal{H} instead of the loss class \\mathcal{L}.\n\nLemma 1 Given a hypothesis class \\mathcal{H} and the corresponding loss class \\mathcal{L}, we have\n\n\\mathrm{Rad}_{n} (\\mathcal{H}) = 2 \\mathrm{Rad}_{n} (\\mathcal{L}).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the definition of Rademacher complexity and 0-1 loss, we have\n$$\n\\begin{aligned}\n\\mathrm{Rad}_{S} (\\mathcal{H})\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\mathbb{1} (h(x_{i}) \\neq y_{i})\n    \n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} \\left(\n        \\frac{ 1 }{ 2 } - y_{i} h (x_{i})\n    \\right)\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\left[\n        \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i}\n        + \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (-y_{i} h (x_{i}))\n    \\right]\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i}\n    + \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} (- y_{i} h (x_{i}))\n\\right]\n\\\\\n& = \\frac{ 1 }{ 2 } \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{h \\in \\mathcal{H}} \\frac{ 1 }{ m } \\sum_{i=1}^{m} \\sigma_{i} h (x_{i})\n\\right]\n& \\quad [\\mathbb{E}\\left[\\sum_{i=1}^{m}\\sigma_{i}\\right] = 0]\n\\\\\n& = \\frac{ 1 }{ 2 }\\text{Rad}_{S}(\\mathcal{H}).\n\\end{aligned}\n$$\n\n\n\n\nCorollary 2 Given a sample \\mathcal{S} that is drawn i.i.d from any distribution \\mathbb{P}_{\\mathcal{Z}^{n}} and a hypothesis class \\mathcal{H}, for every hypothesis h \\in \\mathcal{H}, the difference between its true risk and estimated risk is no greater than the error \\epsilon with probability at least 1 - \\delta\n\n\\mathbb{P} (\\lvert R_{\\mathcal{S}} (h) - R (h) \\rvert \\leq \\epsilon) \\geq 1 - \\delta,\n\\quad \\forall h \\in \\mathcal{H}\n\nwhere the error \\epsilon is\n\n\\epsilon = 2 \\mathrm{Rad}_{n} (\\mathcal{H}) + \\sqrt{\\frac{ \\log \\frac{ 1 }{ \\delta }}{ 2 n }}."
  },
  {
    "objectID": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "href": "Knowledge/Learning Theory/7_Rademacher_Complexity.html#bounding-rademacher-complexity",
    "title": "Rademacher Complexity",
    "section": "",
    "text": "The Rademacher complexity can be upper bounded for any function class with a finite VC dimension.\n\n\n\nLemma 2 (Massart’s lemma) Given any set of vectors \\mathcal{A} \\subseteq \\mathbb{R}^{n} the empirical Rademacher average is upper-bounded\n\n\\mathrm{Rad} (\\mathcal{A}) \\leq \\frac{ R \\sqrt{2 \\log \\lvert \\mathcal{A} \\rvert} }{ n }\n\nwhere R = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\lVert \\mathbf{a} \\rVert_{2}.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy Jensen’s inequality, the following quantity can be upper-bounded\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(1)}{\\leq} \\mathbb{E}_{\\sigma} \\left[\n    \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n        s \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\mathbb{E}_{\\sigma} \\left[\n    \\prod_{i = 1}^{n} \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\\\\n& \\stackrel{(2)}{=} \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n\\end{aligned}\n\nExplanations in the derivations\n\n\nfollows since \\exp is non-negative and therefore \\sup \\leq \\sum.\n\n\nfollows because of the independence between \\sigma_{i}.\n\n\nSince \\mathbb{E}_{\\sigma_{i} a_{i}} = 0, we can apply Hoeffding’s lemma with \\mu = 0\n\n\\begin{aligned}\n\\exp \\left[\n    s \\sigma_{i} a_{i}\n\\right]\n& \\leq \\exp \\left[\n    \\frac{ s^{2} (2 a_{i})^{2}}{ 8 }\n\\right]\n\\\\\n& = \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right],\n\\end{aligned}\n\nand therefore\n\n\\begin{aligned}\n\\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\mathbb{E}_{\\sigma} \\left[\n    \\exp \\left[\n        s \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\prod_{i = 1}^{n} \\exp \\left[\n    \\frac{ s^{2} a_{i}^{2} }{ 2 }\n\\right]\n\\\\\n& = \\sum_{\\mathbf{a} \\in \\mathcal{A}} \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right].\n\\end{aligned}\n\nwhere the last inequality follows because \\sum_{i = 1}^{n} f (a_{i}) \\leq n \\sup_{a_{i}} f (a_{i}), \\forall f.\nCombining all pieces together,\n\n\\begin{aligned}\n\\exp \\left[\n    s \\mathbb{E}_{\\sigma} \\left[\n        \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n    \\right]\n\\right]\n& \\leq \\lvert \\mathcal{A} \\rvert \\exp \\left[\n    \\frac{ s^{2} }{ 2 } \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}\n\\right]\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\end{aligned}\n\nwhere R^{2} = \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} a_{i}^{2}.\nSince \\log \\frac{ \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 } is a convex function, we can minimize it with respect to s\n\n\\begin{aligned}\n\\frac{ d }{ d s } \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n& = 0\n\\\\\n- \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s^{2} } + \\frac{ R^{2} }{ 2 }\n& = 0\n\\\\\ns\n& = \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }.\n\\end{aligned}\n\nPlugging it back\n\n\\begin{aligned}\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ \\log \\lvert \\mathcal{A} \\rvert }{ s } + \\frac{ s R^{2} }{ 2 }\n\\\\\n& = \\frac{\n    \\log \\lvert \\mathcal{A} \\rvert\n}{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R }\n} + \\frac{\n    \\frac{ \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ R } R^{2}\n}{\n    2\n}\n\\\\\n& = R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert }\n\\\\\n\\mathbb{E}_{\\sigma} \\left[\n    \\sup_{\\mathbf{a} \\in \\mathcal{A}} \\frac{ 1 }{ n } \\sum_{i = 1}^{n} \\sigma_{i} a_{i}\n\\right]\n& \\leq \\frac{ R \\sqrt{ 2 \\log \\lvert \\mathcal{A} \\rvert } }{ n },\n\\end{aligned}\n\nwhere the last equation is derived by dividing both sides by n.\n\n\n\n\n\n\n\nLemma 3 The Rademacher complexity of the hypothesis class \\mathcal{H} with the finite VC dimension d is upper-bounded\n\n\\mathrm{Rad}_{n} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\mathcal{H} has a finite VC dimension, its projection to any sample \\mathcal{S} with size n is finite. Since each \\mathcal{H} (\\mathcal{S}) can be seen as a set of vectors of length n, we can replace the set of vectors \\mathcal{A} in Lemma 2 with \\mathcal{H} (\\mathcal{S}).\nSince h^{2} (z_{i}) = 1, \\forall z_{i}, \\forall h \\in \\mathcal{H},\n\nR = \\sup_{h \\in \\mathcal{H}} \\sqrt{\\sum_{i = 1}^{n} h^{2} (z_{i})} = \\sqrt{n}.\n\nso we have\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H} (\\mathcal{S}))\n\\leq \\frac{ \\sqrt{n} \\sqrt{2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert} }{ n }\n= \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}.\n\nBy the definition of growth function and Sauer’s lemma \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert \\leq \\Pi_{\\mathcal{H}} (\\mathcal{S}) \\leq \\left( \\frac{ e }{ d } n \\right)^{d}, where d is the VC dimension of \\mathcal{H}, we can derive\n\n\\mathrm{Rad}_{\\mathcal{S}} (\\mathcal{H})\n\\leq \\sqrt{\\frac{ 2 \\log \\lvert \\mathcal{H} (\\mathcal{S}) \\rvert }{ n }}\n\\leq \\sqrt{\\frac{ 2 \\log \\Pi_{\\mathcal{H}} (\\mathcal{n}) }{ n }}\n\\leq \\sqrt{\\frac{ 2 d \\log \\frac{ e n }{ d } }{ n }}."
  },
  {
    "objectID": "Knowledge/Learning Theory/2_Bayesian_Classifier.html",
    "href": "Knowledge/Learning Theory/2_Bayesian_Classifier.html",
    "title": "Bayesian Classifier",
    "section": "",
    "text": "Bayes classifier is the particular hypothesis h^{*} (x) that minimizes the risk\n\nh^{*} = \\argmin_{h} R (h)\n\nand the risk that Bayes decision rule achieves is called Bayes Risk,\n\nR^{*} = R (h^{*})\n\nwhich is the minimum risk that any hypothesis can achieve if we know the true probabilities \\mathbb{P}_{X, Y}.\n\n\nSince the true risk R (h) does not depend on X and ?@lem-risk shows that R(h) = \\mathbb{E}_{X} \\left[ \\mathbb{E}_{Y \\mid X} \\left[ L (h (X), Y) \\right] \\right], the Bayes classifier can also be written as the hypothesis that minimizes the conditional expectation\n\nh^{*} = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right],\n\nwhich can be further simplied to maximum a-posteriori probability (MAP) rule if the loss function is 0-1 loss and there are m labels y \\in [1, m]\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of Bayes classifier,\n\n\\begin{aligned}\nh^{*}\n& = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right]\n\\\\\n& = \\argmin_{h} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) L (h, y)\n& [\\text{def of } \\mathbb{E}_{Y \\mid X}]\n\\\\\n& = \\argmin_{h} \\sum_{y = h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\times 0\n+\n\\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X}(y \\mid x) \\times 1\n& [\\text{def of 0-1 loss}]\n\\\\\n& = \\argmin_{h} \\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\argmin_{h} \\left[\n    1 - \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n\\right]\n& [\\sum_{x \\neq \\alpha} \\mathbb{P}_{X} (x) = 1 - \\mathbb{P}_{X} (\\alpha) ]\n\\\\\n& = \\argmax_{h} \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n& [\\argmin_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))].\n\\end{aligned}\n\nwhere the last line can be simplied to\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\nsince h (x) \\in [1, m].\n\n\n\nAccording to Bayes Theorem,\n\n\\begin{aligned}\n\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})}\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y],\n\\\\\n\\end{aligned}\n\nMAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior), which is more practical since the class conditional probability and class probability can be more easily obtained from the data than the posterior probability.\nUsing the log trick, the BDR for 0-1 loss is often calculated using:\n\n\\begin{aligned}\n\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)\n\\\\\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n\\\\\n\\end{aligned}\n\n\n\n\nSince there are only 2 labels in the binary classification problem, the MAP rule for binary classification is simplied to\n\n\\begin{aligned}\nh^{*} (x)\n& = \\argmax_{y \\in [0, 1]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\mathbb{1} \\left[\n    \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\mathbb{P}_{Y \\mid X} (0 \\mid x)\n\\right]\n\\\\\n& = \\begin{cases}\n    1, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\frac{ 1 }{ 2 }  \\\\\n    0, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &lt; \\frac{ 1 }{ 2 }.\n\\end{cases}\n\\end{aligned}\n\n\n\nThe conditional distribution \\mathbb{P}_{Y \\mid X} can be modeled with a Bernoulli distribution \\mathbb{P}_{Y \\mid X} (y \\mid x) = \\mathrm{Ber} (\\eta (x)), where \\eta (x) is the regression function\n\n\\eta (x) = \\mathbb{P}_{Y \\mid X} (1 \\mid x) = \\mathbb{E}_{Y \\mid X} (Y).\n\n\nLemma 1 For any hypothesis h, we can write its risk function with 0-1 loss for binary classification as\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the risk function\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\nSince the 0-1 loss for binary classification problem can be written as\n\nL (h (x), y) = \\mathbb{1} \\left[\n    h (x) \\neq y\n\\right] = y (1 - h (x)) + (1 - y) h (x)\n\nwe have\n\n\\begin{aligned}\nR (h)\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        y (1 - h (x)) + (1 - y) h (x)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} [y](1 - h (x))\n    +\n    \\mathbb{E}_{Y \\mid X} [1 - y] h (x)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\\end{aligned}\n\n\n\n\n\nTheorem 1 The risk of the Bayes classifier for binary classification with 0-1 loss is the expectation of the minimum of \\eta (X) and 1 - \\eta (X)\n\nR (h^{*}) = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\nand is less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right] \\leq \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 1 and replacing h with the Bayes classifier h^{*}, we have\n\n\\begin{aligned}\nR (h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h^{*} (X))\n    +\n    (1 - \\eta (X)) h^{*} (X)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) \\mathbb{1} \\left[\n        \\eta (X) &lt; \\frac{ 1 }{ 2 }\n    \\right]\n    +\n    (1 - \\eta (X)) \\mathbb{1} \\left[\n        \\eta (X) &gt; \\frac{ 1 }{ 2 }\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\\end{aligned}\n\nwhere the last inequality follows because\n\nR (h^{*}) = \\mathbb{E}_{X} [\\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &lt; \\frac{ 1 }{ 2 } \\\\\nR (h^{*}) = \\mathbb{E}_{X} [1 - \\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &gt; \\frac{ 1 }{ 2 }.\n\nSince \\min [\\eta (X), 1 - \\eta (X)] &lt; \\frac{ 1 }{ 2 }, its expectation is also less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]] &lt; \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\nFor any hypothesis h, we are interested in the difference between its risk R (h) and Bayes risk R (h^{*}), which is called excess risk of h\n\n\\mathcal{E} (h) = R (h) - R (h^{*}).\n\n\nTheorem 2 For any hypothesis h, the excess risk satisfies\n\n\\mathcal{E} (h) = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} \\left[\n        h (X) \\neq h^{*} (X)\n    \\right]\n\\right]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 1 for R (h) and R (h^{*}) and linearity of expectation, we have\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (h^{*} (X) - h (X))\n    +\n    (1 - \\eta (X)) (h (X) - h^{*} (X))\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) (h^{*} (X) - h (X))\n\\right].\n\\end{aligned}\n\nNote that\n\nh^{*} (X) - h (X) =  \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\nbecause it combines all 3 cases for the results of h^{*} (X) - h (X).\n\nIf h^{*} (X) = h (X),\n\nh^{*} (X) - h (X) = 0.\n\nSince \\eta (X) &gt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 1, if h^{*} (X) = 1, h (X) = 0,\n\nh^{*} (X) - h (X) = 1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\nSince \\eta (X) &lt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 0, if h^{*} (X) = 0, h (X) = 1,\n\nh^{*} (X) - h (X) = -1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\n\nTherefore,\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} [h^{*} \\neq h (X)]\n\\right].\n\\end{aligned}\n\nwhere the last equality holds since x \\times \\mathrm{sgn} [x] = \\lvert x \\rvert."
  },
  {
    "objectID": "Knowledge/Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "href": "Knowledge/Learning Theory/2_Bayesian_Classifier.html#map-rule",
    "title": "Bayesian Classifier",
    "section": "",
    "text": "Since the true risk R (h) does not depend on X and ?@lem-risk shows that R(h) = \\mathbb{E}_{X} \\left[ \\mathbb{E}_{Y \\mid X} \\left[ L (h (X), Y) \\right] \\right], the Bayes classifier can also be written as the hypothesis that minimizes the conditional expectation\n\nh^{*} = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right],\n\nwhich can be further simplied to maximum a-posteriori probability (MAP) rule if the loss function is 0-1 loss and there are m labels y \\in [1, m]\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of Bayes classifier,\n\n\\begin{aligned}\nh^{*}\n& = \\argmin_{h} \\mathbb{E}_{Y \\mid X} \\left[\n    L (h (X), Y)\n\\right]\n\\\\\n& = \\argmin_{h} \\sum_{y=1}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) L (h, y)\n& [\\text{def of } \\mathbb{E}_{Y \\mid X}]\n\\\\\n& = \\argmin_{h} \\sum_{y = h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\times 0\n+\n\\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X}(y \\mid x) \\times 1\n& [\\text{def of 0-1 loss}]\n\\\\\n& = \\argmin_{h} \\sum_{y \\neq h (x)}^{m} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\argmin_{h} \\left[\n    1 - \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n\\right]\n& [\\sum_{x \\neq \\alpha} \\mathbb{P}_{X} (x) = 1 - \\mathbb{P}_{X} (\\alpha) ]\n\\\\\n& = \\argmax_{h} \\mathbb{P}_{Y \\mid X} (h (x) \\mid x)\n& [\\argmin_{x} (1 - f(x)) = \\arg\\max_{x} (f(x))].\n\\end{aligned}\n\nwhere the last line can be simplied to\n\nh^{*} (x) = \\argmax_{y \\in [1, m]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\nsince h (x) \\in [1, m].\n\n\n\nAccording to Bayes Theorem,\n\n\\begin{aligned}\n\\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\frac{\\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)}{\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x})}\n\\\\\n& = \\arg\\max_{y} \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y) & [\\mathbb{P}_{\\mathbf{X}}(\\mathbf{x}) \\text{ doesn't depend on } y],\n\\\\\n\\end{aligned}\n\nMAP rule can thus be computed using the class conditional probability (likelihood) and the class probability (prior), which is more practical since the class conditional probability and class probability can be more easily obtained from the data than the posterior probability.\nUsing the log trick, the BDR for 0-1 loss is often calculated using:\n\n\\begin{aligned}\n\\arg\\max_{y} \\ln \\mathbb{P}_{Y \\mid \\mathbf{X}}(y \\mid \\mathbf{x})\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) \\mathbb{P}_{Y}(y)\n\\\\\n& = \\arg\\max_{y} \\ln \\mathbb{P}_{\\mathbf{X} \\mid Y}(\\mathbf{x} \\mid y) + \\ln \\mathbb{P}_{Y}(y).\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "href": "Knowledge/Learning Theory/2_Bayesian_Classifier.html#map-rule-for-binary-classification",
    "title": "Bayesian Classifier",
    "section": "",
    "text": "Since there are only 2 labels in the binary classification problem, the MAP rule for binary classification is simplied to\n\n\\begin{aligned}\nh^{*} (x)\n& = \\argmax_{y \\in [0, 1]} \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\mathbb{1} \\left[\n    \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\mathbb{P}_{Y \\mid X} (0 \\mid x)\n\\right]\n\\\\\n& = \\begin{cases}\n    1, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &gt; \\frac{ 1 }{ 2 }  \\\\\n    0, \\quad \\mathbb{P}_{Y \\mid X} (1 \\mid x) &lt; \\frac{ 1 }{ 2 }.\n\\end{cases}\n\\end{aligned}\n\n\n\nThe conditional distribution \\mathbb{P}_{Y \\mid X} can be modeled with a Bernoulli distribution \\mathbb{P}_{Y \\mid X} (y \\mid x) = \\mathrm{Ber} (\\eta (x)), where \\eta (x) is the regression function\n\n\\eta (x) = \\mathbb{P}_{Y \\mid X} (1 \\mid x) = \\mathbb{E}_{Y \\mid X} (Y).\n\n\nLemma 1 For any hypothesis h, we can write its risk function with 0-1 loss for binary classification as\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nAccording to the definition of the risk function\n\nR (h) = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right].\n\nSince the 0-1 loss for binary classification problem can be written as\n\nL (h (x), y) = \\mathbb{1} \\left[\n    h (x) \\neq y\n\\right] = y (1 - h (x)) + (1 - y) h (x)\n\nwe have\n\n\\begin{aligned}\nR (h)\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        L (h (X), Y)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} \\left[\n        y (1 - h (x)) + (1 - y) h (x)\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\mathbb{E}_{Y \\mid X} [y](1 - h (x))\n    +\n    \\mathbb{E}_{Y \\mid X} [1 - y] h (x)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h (X))\n    +\n    (1 - \\eta (X)) h (X)\n\\right].\n\\end{aligned}\n\n\n\n\n\nTheorem 1 The risk of the Bayes classifier for binary classification with 0-1 loss is the expectation of the minimum of \\eta (X) and 1 - \\eta (X)\n\nR (h^{*}) = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\nand is less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right] \\leq \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 1 and replacing h with the Bayes classifier h^{*}, we have\n\n\\begin{aligned}\nR (h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (1 - h^{*} (X))\n    +\n    (1 - \\eta (X)) h^{*} (X)\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) \\mathbb{1} \\left[\n        \\eta (X) &lt; \\frac{ 1 }{ 2 }\n    \\right]\n    +\n    (1 - \\eta (X)) \\mathbb{1} \\left[\n        \\eta (X) &gt; \\frac{ 1 }{ 2 }\n    \\right]\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\min \\left[\n        \\eta (X), 1 - \\eta (X)\n    \\right]\n\\right]\n\\end{aligned}\n\nwhere the last inequality follows because\n\nR (h^{*}) = \\mathbb{E}_{X} [\\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &lt; \\frac{ 1 }{ 2 } \\\\\nR (h^{*}) = \\mathbb{E}_{X} [1 - \\eta (X)] = \\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]], \\quad \\text{ if } \\eta (X) &gt; \\frac{ 1 }{ 2 }.\n\nSince \\min [\\eta (X), 1 - \\eta (X)] &lt; \\frac{ 1 }{ 2 }, its expectation is also less than \\frac{ 1 }{ 2 }\n\n\\mathbb{E}_{X} [\\min [\\eta (X), 1 - \\eta (X)]] &lt; \\frac{ 1 }{ 2 }.\n\n\n\n\n\n\n\nFor any hypothesis h, we are interested in the difference between its risk R (h) and Bayes risk R (h^{*}), which is called excess risk of h\n\n\\mathcal{E} (h) = R (h) - R (h^{*}).\n\n\nTheorem 2 For any hypothesis h, the excess risk satisfies\n\n\\mathcal{E} (h) = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} \\left[\n        h (X) \\neq h^{*} (X)\n    \\right]\n\\right]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy applying Lemma 1 for R (h) and R (h^{*}) and linearity of expectation, we have\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    \\eta (X) (h^{*} (X) - h (X))\n    +\n    (1 - \\eta (X)) (h (X) - h^{*} (X))\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) (h^{*} (X) - h (X))\n\\right].\n\\end{aligned}\n\nNote that\n\nh^{*} (X) - h (X) =  \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\nbecause it combines all 3 cases for the results of h^{*} (X) - h (X).\n\nIf h^{*} (X) = h (X),\n\nh^{*} (X) - h (X) = 0.\n\nSince \\eta (X) &gt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 1, if h^{*} (X) = 1, h (X) = 0,\n\nh^{*} (X) - h (X) = 1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\nSince \\eta (X) &lt; \\frac{ 1 }{ 2 } \\implies h^{*} (X) = 0, if h^{*} (X) = 0, h (X) = 1,\n\nh^{*} (X) - h (X) = -1 = \\mathrm{sgn} [2 \\eta (X) - 1].\n\n\nTherefore,\n\n\\begin{aligned}\nR (h) - R(h^{*})\n& = \\mathbb{E}_{X} \\left[\n    (2 \\eta (X) - 1) \\mathrm{sgn} [2 \\eta (X) - 1] \\times \\mathbb{1} [h^{*} \\neq h (X)],\n\\right]\n\\\\\n& = \\mathbb{E}_{X} \\left[\n    \\lvert 2 \\eta (X) - 1 \\rvert \\times \\mathbb{1} [h^{*} \\neq h (X)]\n\\right].\n\\end{aligned}\n\nwhere the last equality holds since x \\times \\mathrm{sgn} [x] = \\lvert x \\rvert."
  },
  {
    "objectID": "Knowledge/Supervised Learning/1_Perceptron.html",
    "href": "Knowledge/Supervised Learning/1_Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "Linear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/1_Perceptron.html#preliminary",
    "href": "Knowledge/Supervised Learning/1_Perceptron.html#preliminary",
    "title": "Perceptron",
    "section": "",
    "text": "Linear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/1_Perceptron.html#the-perceptron-algorithm",
    "href": "Knowledge/Supervised Learning/1_Perceptron.html#the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "The Perceptron algorithm",
    "text": "The Perceptron algorithm\nThe perceptron algorithm is the first machine learning algorithm that learns a linear discriminant f (\\mathbf{x}) from a training set using gradient descent, which does binary classification using the following decision rule\n\ng (\\mathbf{x}) = \\text{sign} (f (\\mathbf{x})) = \\begin{cases}\n1 & f (\\mathbf{x}) &gt; 0 \\\\\n0 & f (\\mathbf{x}) &lt; 0 \\\\\n\\end{cases}.\n\nThe loss function for the Perceptron algorithm is\n\nL(f (\\mathbf{x}), y) = \\max (0, - y f (\\mathbf{x})) =\n\\begin{cases}\nf (\\mathbf{x}) & \\text{sign} (f (\\mathbf{x})) \\neq y \\\\\n0 & \\text {sign} (f (\\mathbf{x})) = y, \\\\\n\\end{cases}\n\nwhich is called the Perceptron loss."
  },
  {
    "objectID": "Knowledge/Supervised Learning/1_Perceptron.html#covergence-analysis",
    "href": "Knowledge/Supervised Learning/1_Perceptron.html#covergence-analysis",
    "title": "Perceptron",
    "section": "Covergence analysis",
    "text": "Covergence analysis\nTODO: why (\\frac{2R}{\\gamma})^2 instead of (\\frac{R}{\\gamma})^2\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote03.html\nhttp://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/4_Boosting.html",
    "href": "Knowledge/Supervised Learning/4_Boosting.html",
    "title": "Boosting",
    "section": "",
    "text": "Linear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/4_Boosting.html#preliminary",
    "href": "Knowledge/Supervised Learning/4_Boosting.html#preliminary",
    "title": "Boosting",
    "section": "",
    "text": "Linear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/4_Boosting.html#boosting-for-learning-an-ensemble-learner",
    "href": "Knowledge/Supervised Learning/4_Boosting.html#boosting-for-learning-an-ensemble-learner",
    "title": "Boosting",
    "section": "Boosting for learning an ensemble learner",
    "text": "Boosting for learning an ensemble learner\n\nEnsemble classifier\nBoosting is a framework to learn a function that is a weighted sum of an ensemble of some base functions\n\nf (\\mathbf{x}) = \\sum_{i} w_{i} h_{i} (\\mathbf{x}),\n\nwhich is also named generalized additive models (GAMs) in statistics.\n\n\nGradient descent in functional space\nInstead of minimizing the empirical risk function over the space of possible parameters, boosting minimizes the risk over the space of a set of functions \\mathcal{U}\n\nR [f] = \\frac{1}{n} \\sum_{i=1}^{n} L[f].\n\nLet f_{t + 1} denote the classifier learned at the iteration t + 1 by minimizing a differentiable empirical risk function R [f] using gradient descent.\n\n\\begin{aligned}\nf_{t + 1}\n& = f_{t} - \\eta_{t} \\nabla R [f_{t}]\n\\\\\n& = \\left(\n    f_{t - 1} - \\eta_{t - 1} \\nabla R [f_{t - 1}]\n\\right) - \\eta_{t} \\nabla R [f_{t}]\n\\\\\n& = f_{t - 1} - \\left(\n    \\eta_{t - 1} \\nabla R [f_{t - 1}] + \\eta_{t} \\nabla R [f_{t}]\n\\right)\n\\\\\n& = ...\n\\\\\n& = f_{1} - \\sum_{i}^{t} \\eta_{t} \\nabla R [f_{i}].\n\\end{aligned}\n\nIf f_{1} is initialized to be 0 and the step size \\eta_{i} is different in each iteration, then f_{t + 1} can be interpreted as an ensemble of all gradients as the classifiers and the step sizes as weights\n\n\\begin{aligned}\nf_{t + 1}\n& = \\sum_{i = 1}^{t} \\eta_{i} \\left(\n    - \\nabla R [f_{i}]\n\\right)\n\\\\\n& = \\sum_{i = 1}^{t} w_{i} h_{i}.\n\\end{aligned}\n\nTherefore, the boosting learning algorithm can be characterized as performing the gradient descent in functional space.\n\n\nBoosting framework\n:::{prf:algorithm} Boosting\n\nInitialize f_{t} = 0\nWhile R [f_{t}] is decreasing\n\nCompute the negative function gradient - \\nabla R [f_{t}] as the function h_{t}, which is the steepest direction among the possible directions that the empirical risk function decreases the fastest.\n\nh_{t} = - \\nabla R [f_{t}]\n\nCompute the step size \\eta_{t} as the function weight w_{t}, which is how much step we should make along the fastest direction.\n\nw_{t} = \\eta_{t}\n\nUpdate the learned function\n\nf_{t + 1} = f_{t} + w_{t} h_{t} = f_{t} - \\eta_{t} \\nabla R [f_{t}]\n :::\n\n\nTo design a boosting algorithm, we need to specify the following.\n\nWhat is the Loss function with respect to a function L [f].\nHow base functions \\mathcal{U} are defined.\nHow to determine the learning rate \\eta_{t} in each iteration."
  },
  {
    "objectID": "Knowledge/Supervised Learning/4_Boosting.html#adaboost",
    "href": "Knowledge/Supervised Learning/4_Boosting.html#adaboost",
    "title": "Boosting",
    "section": "Adaboost",
    "text": "Adaboost\n\nBase function\nAdaboost requires that the type of the functions learned in each iteration is also a binary classifier\n\nh_{t} (\\mathbf{x}) \\in \\{-1, 1\\}, \\forall \\mathbf{x}, t,\n\nin which case the ensemble function f (\\mathbf{x}) is a true voting classifier.\n\nh_{t} (\\mathbf{x}) can vote for the positive and negative classes with the weight w_{t}.\nThe ensemble function makes the decisions based on the difference between the weighted strength of positive and negative votes\n\n  f (\\mathbf{x}) = \\sum_{t} w_{t} h_{t} (\\mathbf{x}) = \\sum_{t \\mid h_{t} (\\mathbf{x}) = 1} w_{t} - \\sum_{t \\mid h_{t} (\\mathbf{x}) = -1} w_{t}.\n  \n\n\n\nExponential loss\nAdaboost minimizes the exponential loss\n\nL (y, f (\\mathbf{x}) = \\phi (y f(\\mathbf{x})) = \\exp (-y f (\\mathbf{x}))\n\nwhich takes the exponential on the margins of the examples.\n\nThe exponential loss is an example of margin-enforcing loss, which encourages the classifier to have a large margin by penalizing both negative margins and small positive margins.\nThe exponential loss is an upper bound on the 0-1 loss.\n\nBy taking the functional gradients of empirical risk with the exponential loss with respect to the current function f_{t} at the iteration t, we can see how h_{t} is selected:\n\n\\begin{aligned}\n\\nabla R [f_{t}]\n& = \\arg\\max_{u} D_{u} R [f_{t}]\n\\\\\n& = \\arg\\max_{u} \\frac{d}{d \\epsilon} R [f_{t} + \\epsilon u] \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{d}{d \\epsilon} \\frac{1}{n} \\sum_{i}^{n} \\exp \\left(\n    - y_{i} \\left(\n        f_{t} (\\mathbf{x}_{i}) + \\epsilon u (\\mathbf{x}_{i})\n    \\right)\n\\right) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{1}{n} \\sum_{i}^{n} u (\\mathbf{x}_{i}) \\exp \\left(\n    - y_{i} \\left(\n        f_{t} (\\mathbf{x}_{i}) + \\epsilon u (\\mathbf{x}_{i})\n    \\right)\n\\right) \\Big|_{\\epsilon = 0}\n\\\\\n& = \\arg\\max_{u} \\frac{1}{n} \\sum_{i}^{n} - y_{i}  u (\\mathbf{x}_{i}) \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right).\n\\end{aligned}\n\nAfter simplifying the equation, the gradient function h_{t} (\\mathbf{x}) learned in the iteration t is\n\nh_{t} (\\mathbf{x}) = - \\nabla R [f_{t}]  = \\arg\\max_{u} \\sum_{i}^{n}  y_{i} u (\\mathbf{x}_{i}) \\exp (- y_{i} f_{t} (\\mathbf{x}_{i})).\n\nTherefore, the function learned in each iteration is the one that maximizes the sum of the weighted margins on the training examples.\n\ny_{i} u (\\mathbf{x}_{i}) is the margin of example \\mathbf{x}_{i} with respect to the function u.\n\\exp (- y_{i} f_{t} (\\mathbf{x}_{i})) is the weight of example \\mathbf{x}_{i} for learning h_{t}, which is large if \\mathbf{x}_{i} has large negative margin for the current function f_{t}, and close to 0 if \\mathbf{x} has positive margin. Therefore, the weights select the function u that focuses on the examples that are hard to classify correctly by the current function f_{t}.\n\n\n\nStep size\nThe optimal step size is calculated using line search algorithm in Adaboost\n\n\\begin{aligned}\nw_{t}\n& = \\arg\\min_{\\eta} R [f_{t} + \\eta h_{t}]\n\\\\\n& = \\arg\\min_{\\eta} \\frac{1}{n} \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n\\\\\n& = \\arg\\min_{\\eta} c (\\eta).\n\\end{aligned}\n\nSince the function c (\\eta) = \\exp(- a + b \\eta)) is a convex function with respect to the variable \\eta, its minimum can be obtained by setting its derivative to 0\n\n\\begin{aligned}\n\\frac{d c}{d \\eta} (\\eta)\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} f_{t + 1} (\\mathbf{x}_{i}; \\eta))\n& = 0\n& [f_{t + 1} = f_{t} + \\eta h_{t}]\n\\\\\n\\end{aligned}\n\nThe closed-form expression of the step-size can be derived since h_{t} (\\mathbf{x}_{i}) \\in \\{1, -1\\}\n$$\n\\begin{aligned}\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} (f_{t} (\\mathbf{x}_{i}) + \\eta h_{t} (\\mathbf{x}_{i})))\n& = 0\n\\\\\n\\sum_{i}^{n} - y_{i} h_{t} (\\mathbf{x}_{i}) \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- y_{i} \\eta h_{t} (\\mathbf{x}_{i}))\n& = 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} - \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- \\eta) + \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (\\eta)\n& = 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (- \\eta)\n& = \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) \\exp (\\eta)\n\\\\\n\\frac{\n    \\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}\n& = \\frac{\n    e^{\\eta}\n}{\n    e^{-\\eta}\n}\n\\\\\n\\frac{\n    \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})) -\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}\n& = e^{2 \\eta}\n\\end{aligned}\n$$\nDivide both numerator and denominator by \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i})),\n\n\\begin{aligned}\n\\eta = \\frac{1}{2} \\log \\frac{1 - \\epsilon}{\\epsilon},\n\\end{aligned}\n\nwhere\n\n\\epsilon = \\frac{\n    \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}{\n    \\sum_{i = 1}^{n} \\exp(- y_{i} f_{t} (\\mathbf{x}_{i}))\n}.\n\n\\epsilon is the weighted error of the week learner h_{t}, as it divides the sum of the weights for the incorrectly classified examples by the sum of the weights of all examples.\n\n\nWeak learner\nThe base function h_{t} in Adaboost is called the weak learner, because Adaboost can always converge even if h_{t} is not a good learner.\nThe empirical risk R can decrease if\n\n\\begin{aligned}\n\\lVert \\nabla R [f_{t}] \\rVert\n& &gt; 0\n\\\\\n\\sum_{i}^{n} - y_{i}  h_{t} (\\mathbf{x}_{i}) \\exp \\left(\n    y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\sum_{i \\mid y_{i} = h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right) - \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\sum_{i}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right) - 2 \\sum_{i \\mid y_{i} \\neq h_{t} (\\mathbf{x}_{i})}^{n} \\exp \\left(\n    - y_{i} f_{t} (\\mathbf{x}_{i})\n\\right)\n& &gt; 0\n\\\\\n\\epsilon\n& &lt; 0.5,\n\\end{aligned}\n\nwhich require that the weak learner in each iteration makes no more than half incorrect predictions on the training set.\nTherefore, Adaboost can also be seen as an algorithm that combines weak learners into a strong learner."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#preliminary",
    "href": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#preliminary",
    "title": "Naive Bayes Classifier (NBC)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nBayes’ theorem\nThe conditional possibility of event A given the event B is true P(A \\mid B) can be computed as:\n P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)} \nwhich in the Bayesian term is written as:\n \\mathrm{Posterior} = \\frac{\\mathrm{Likelihood} \\times \\mathrm{Prior}}{\\mathrm{Evidence}} \nUnderstand prior and posterior:\n\nPrior P(A) is the prior knowledge of the event A before knowing anything about event B.\nPosterior P(A \\mid B) is the updated knowledge of the event A after knowing something about event B.\n\nIf we think A as a label and B as a set of features:\n\nP(A \\mid B) is the posterior probability of a label given a set of features.\nP(B \\mid A) is the likelihood which is the probability of a set of features given a label.\nP(A) is the prior probability of a label.\nP(B) is the evidence probability of a set of features."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#statistical-estimation",
    "href": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#statistical-estimation",
    "title": "Naive Bayes Classifier (NBC)",
    "section": "## Statistical estimation",
    "text": "## Statistical estimation\nQuestion: assuming that we have a certain model (distribution) with unknown parameter \\theta and we have observations \\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n}) sampled from the model, how can we have a good estimate of the model parameter \\theta? Here we present two methods to answer the above question.\n\nMaximum Likelihood Estimation (MLE)\nLet \\mathbf{x} = (x_{1}, \\dots, x_{n}) be samples from a model (distribution) with a parameter (or a vector of parameters) \\theta. We define the likelihood of \\mathbf{x} given \\theta to be the “probability” of observing \\mathbf{x} if the true parameter is \\theta.\n L(\\mathbf{x} \\mid \\theta) \nThe best parameter \\theta is the one that simply maximizes the likelihood (log-likelihood), which is called maximum likelihood estimation (of the model parameter).\n \\theta_{MLE} = \\arg\\max_{\\theta} L(\\mathbf{x} \\mid \\theta) \n\n\nMaximum A Posteriori Estimation (MAP)\nBoth “a posteriori” and “a priori” are Latin phrases\n\n“a posteriori” (posterior): “relating to or derived by reasoning from observed facts” or “from the later”.\n“a priori” (prior): “relating to or derived by reasoning from self-evident propositions” or “from the earlier”.\n\nInstead of maximizing P(\\mathbf{x} \\mid \\theta) using MLE, we can also maximizing P(\\theta \\mid \\mathbf{x}), which is exactly the posterior in Bayes’ theorem. Hence the maximum A Posteriori estimation (maximizing posterior estimation).\n\n\\begin{align}\n\\theta_{MAP} & = \\arg\\max_{\\theta} P(\\theta \\mid \\mathbf{x}) \\\\\n& = \\arg\\max_{\\theta} \\frac{L(\\mathbf{x} \\mid \\theta) P(\\theta)}{P(\\mathbf{x})} & \\text{[Bayes' theorem]} \\\\\n& = \\arg\\max_{\\theta} L(\\mathbf{x} \\mid \\theta) P(\\theta) & \\text{[$P(\\mathbf{x}$) is a fixed value]} \\\\\n\\end{align}\n\nAs we can see, Bayes’ theorem turns MAP into the likelihood times the prior.\n\n\nMLE vs MAP\nBoth MLE and MAP are ways to estimate unknown parameters \\theta of a model based on the observed samples \\mathbf{x} from the model.\n\nMLE directly maximizes likelihood P(\\mathbf{x} \\mid \\theta), which is defined as the probability of the observation the samples \\mathbf{x} if the true parameter is \\theta.\nMAP incorporates the prior knowledge of the parameter P(\\theta) and maximizes the prior times the likelihood.\n\n\n\nIndependent variables and the log trick\nIf we further assume that random variables \\mathbf{x} = (x_{1}, \\dots, x_{n}) are independent and identically distributed (i.i.d), the likelihood can be further decomposed:\n L(\\mathbf{x} \\mid \\theta) = \\prod_{i=1}^{n} P(x_{i} \\mid \\theta) \nSince the probabilities are decimal values, the product of probabilities will often result in a very small values, which will cause problems in real computations.\nTo simplify the computation process, we usually use log-likelihood, which is just to take the natural logarithm of likelihood, since logarithm turns a product to a sum.\n\n\\begin{align}\n\\theta_{MLE}\n& = \\arg\\max_{\\theta} L(\\mathbf{x} \\mid \\theta) \\\\\n& = \\arg\\max_{\\theta} \\log \\prod_{i=1}^{n} P(x_{i} \\mid \\theta) \\\\\n& = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log P(x_{i} \\mid \\theta) \\\\\n\\end{align}\n\nSimilar to MLE, we also maximize \\log form of MAP to simplify the process,\n\n\\begin{align}\n\\theta_{MAP}\n& = \\arg\\max_{\\theta} L(\\mathbf{x} \\mid \\theta) P(\\theta) \\\\\n& = \\arg\\max_{\\theta} \\log L(\\mathbf{x} \\mid \\theta) P(\\theta) \\\\\n& = \\arg\\max_{\\theta} \\log L(\\mathbf{x} \\mid \\theta) + \\log P(\\theta) \\\\\n& = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log P(x_{i} \\mid \\theta) + \\log P(\\theta)  \\\\\n\\end{align}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#naive-bayes-as-a-classifier",
    "href": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#naive-bayes-as-a-classifier",
    "title": "Naive Bayes Classifier (NBC)",
    "section": "Naive Bayes as a classifier",
    "text": "Naive Bayes as a classifier\n\n\nNBC as MAP\nNaive Bayes is a classifier that selects the label \\hat{y} from all possible labels y \\in Y that has maximum conditional probability given the instance \\mathbf{x} \\in \\mathbb{R}^{d}\n \\hat{y} = \\arg\\max_{y \\in Y} P(y \\mid \\mathbf{x}) \nThe above conditional probability can be seen as a Maximum A Posteriori Estimation problem if:\n\nwe consider y as the unknown parameter of a model.\nwe assume that each feature is independent from each other (naive conditional independence assumption).\n\nFollowing the formulation of MAP,\n\n\\begin{align}\n\\hat{y} & = \\arg\\max_{y \\in Y} P(y \\mid \\mathbf{x}) \\\\\n& = \\arg\\max_{y \\in Y} \\sum_{j=1}^{d} \\log P(x_{j} \\mid y) + \\log P(y) \\\\\n\\end{align}\n\nwhere x_{j} is the value of jth feature in the instance \\mathbf{x}.\n\n\nLearn a NBC from the training set\nWe can learn both \\sum_{j=1}^{d} \\log P(x_{j} \\mid y) and \\log P(y) from the training set. Suppose we have a training set with instances \\mathbf{X} \\in \\mathbb{R}^{n \\times d} and labels \\mathbf{y} \\in \\mathbb{Z}^{n}:\nAs a classification problem, P(y) can be easily calculated by counting the number of instances with label y:\n P(y) = \\frac{\\text{# instances with label $y$}}{\\text{# all instances}} = \\frac{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y]}{n} \nHowever, P(x_{j} \\mid y) is more difficult to evaluate and depends on the type of the jth feature.\n\nIf the jth feature is a categorical feature, the likelihood P(x_{j} \\mid y) can also be easily calculated by counting the number of instances with label y and value x_{j} for the jth feature:\n P(x_{j} \\mid y) = \\frac{\\text{# instances with label $y$ and value $x_{j}$ for the $j$th feature}}{\\text{# instances with label $y$}} = \\frac{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y, \\mathbf{X}_{i, j}=x_{j}]}{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y]} \nHowever, if for some reason there is no instance in the training set that has value x_{j} for the jth feature, P(x_{j} \\mid y) will be 0 and \\log P(x_{j} \\mid y) = -\\inf, which is problem since then P(\\mathbf{x} \\mid y) = -\\inf and thus label y will never will selected. This problem can be solved by including Laplace smoothing with a smoothing parameter \\alpha:\n P(x_{j} \\mid y) = \\frac{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y, \\mathbf{X}_{i, j}=x_{j}] + \\alpha}{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y] + \\alpha \\lvert X_{*, j} \\rvert} \nwhere \\lvert X_{*, j} \\rvert is the number of unique values that the jth feature can take.\nIf the jth feature is a continuous feature, we may need to assume a the likelihood follows a specific distribution. If we select Gaussian distribution:\n P(x_{j} \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{j, y}^{2}}}\\exp({-\\frac{(x_{j} - \\mu_{j, y})^{2}}{2\\sigma_{j, y}^{2}}}) \nwhere \\mu_{j, y} is the mean of the values of the jth feature in the instances with label y\n \\mu_{j, y} = \\frac{\\sum_{i}^{n} \\mathbf{X}_{i, j} \\mathbb{1}[\\mathbf{y}_{i} = y]}{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y]} \nand \\sigma_{j, y} is the standard deviation of the values of the jth feature in the instances with label y\n \\sigma_{j, y} = \\sqrt{\\frac{\\sum_{i}^{n} (\\mathbf{X}_{i, j} - \\mu_{i, j})^{2} \\mathbb{1}[\\mathbf{y}_{i} = y]}{\\sum_{i}^{n} \\mathbb{1}[\\mathbf{y}_{i} = y]}}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#references",
    "href": "Knowledge/Supervised Learning/Naive Bayes Classifier (NBC).html#references",
    "title": "Naive Bayes Classifier (NBC)",
    "section": "## References",
    "text": "## References\n\nhttps://scikit-learn.org/stable/modules/naive_bayes.html\nCharter 7 in http://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf\nhttps://www.cs.cmu.edu/~epxing/Class/10701-10s/Lecture/lecture5.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/2_Logistic_Regression.html",
    "href": "Knowledge/Supervised Learning/2_Logistic_Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Bayesian Decision Theory (BDT)\nMaximum Likelihood Estimation (MLE)\n\n\n\n\n\nLinear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/2_Logistic_Regression.html#preliminary",
    "href": "Knowledge/Supervised Learning/2_Logistic_Regression.html#preliminary",
    "title": "Logistic Regression",
    "section": "",
    "text": "Bayesian Decision Theory (BDT)\nMaximum Likelihood Estimation (MLE)\n\n\n\n\n\nLinear Discriminant"
  },
  {
    "objectID": "Knowledge/Supervised Learning/2_Logistic_Regression.html#logistic-regression-as-a-gaussian-classifier",
    "href": "Knowledge/Supervised Learning/2_Logistic_Regression.html#logistic-regression-as-a-gaussian-classifier",
    "title": "Logistic Regression",
    "section": "Logistic regression as a Gaussian classifier",
    "text": "Logistic regression as a Gaussian classifier\nLogistic regression is a classification model that models the posterior probability of the positive class and assigns labels based on the MAP rule\n\ny = \\begin{cases}\n1 & \\sigma (f (\\mathbf{x})) \\geq 0.5 \\\\\n0 & \\sigma (f (\\mathbf{x}))\n&lt; 0.5 \\\\\n\\end{cases},\n\nwhere \\sigma is the sigmoid function and f (\\mathbf{x}) is a linear function on the instance \\mathbf{x}.\n\nMAP rule and posterior probability\nRecall that the BDR with 0-1 loss is the MAP rule\n\nf (\\mathbf{x}) = \\arg\\max_{y} \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x})\n\nwhere \\mathbb{P}_{Y \\mid \\mathbf{X}} (y \\mid \\mathbf{x}) is the posterior probability that the true class for instance \\mathbf{x} is y.\nFor a binary classification problem, the MAP rule can be simplified to select the class 1 for \\mathbf{x} if\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& \\geq \\mathbb{P}_{Y \\mid \\mathbf{X}} (0 \\mid \\mathbf{x})\n\\\\\n& \\geq 1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\\\\n& \\geq 0.5.\n\\end{aligned}\n\nUsing the Bayes theorem, the posterior probability of the positive class can be represented using the class conditional probabilities and class probabilities\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X}} (\\mathbf{x})\n}\n& [\\text{Bayes' theroem}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 0) + \\mathbb{P}_{\\mathbf{X}, Y} (\\mathbf{x}, 1)\n}\n& [\\text{Law of total probability}]\n\\\\\n& = \\frac{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}{\n    \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0) + \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n}\n& [\\text{Chain rule}]\n\\\\\n& = \\left(1 + \\frac{\n        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) \\mathbb{P}_{Y} (0)\n    }{\n        \\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) \\mathbb{P}_{Y} (1)\n    }\n\\right)^{-1}\n\\\\\n\\end{aligned}\n\n\n\nSigmoid function\nThe sigmoid function is a saturating function that maps the real number x into a number that ranges from 0 to 1\n\n\\sigma(x) = \\frac{\n    1\n}{\n    1 + e^{- x}\n}.\n\nThe posterior probability is the result of the sigmoid function if we assume the class conditional probabilities are Gaussian distributions.\nRecall that the multivariate Gaussian with the mean \\boldsymbol{\\mu} and covariance matrix \\boldsymbol{\\Sigma} is\n\n\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{\n    1\n}{\n    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n} \\exp \\left(\n    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}_{1}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n\\right),\n\nwhich can be compactly written as follows\n\n\\begin{aligned}\n\\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n& = \\frac{\n    1\n}{\n    \\sqrt{(2 \\pi)^{2} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert}\n} \\exp \\left(\n    -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_{1})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_{1}})\n\\right)\n\\\\\n& = \\exp \\left(\n    \\log \\left(\n        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n    \\right)^{-\\frac{1}{2}} - \\frac{1}{2} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)\n\\right)\n\\\\\n& = \\exp \\left(\n    -\\frac{1}{2} \\log \\left(\n        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n    \\right) - \\frac{1}{2} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(\n        \\mathbf{x} - \\boldsymbol{\\mu}\n    \\right)\n\\right)\n\\\\\n& = \\exp \\left(\n    -\\frac{1}{2} \\left(\n        \\log \\left(\n            (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma} \\rvert\n        \\right) + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu})\n    \\right)\n\\right),\n\\end{aligned}\n\nwhere d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\mathbf{y}) = \\frac{1}{2} \\left(  \\mathbf{x} - \\mathbf{y} \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\left(  \\mathbf{x} - \\mathbf{y} \\right) is the Mahalanobis distance between \\mathbf{x} and \\mathbf{y} with covariance matrix \\boldsymbol{\\Sigma}.\nIf we assume the class conditional probabilities for both classes are Gaussian distributions:\n\n\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 0) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Sigma}_{0})\n\\mathbb{P}_{\\mathbf{X} \\mid Y} (\\mathbf{x} \\mid 1) = \\mathcal{G} (\\mathbf{x}; \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{1}),\n\nthen the posterior possibility is\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n& = \\left(\n    1 + \\frac{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n            \\right) \\mathbb{P}_{Y} (0)\n        \\right)\n    }{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n            \\right) \\mathbb{P}_{Y} (1)\n        \\right)\n    }\n\\right)^{-1}\n\\\\\n& = \\left(\n    1 + \\frac{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{0} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{0}}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n            \\right) + \\log \\mathbb{P}_{Y} (0)\n        \\right)\n    }{\n        \\exp \\left(\n            -\\frac{1}{2} \\left(\n                \\log \\left(\n                        (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{1} \\rvert\n                \\right) + d_{\\boldsymbol{\\Sigma_{1}}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n            \\right) + \\log \\mathbb{P}_{Y} (1)\n        \\right)\n    }\n\\right)^{-1}\n\\\\\n& = \\left(\n    1 + \\exp \\left(\n        - f (\\mathbf{x})\n    \\right)\n\\right)^{-1}\n\\end{aligned}\n\nwhere $f () = ( {0} - {1} + d_{} (, ) - d_{} (, ) + 2 ) $ and \\alpha_{i} = \\log \\left(  (2 \\pi)^{d} \\lvert \\boldsymbol{\\Sigma}_{i} \\rvert \\right).\n\n\nLinear function\nIf we further assume that the Gaussian distributions for both classes have the same covariance matrix \\boldsymbol{\\Sigma}_{0} = \\boldsymbol{\\Sigma}_{1} = \\boldsymbol{\\Sigma}, then f (\\mathbf{x}) is a linear function\n\n\\begin{aligned}\nf (\\mathbf{x})\n& = \\frac{1}{2} \\left(\n    \\alpha - \\alpha\n    + d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{0}})\n    - d_{\\boldsymbol{\\Sigma}} (\\mathbf{x}, \\boldsymbol{\\mu_{1}})\n    + 2 \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\right)\n\\\\\n& = \\frac{1}{2} \\left(\n    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} +\n    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} +\n    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n    \\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} -\n    2\\mathbf{x}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1} -\n    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\\\\n& = \\left(\n    \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1}\n\\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x} + \\frac{1}{2} \\left(\n    \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -\n    \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}\n\\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}\n\\\\\n& = \\mathbf{w}^{T} \\mathbf{x} + b\n\\end{aligned}\n\nwhere\n\n\\mathbf{w}^{T} = \\left(  \\boldsymbol{\\mu}_{0} - \\boldsymbol{\\mu}_{1} \\right)^{T} \\boldsymbol{\\Sigma}^{-1} \\mathbf{x},\nb = \\frac{1}{2} \\left(  \\boldsymbol{\\mu}_{0}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{0} -  \\boldsymbol{\\mu}_{1}^{T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1} \\right) + \\log \\frac{\\mathbb{P}_{Y} (1)}{\\mathbb{P}_{Y} (0)}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/2_Logistic_Regression.html#learning-of-logistic-regression",
    "href": "Knowledge/Supervised Learning/2_Logistic_Regression.html#learning-of-logistic-regression",
    "title": "Logistic Regression",
    "section": "Learning of logistic regression",
    "text": "Learning of logistic regression\nWith the generative approach, parameters \\boldsymbol{\\mu}_{0}, \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}_{0}, and \\boldsymbol{\\Sigma}_{1} are learned from the training set using MLE. In particular, the parameters for the conditional probability of class j are learned by solving the following optimization problem\n\n\\arg\\max_{\\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}} \\prod_{y_{j} = j} \\mathbb{P}_{\\mathbf{X} \\mid Y} \\left(\n    \\mathbf{x}_{i} \\mid j\n\\right) = \\arg\\max_{\\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}} \\prod_{y_{j} = j} \\mathcal{G} \\left(\n    \\mathbf{x}_{i}; \\boldsymbol{\\mu}_{j}, \\boldsymbol{\\Sigma}_{j}\n\\right).\n\nHowever, logistic regression is usually learned using a discriminative approach, where the parameters \\mathbf{w}, b are directly learned from the data by minimizing binary cross-entropy loss.\n\nLearning as a MLE problem\nRecall that the learning of the linear regression can be formulated as an MLE problem\n\n\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y_{i} \\mid \\mathbf{x}_{i}\n\\right) = \\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathcal{G} \\left(\n    y_{i}; \\mathbf{w}^{T} \\mathbf{x}_{i} + b, \\sigma^{2}\n\\right),\n\nwhere the posterior probability of the label \\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(  y_{i} \\mid \\mathbf{x}_{i} \\right) follows a univariate Gaussian distribution with the mean \\mathbf{w}^{T} \\mathbf{x} + b and a known variance \\sigma^{2}.\nFor logistic regression, the posterior probability of the label should be a Bernoulli distribution\n\n\\begin{aligned}\n\\mathbb{P}_{Y \\mid \\mathbf{X}} \\left(\n    y \\mid \\mathbf{x}\n\\right)\n& = \\mathcal{B} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\right)\n\\\\\n& = \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})^{y} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x})\n\\right)^{(1 - y)}\n\\end{aligned}\n\nand therefore the MLE problem is defined as\n\n\\begin{aligned}\n\\arg\\max_{\\mathbf{w}, b} \\prod_{i} \\mathcal{B} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)\n& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathcal{B} \\left(\n    y; \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)\n\\\\\n& = \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)^{(1 - y_{i})}.\n\\end{aligned}\n\n\n\nBinary cross-entropy (BCE) loss\nThe binary cross-entropy loss is defined as\n\n\\text{BCE} (y, \\hat{y}) =  - y \\log \\hat{y} - (1 - y) \\log (1 - \\hat{y})\n\nwhere y \\in \\{0, 1\\} is the binary label and \\hat{y} \\in [0, 1] is the probability of the positive class.\nSolving the MLE of parameters of the logistic regression problem is the same as minimizing the BCE loss\n\n\\begin{aligned}\n& \\arg\\max_{\\mathbf{w}, b} \\sum_{i} \\log \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})^{y_{i}} \\left(\n    1 - \\mathbb{P}_{Y \\mid \\mathbf{X}} (1 \\mid \\mathbf{x}_{i})\n\\right)^{(1 - y_{i})}\n\\\\\n= & \\arg\\max_{\\mathbf{w}, b} \\sum_{i} y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) + (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\\\\\n= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\\\\\n= & \\arg\\min_{\\mathbf{w}, b} \\sum_{i} \\text{BCE} (y_{i}, \\sigma (f (\\mathbf{x}_{i})).\n\\end{aligned}\n\nTherefore, logistic regression can be learned by minimizing the BCE loss between the predicted labels and training labels.\n\n\nMinimizing loss with gradient descent\nUnlike linear regression, the optimization problem of logistic regression\n\n\\arg\\min_{\\mathbf{w}, b} \\sum_{i} - y_{i} \\log \\sigma (f (\\mathbf{x}_{i})) - (1 - y_{i}) \\log \\left(\n    1 - \\sigma (f (\\mathbf{x}_{i}))\n\\right)\n\ncan not be analytically solved to obtain a closed-form solution because of the non-linear sigmoid function.\nInstead, gradient descent is used to solve the optimization problem numerically.\nTODO: waiting for convex optimization"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#preliminary",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#preliminary",
    "title": "Support Vector Machine (SVM)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nConvex Optimization\n\nLagrangian\nGiven a (possibly non-convex) minimization primal problem:\n\n\\begin{aligned}\n\\min_{x} \\quad & f(x) \\\\\n\\text{s.t. } \\quad & g_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\quad & h_{i}(x) = 0, \\quad j = 1, \\dots, m \\\\\n\\end{aligned}\n\nwhere x here is used to represent all the input variables.\nThe Lagrangian of the primal problem above is defined as:\n \\mathcal{L}(x, \\lambda, \\nu) = f(x) + \\sum_{i=1}^{n} \\lambda_{i} g_{i}(x) + \\sum_{j=1}^{m} \\nu_{j} h_{i}(x) \nwhere \\{ \\lambda_{1}, \\dots, \\lambda_{n} \\} and \\{ \\nu_{1}, \\dots, \\nu_{m} \\} are two sets of new variables called Lagrangian multipliers.\nThe Lagrangian can be used to convert the primal problem with constraints to the following unconstrained problem\n\n\\begin{aligned}\n\\min_{x} \\mathcal{P}(x) = \\min_{x} \\quad \\max_{\\lambda, \\nu} \\quad & \\mathcal{L}(x, \\lambda, \\nu) \\\\\n\\text{s.t. } \\quad & \\lambda_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nSince the values of \\{ \\lambda_{1}, \\dots, \\lambda_{n} \\} and \\{ \\nu_{1}, \\dots, \\nu_{m} \\} can be freely chosen to maximize the Lagrangian, \\mathcal{P}(x) is the same as f(x) when x satisfies the constraints in the original primal problem. Otherwise, \\mathcal{P}(x) becomes infinity.\n\n\\mathcal{P}(x) =\n\\begin{cases}\n\\begin{aligned}\n& f(x) && \\text{if } g_{i}(x) \\leq 0, i = 1, \\dots, n \\text{ and } h_{i}(x) = 0, j = 1, \\dots, m \\\\\n& \\infty && \\text{otherwise} \\\\\n\\end{aligned}\n\\end{cases}\n\n\n\nDuality\nWe can create a new optimization problem by reverting the order of \\min and \\max in the Lagrangian unconstrained optimization problem.\n\n\\begin{aligned}\n\\max_{\\lambda, \\nu} \\mathcal{D}(\\lambda, \\nu) = \\max_{\\lambda, \\nu} \\quad & \\min_{x} \\quad \\mathcal{L}(x, \\lambda, \\nu) \\\\\n\\text{s.t. } \\quad & \\lambda_{i} \\geq 0, i = 1, \\dots, n \\\\\n\\end{aligned}\n\nwhere\n\n\\mathcal{D}(\\lambda, \\nu) = \\min_{x} \\mathcal{L}(x, \\lambda, \\nu) is the Lagrange dual function.\n\\max_{\\lambda, \\nu} \\mathcal{D}(\\lambda, \\nu) is the Lagrange dual problem.\n\nThe properties of the dual problem:\n\nThe dual problem is always convex even if the primal problem is not convex.\nFor any primal problem and its dual problem, the weak duality always holds.\n\n\n\nWeak duality\nWeak duality states that the optimal value of the primal problem is greater or equal to the optimal value of its dual problem.\nWeak duality holds for any primal problem and its dual problem, even if the primal problem is not convex.\n\n\nStrong duality\nStrong duality states that the optimal value of the primal problem is the same as the optimal value of its dual problem.\nFor convex primal problems, the strong duality holds if Slater’s conditions holds. Slater’s conditions test whether there exists an x that meet all the constraints of the primal problem.\n\n\nKarush-Kuhn-Tucker (KKT) conditions\nGiven the Lagrange dual problem stated above, the KKT conditions are:\n\nStationarity condition:\n \\frac{\\partial}{\\partial x}  \\mathcal{L}(x, \\lambda, \\nu) = 0 \nComplementary slackness condition:\n \\lambda_{i} g_{i}(x) = 0, \\quad i = 1, \\dots, n \nPrimal feasibility condition:\n g_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \n h_{i}(x) = 0, \\quad j = 1, \\dots, m \nDual feasibility condition:\n \\lambda_{i} \\geq 0, \\quad i = 1, \\dots, n \n\nIf a strong duality holds, the x^{*} and \\lambda^{*}, \\nu^{*} are primal and dual solutions if and only if x^{*} and \\lambda^{*}, \\nu^{*} satisfy the KKT conditions."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#hyperplane-and-margin",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#hyperplane-and-margin",
    "title": "Support Vector Machine (SVM)",
    "section": "## Hyperplane and margin",
    "text": "## Hyperplane and margin\n\nHyperplane\nIn the following context, a hyperplane in a d-dimensional space is represented by\n \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \nwhere \\mathbf{w} \\in \\mathbb{R}^{d} and b \\in \\mathbb{R} are constants defining the hyperplane.\n\nGiven a point \\mathbf{x}_{i} \\in \\mathbb{R}^{d},\n\n\\mathbf{x}_{i} is on the hyperplane if \\mathbf{w} \\cdot \\mathbf{x}_{i} + b = 0.\n\\mathbf{x}_{i} is above the hyperplane if \\mathbf{w} \\cdot \\mathbf{x}_{i} + b &gt; 0.\n\\mathbf{x}_{i} is below the hyperplane if \\mathbf{w} \\cdot \\mathbf{x}_{i} + b &lt; 0.\n\nThe hyperplane doesn’t change if both \\mathbf{w} and b are multiplied by the same scaling factor.\n \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\Leftrightarrow k \\mathbf{w} \\cdot \\mathbf{x} + k b = 0 \nwhere k is an arbitrary non-zero scaling factor.\nThe distance of a point \\mathbf{x}_{i} to the hyperplane defined by \\mathbf{w} and b is the perpendicular distance of the point to the hyperplane:\n d(\\mathbf{x}_{i}) = \\frac{\\lvert \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\rvert}{\\lVert \\mathbf{w} \\rVert} \nwhere \\lVert \\mathbf{w} \\rVert is the L_{2} norm of \\mathbf{w}.\n\n\n\nMargin\nWe define geometric margin of an instance \\mathbf{x}_{i} and label y_{i} with respect to a hyperplane defined by \\mathbf{w} and b to be\n \\gamma_{i} = \\frac{y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b )}{\\lVert \\mathbf{w} \\rVert} \n\nDue to the definition of label to be \\{ -1, 1 \\} instead of \\{ 0, 1 \\}, the sign of the geometric margin indicates whether the instance is classified correctly by the hyperplane. The geometric margin of \\mathbf{x}_{i} is positive only if the \\mathbf{x}_{i} is on the correct side of the hyperplane.\nNote the similarity between the definition of geometric margin and the definition of the distance of the point to the hyperplane. The magnitude of the geometric margin is the distance between the instance and the hyperplane."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#hard-margin-svm-svm-without-slacks",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#hard-margin-svm-svm-without-slacks",
    "title": "Support Vector Machine (SVM)",
    "section": "Hard margin SVM (SVM without slacks)",
    "text": "Hard margin SVM (SVM without slacks)\n\n\nFormulation\nGiven a dataset with n instances \\mathbf{x}_{i} \\in \\mathbb{R}^{d} and n labels y_{i} \\in \\{-1, 1\\}, we assume that there exists at least a hyperplane that can perfectly separates all training instances. That is,\n\nAll instances with label 1 are above the hyperplane.\nAll instances with label -1 are below the hyperplane.\n\nIn case there are more than one hyperplanes that can perfectly separates the training instances, a hard margin SVM model will choose the hyperplane that has the largest geometric margin to the training instances that are closest (minimum geometric margin) to the hyperplane.\n\n\\begin{aligned}\n\\max_{\\mathbf{w}, b} \\quad & \\hat{\\gamma} \\\\\n\\text{s.t. } \\quad & \\gamma_{i} \\geq \\hat{\\gamma}, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nwhere \\hat{\\gamma} is the minimum geometric margin that the hyperplane has with respect to all the training instances.\nWe can get the following convex optimization problem by simplifying the optimization problem above:\n\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^{2} \\\\\n\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\n:::{admonition} Proof: derivation of the SVM primal problem :class: dropdown\nExpand the definition of the geometric margin in the original optimization problem:\n\n\\begin{aligned}\n\\max_{\\mathbf{w}, b} \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert} \\\\\n\\text{s.t. } \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b )}{\\lVert \\mathbf{w} \\rVert} \\geq \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert}, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nwhere \\hat{\\mathbf{x}} represents the instance that achieves the minimum geometric margin to the hyperplane.\nSince \\lVert \\mathbf{w} \\rVert is non-negative, we can multiply \\lVert \\mathbf{w} \\rVert on both sides of the constraint to get\n\n\\begin{aligned}\n\\max_{\\mathbf{w}, b} \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert} \\\\\n\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b ), \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nSince the value of \\geq y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b ) can be arbitrarily scaled up or down by multiplying with a scaling factor, we can introduce a implicit constraint that y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b ) = 1\n\n\\begin{aligned}\n\\max_{\\mathbf{w}} \\quad & \\frac{1}{\\lVert \\mathbf{w} \\rVert} \\\\\n\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nTo help simplifying in the solving process, maximizing \\frac{1}{\\lVert \\mathbf{w} \\rVert} is the same as minimizing \\frac{\\lVert \\mathbf{w} \\rVert^{2}}{2}:\n\n\\begin{aligned}\n\\max_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert \\\\\n\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\n:::\nSolving the above optimization problem will give us two parallel hyperplanes (\\mathbf{w} x + b = 1 and \\mathbf{w} x + b = -1) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between.\n\nThe objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as\n \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert \\mathbf{w} \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert \\mathbf{w} \\rVert} = \\frac{2}{\\lVert \\mathbf{w} \\rVert} \nThe constraints specify that the instances must be on the correct side of the two hyperplanes:\n\ny_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) \\geq 1 \\Leftrightarrow\n\\begin{cases}\n\\begin{aligned}\n& \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\geq 1 && \\text{if } y_{i} = 1 \\\\\n& \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\leq -1 && \\text{if } y_{i} = -1 \\\\\n\\end{aligned}\n\\end{cases}\n\n\n\n\nSolving hard margin SVM\nRewrite the primal problem:\n\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} \\\\\n\\text{s.t. } \\quad & -(y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nDerive the Lagrangian from the primal problem:\n\n\\begin{aligned}\nL(\\mathbf{w}, b, \\alpha) & = f(\\mathbf{w}, b) + \\sum_{i=1}^{n} \\alpha_{i} g_{i}(\\mathbf{w}, b) \\\\\n& = \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n\\end{aligned}\n\nThen we can write the dual function:\n\n\\begin{aligned}\n\\mathcal{D}(\\alpha) & = \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b, \\alpha) \\\\\n& = \\min_{\\mathbf{w}, b} \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n\\end{aligned}\n\n\nSolving the dual function by taking the derivative of L(\\mathbf{w}, b, \\alpha) over \\mathbf{w} and b:\n\n\\begin{aligned}\n& \\frac{\\partial L}{\\partial \\mathbf{w}} = 0 \\Rightarrow \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i}y_{i} \\mathbf{x}_{i} = 0 \\Rightarrow \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i}\\mathbf{x}_{i} \\\\\n& \\frac{\\partial L}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i}y_{i} = 0\n\\end{aligned}\n\nPlug in \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i}\\mathbf{x}_{i} back and simplify \\mathcal{D}(\\alpha):\n\n\\begin{aligned}\n\\mathcal{D}(\\alpha)\n& = b \\sum_{i=1}^{n} \\alpha_{i} y_{i} + \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} (\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}) \\\\\n& = \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n\\quad & [\\text{since } \\sum_{i=1}^{n} \\alpha_{i}y_{i} = 0] \\\\\n\\end{aligned}\n\n\n:::{admonition} Proof: simplify \\mathcal{D}(\\alpha) for hard margin SVM :class: dropdown\n\n\\begin{aligned}\n\\mathcal{D}(\\alpha)\n& = \\min_{\\mathbf{w}, b}\n    \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w}\n    - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n& = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right)\n    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) - 1 \\right) \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right)\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i}\n    + b\\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    - \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right)\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = b \\sum_{i=1}^{n} \\alpha_{i} y_{i} + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\\n\\end{aligned}\n\n:::\nThe Lagrange dual problem is written as:\n\n\\begin{aligned}\n\\max_{\\alpha} \\quad & \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n\\end{aligned}\n\nSince the primal problem is a convex function and there definitely exists at least one solution to the primal problem, Slater’s condition proves that strong duality holds.\nSince solving the dual problem is the same as solving the primal problem, the benefits of solving the dual problem are:\n\nThe Lagrange dual problem only involves \\alpha_{i} and most of them are 0, but primal problem has \\mathbf{w} and b, which are much more parameters.\nThe Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn’t."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#soft-margin-svm-svm-with-slacks",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#soft-margin-svm-svm-with-slacks",
    "title": "Support Vector Machine (SVM)",
    "section": "## Soft margin SVM (SVM with slacks)",
    "text": "## Soft margin SVM (SVM with slacks)\n\nFormulation\nIn case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances.\n\n\\begin{aligned}\n\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^{2} + C \\sum_{i=1}^{n} \\xi_{i} \\\\\n\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots, n \\\\\n\\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n\\end{aligned}\n\nwhere \\xi_{i} is the slack variable for the instance \\mathbf{x}_{i} and C is a hyperparameter that penalizes the misclassification of \\mathbf{x}_{i}.\n\nIf \\xi_{i} is nonzero for \\mathbf{x}_{i}, it means that \\mathbf{x}_{i} is on the misclassified side of \\mathbf{w} \\cdot \\mathbf{x}_{i} + b = 1 (or \\mathbf{w} \\cdot \\mathbf{x}_{i} + b = -1) and the distance is \\xi_{i}.\nIf C = 0, \\xi_{i} can be arbitrary large for each \\mathbf{x}_{i}. If C \\to \\inf, it is the same as hard margin SVM because any misclassification can induce infinite loss.\n\n\n\nSolving soft margin SVM\nSimilar as hard margin SVM, we can write Lagrangian dual function as:\n\n\\begin{aligned}\n\\mathcal{D}(\\alpha, \\beta) & = \\min_{\\mathbf{w}, b} \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 + \\xi_{i} \\right) - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n\\end{aligned}\n\nAgain, we can solve Lagrangian dual function by taking the derivatives over the \\mathbf{w}, b, and \\xi_i:\n\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{w}} = 0 & \\Rightarrow \\mathbf{w} - \\sum_{i=1}{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} = 0 \\Rightarrow \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\\\\n\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n\\end{aligned}\n\nPlug the \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} and C = \\alpha_{i} + \\beta_{i} back and simplify \\mathcal{D}(\\alpha, \\beta).\n \\mathcal{D}(\\alpha, \\beta) = \\sum_{i=1}^{n} \\alpha_{i}  - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) \nwhich has exactly the same form as Lagrangian dual function of hard margin SVM.\n:::{admonition} Proof: simplify \\mathcal{D}(\\alpha) for soft margin SVM :class: dropdown\n\n\\begin{aligned}\n\\mathcal{D}(\\alpha, \\beta)\n& = \\min_{\\mathbf{w}, b}\n    \\frac{1}{2} \\mathbf{\\mathbf{w}} \\mathbf{\\mathbf{w}}\n    + C \\sum_{i=1}^{n} \\xi_{i} - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 + \\xi_{i} \\right)\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right)\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) - 1 + \\xi_{i} \\right)\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right)\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i}) \\xi_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i}\n    + b \\sum_{i=1}^{n} \\alpha_{i}y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n    - \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right)\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n    - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + b \\sum_{i=1}^{n} \\alpha_{i}y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\    \n& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i}\\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    + \\sum_{i=1}^{n} \\beta_{i} \\xi_{i}\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i}\n    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i}\n    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\  \n& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i}\n    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n& = \\sum_{i=1}^{n}\\alpha_{i}\n    - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\\n\\end{aligned}\n\n:::\nThe Lagrange dual problem is written as:\n\n\\begin{aligned}\n\\max_{\\alpha} \\quad & \\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) \\\\\n\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n& \\beta_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n\\end{aligned}\n\nSince we know C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}, the constraint \\beta_{i} \\geq 0 can be removed by merging into \\alpha_{i} \\geq 0:\n\n\\begin{aligned}\n\\max_{\\alpha} \\quad & \\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) \\\\\n\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n\\end{aligned}\n\nThe only difference with Lagrange dual problem of hard margin SVM is the addition of C \\geq \\alpha_{i}, \\quad i = 1, \\dots, n."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#inference-of-svm",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#inference-of-svm",
    "title": "Support Vector Machine (SVM)",
    "section": "Inference of SVM",
    "text": "Inference of SVM\n\nSolving the SVM dual problem will generate an optimal Lagrange multiplier \\alpha_{i}^{*} for each training instance \\mathbf{x}_{i}. All \\alpha_{i}^{*}’s, \\mathbf{x}_{i}’s and y_{i}’s can be used to calculate the optimal \\mathbf{w}^{*} and b^{*} that define the hyperplane as the classifier.\n\nSupport vectors\nSupport vectors are the training instances whose optimal Lagrange multipliers \\alpha_{i}^{*}’s are positive.\nFor hard margin SVM:\n\nAll support vectors are the training instances that are the closest to the decision hyperplane \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 0.\n\nAll positive support vectors are on the hyperplane \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 1.\nAll negative support vectors are on the hyperplane \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = -1.\n\nWhile there are some training instances on the hyperplanes \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1 have \\alpha_{i}^{*} = 0, the training instances that are NOT on the hyperplanes must have \\alpha_{i}^{*} = 0.\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &gt; 1 \\Rightarrow \\alpha_{i}^{*} = 0 \n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow \\alpha_{i}^{*} \\geq 0 \nNote that there won’t be any training instances that have y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &lt; 1 because of the constraints specified in the primal problem.\n\n:::{admonition} Proof: the values of \\alpha_{i}^{*}’s for the support vectors in the hard margin SVM :class: dropdown\nStrong duality implies that the optimal solutions \\alpha^{*}, \\mathbf{w}^{*} and b^{*} meet the KKT conditions. The complementary slackness condition states that\n \\alpha_{i} g_{i}(x) = 0 \\Rightarrow - \\alpha_{i} ( y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 ) = 0, \\quad i = 1, \\dots, n \n\n\\alpha_{i}^{*} must be 0 if y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &gt; 1\n\\alpha_{i}^{*} can be non-zero if y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1\n\n:::\nFor soft margin SVM,\n\nAll support vectors satisfy the constraint:\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 - \\xi_{i}, \\quad i \\in \\mathcal{S} \nwhere \\mathcal{S} = \\{ i \\mid \\alpha_{i}^{*} &gt; 0 \\}.\n\nAll positive support vectors are either on or below the hyperplane \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 1.\nAll negative support vectors are either on or above the hyperplane \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = -1.\n\nAgain the training instances on the hyperplanes \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1 can have 0 \\leq \\alpha_{i}^{*} \\leq C, but the training instances that are NOT on the hyperplanes must have \\alpha_{i}^{*} = 0 or \\alpha_{i}^{*} = C.\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow 0 \\leq \\alpha_{i}^{*} \\leq 0 \n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &gt; 1 \\Rightarrow \\alpha_{i}^{*} = 0 \n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &lt; 1 \\Rightarrow \\alpha_{i}^{*} = C ,\n\n:::{admonition} Proof: the values of \\alpha_{i}^{*}’s for the support vectors in the soft margin SVM :class: dropdown\nStrong duality implies that the optimal solutions \\alpha^{*}, \\mathbf{w}^{*} and b^{*} meet the KKT conditions. The complementary slackness condition states that\n - \\alpha_{i}^{*} ( y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 + \\xi_{i} ) = 0, \\quad i = 1, \\dots, n \n - \\beta_{i} \\xi_{i} = 0, \\quad i = 1, \\dots, n \n\nIf y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1, \\alpha_{i}^{*} is a non-negative number between 0 and C.\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow 0 \\leq \\alpha_{i}^{*} \\leq 0 \nIf y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &gt; 1, \\alpha_{i}^{*} must be 0,\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &gt; 1 \\Rightarrow \\alpha_{i}^{*} = 0 \nwhich follows by the equation 1 and the constraint \\xi_{i} \\geq 0 in the primal problem.\nIf y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &lt; 1, \\alpha_{i}^{*} must be C,\n y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) &lt; 1 \\Rightarrow \\alpha_{i}^{*} = C ,\nwhich is explained as follows:\n\nSince the constraint y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 + \\xi_{i} \\geq 0 in the primal problem, \\xi_{i} &gt; 0.\nSince the \\beta_{i} \\xi_{i} = 0 from the equation 2, \\beta_{i} = 0.\nSince we know that \\alpha_{i}^{*} + \\beta_{i} = C, \\alpha_{i}^{*} = C.\n\n\n:::\n\n\nCalculate \\mathbf{w}^{*} and b^{*} using \\alpha_{i}^{*}’s\nIn solving both hard and soft margin SVM, we have\n \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \nSince only \\alpha_{i}^{*}’s for the support vectors are non-zero, we can derive \\mathbf{w}^{*} by:\n \\mathbf{w}^{*} = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} \nwhere \\mathcal{S} = \\{ i \\mid \\alpha_{i}^{*} &gt; 0 \\}.\nTo calculate b^{*}, we can select any training instance \\hat{\\mathbf{x}} with label \\hat{y} that is on the hyperplanes \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1. Thus, we have\n\n\\begin{aligned}\n\\hat{y} ( \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} ) & = 1 \\\\\n\\hat{y}^{2} ( \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} ) & = \\hat{y} & [\\text{multiply both sides by } \\hat{y}] \\\\\n\\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} & = \\hat{y} & [\\hat{y}^{2} = 1] \\\\\nb^{*} & = \\hat{y} - \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}}  \\\\\n\\end{aligned}\n\nAssuming b^{*} has been calculated, we can express the inference of a SVM model as follows:\n\n\\begin{aligned}\n\\operatorname{svm}(\\mathbf{x}) & = \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} \\\\\n& = \\left( \\sum_{i \\in \\mathcal{S}} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\mathbf{x} + b^{*} \\\\\n& = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} \\cdot \\mathbf{x} + b^{*} \\\\\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#kernel-trick",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#kernel-trick",
    "title": "Support Vector Machine (SVM)",
    "section": "## Kernel trick",
    "text": "## Kernel trick\nKernel trick is a method that can efficiently make the learning algorithm (e.g. SVM, KNN) learn in a higher dimensional space without explicitly transforming the training instances to the higher dimensional space.\n\nWhy higher dimensions\nTake SVM for an example. Most of the time, the training set cannot be perfectly separated using a hyperplane, but can be perfectly separated if the feature space of the training set is mapped to a higher dimensional space.\nIn the following example, the training set \\mathbf{X} \\in \\mathbb{R} contains 3 training instances that are all scalars (each instance only has 1 feature).\n\n\n\n\nf1\ny\n\n\n\n\nx_{1}\n-2\n0\n\n\nx_{2}\n0\n1\n\n\nx_{3}\n2\n0\n\n\n\nClearly the positive instance x_{2} cannot be linearly separated from the negative instances x_{1} and x_{3}. However, we can use a feature mapping function \\Phi to map the training instances from scalars to vectors of length 2.\n\n\\Phi(x) =\n\\begin{bmatrix}\nx & x^{2}\n\\end{bmatrix}\n\nAfter applying the mapping function, we can get the transformed dataset in the 2-dimensional instead of the 1-dimensional space.\n\n\n\n\nf1\nf2\ny\n\n\n\n\n\\Phi(x_{1})\n-2\n4\n0\n\n\n\\Phi(x_{2})\n0\n0\n1\n\n\n\\Phi(x_{3})\n2\n4\n0\n\n\n\nWe can easily find a hyperplane (e.g. y = 2) to separate the positive instances from the negative instances in the transformed high-dimensional space.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(0)\nx0 = np.array([-2, 0, 2])\nx1 = np.zeros(x0.shape[0])\n\ny0_indices = np.array([1])\ny1_indices = np.delete(np.arange(x1.shape[0]), y0_indices)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[16, 4], sharey=True)\nax1.scatter(x0[y0_indices], x1[y0_indices], marker='o')\nax1.scatter(x0[y1_indices], x1[y1_indices], marker='x')\n\nx1 = x0 ** 2\n\nax2.scatter(x0[y0_indices], x1[y0_indices], marker='o')\nax2.scatter(x0[y1_indices], x1[y1_indices], marker='x')\n\n&lt;matplotlib.collections.PathCollection at 0x7f800b9cd9a0&gt;\n\n\n\n\n\n\n\nKernel function\nIf we have defined a feature mapping function \\Phi(\\mathbf{x}), we can apply it to SVM (and other machine learning models) by replacing every \\mathbf{x} by \\Phi(\\mathbf{x}) in all equations. Then we can find that in all equations of SVM (hard margin SVM, soft margin SVM and SVM inference), all transformed training instances \\Phi(\\mathbf{x}_{i}) are involved in the inner product (dot product) with another transformed \\Phi(\\mathbf{x}_{j})\nFurthermore, given a feature mapping function \\Phi, we define the corresponding Kernel function to be\n K(\\mathbf{x}_{1}, \\mathbf{x}_{2}) = \\Phi(\\mathbf{x}_{1}) \\cdot \\Phi(\\mathbf{x}_{2}) \nTaking the SVM inference equation for an example,\n\n\\begin{aligned}\n\\operatorname{svm}(\\mathbf{x}) & = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} \\cdot \\mathbf{x} + b^{*} \\\\\n& = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} \\Phi(\\mathbf{x}_{i}) \\cdot \\Phi(\\mathbf{x}) + b^{*} \\\\\n& = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} K(\\mathbf{x}_{i}, \\mathbf{x}_{j}) + b^{*} \\\\\n\\end{aligned}\n\nwe can see that we don’t necessarily need to compute \\Phi(\\mathbf{x}) if we have a way to compute K(\\mathbf{x}_{i}, \\mathbf{x}_{j}) directly. - The assumption here is that computing \\Phi(\\mathbf{x}) can be quite expensive if the output is an extremely high dimensional vector."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#reference",
    "href": "Knowledge/Supervised Learning/Support Vector Machine (SVM).html#reference",
    "title": "Support Vector Machine (SVM)",
    "section": "Reference",
    "text": "Reference\n\n\nhttps://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture7.pdf\nhttps://www.stat.cmu.edu/~ryantibs/convexopt-F16/scribes/kkt-scribed.pdf\nhttps://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/\nhttps://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf\nhttps://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf\nhttps://www-ai.cs.tu-dortmund.de/LEHRE/SEMINARE/SS09/AKTARBEITENDESDM/LITERATUR/PlattSMO.pdf"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#preliminary",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#preliminary",
    "title": "Decision Tree (DT)",
    "section": "Preliminary",
    "text": "Preliminary\n\n\nStatistics\n\nKullback-Leibler Divergence (KL Divergence)\nKL Divergence is a method to measure difference between two probability distributions over the same random variable X. Given a discrete random variable X and two probability distributions P(X) and Q(X), KL Divergence is defined as:\n\n\\begin{align}\nD_{KL}(P \\Vert Q) & = \\mathbb{E}_{X \\sim P}[\\log P(X) - \\log Q(X)] \\\\\n& = \\sum_{x \\in X} P(x) (\\log P(x) - \\log Q(x)) & [\\text{definition of expected value of } \\log P(x) - \\log Q(x)] \\\\\n& = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}  & [\\log a - \\log b = \\log \\frac{a}{b}] \\\\\n\\end{align}\n\n\nKL Divergence is not symmetric and thus cannot be used as an distance metric\n D_{KL}(P \\Vert Q) \\neq D_{KL}(Q \\Vert P)"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#tree-basics",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#tree-basics",
    "title": "Decision Tree (DT)",
    "section": "## Tree basics",
    "text": "## Tree basics\nDecision tree is composed of nodes and edges. - Each node corresponds to a subset of the original dataset. - The root node is the original training dataset provided to train the decision tree. - The path from the root node to a node specifies how the subset of the dataset is partitioned from the original training dataset.\nWe follow the notations listed below: - A node: t - A decision tree is a set of nodes: T - The original training set: \\mathbf{D} - The subset of the dataset that corresponds to node t: \\mathbf{D}_{t}"
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#impurity-function",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#impurity-function",
    "title": "Decision Tree (DT)",
    "section": "Impurity function",
    "text": "Impurity function\n\nThe impurity function F(\\cdot) measures the impureness of a set of labels. - F(\\cdot) achieves maximum only when the labels are in uniform distribution. - F(\\cdot) achieves minimum only when the labels provided are the same.\nWe use F(\\mathbf{D}) in the following context to compute the impureness of the labels in the dataset \\mathbf{D} using the impurity function F.\n\nClassification\nGiven a dataset \\mathbf{D} with C unique labels, P(c) is the probability of label c in the dataset, which is computed by dividing the number of instances with label c by the total number instances in \\mathbf{D}.\n\nWhen there is only one class in \\mathbf{D}, the dataset is pure and thus impurity functions should return 0.\nOn the contrary, if all possible labels are in \\mathbf{D} and also have the same number of instances, \\mathbf{D} achieves the maximum impureness.\n\n\nGini Impurity Index (Gini Impurity)\nGini impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n\n\\begin{align}\nF(\\mathbf{D}) & = \\sum_{c \\in C} P(c)(1 - P(c)) \\\\\n& = \\sum_{c \\in C} P(c) - P(c)^{2} \\\\\n& = 1 - \\sum_{c \\in C} P(c)^2 \\\\\n\\end{align}\n\n\n\nShannon Entropy (Entropy)\n F(\\mathbf{D}) = \\sum_{c \\in C} P(c) \\log P(c) \nEntropy can be thought as the difference measured by KL Divergence between the probability distribution of the unique labels represented in the current dataset \\mathbf{D} and the distribution of the most impure dataset. - The larger the entropy, the more far away from a uniform distribution is the distribution of the labels represented by \\mathbf{D}.\n\n\\begin{align}\nD_{KL}(P \\Vert Q) & = \\sum_{x \\in X} P(x)(\\log P(x) - \\log Q(x)) & [\\text{KL Divergence definition}] \\\\\n& = \\sum_{c \\in C} P(c)(\\log P(c) - \\log Q(c)) & [\\text{substitute } x \\text{ with label } c] \\\\\n& = \\sum_{c \\in C} P(c)(\\log P(c) - \\log \\frac{1}{C}) & [Q(c) = \\frac{1}{C} \\text{ is uniform distribution}] \\\\\n& = \\sum_{c \\in C} P(c)(\\log P(c) - (\\log 1 - \\log C)) \\\\\n& = \\sum_{c \\in C} P(c)(\\log P(c) + \\log C) \\\\\n& = \\sum_{c \\in C} P(c)\\log P(c) + \\log C \\sum_{c \\in C} P(c) \\\\\n& = \\sum_{c \\in C} P(c) \\log P(c) + \\log C & [\\sum_{c \\in C} P(c) = 1] \\\\\n& = \\sum_{c \\in C} P(c) \\log P(c) & [\\log C \\text{ is a constant and can be dropped}] \\\\\n\\end{align}\n\n\n\n\nRegression\nGiven a dataset \\mathbf{D} with continuous labels \\{y_{1}, y_{2}, \\dots, y_{n}\\}, impurity functions can be defined a similar way. - If the labels in \\mathbf{D} are very similar (low variance), the impurity functions should return a value closed to 0. - If the labels in \\mathbf{D} are very different from each other (high variance), impurity functions should return a very large value.\n\nMean squared error\nA dataset’s impurity can be simply measured by the mean squared error.\n F(\\mathbf{D}) = \\frac{1}{N} \\sum_{i}^{n} (y_{i} - \\bar{y})^{2} \nwhere \\bar{y} is the mean value of the labels in dataset \\mathbf{D}."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#splitting-criteria",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#splitting-criteria",
    "title": "Decision Tree (DT)",
    "section": "## Splitting criteria",
    "text": "## Splitting criteria\nA split is a way that divides a feature space into different groups and is used in the tree building process to split a node to children nodes. - Binary split (2-way split): split a feature space into 2 groups. A node will have 2 sub-nodes. - k-way split: split a feature space into k groups. A node will have k sub-nodes.\nThe most important question of building a decision tree is how to choose the best split to split a node (dataset) into different child nodes (sub-datasets). - A splitting criteria is a function that measures the impurity difference between the dataset before splitting and the datasets after splitting. - The best split s for the node t should be the one that has the maximum splitting criteria \\Delta F(\\mathbf{D}_{t}, s).\nGiven a set of datasets D = \\{ \\mathbf{D}_{1}, \\mathbf{D}_{2}, \\dots, \\mathbf{D}_{k} \\} created by applying split s to the dataset \\mathbf{D}_{t}, the splitting criteria is defined as:\n \\Delta F(\\mathbf{D}_{t}, s) = F(\\mathbf{D}_{t}) - \\sum_{\\mathbf{D}_{t} \\in D} \\frac{\\lvert \\mathbf{D} \\rvert}{\\lvert \\mathbf{D}_{t} \\rvert} F(\\mathbf{D}) \nIf F(\\cdot) is the entropy function, \\Delta F(\\mathbf{D}, s) is called Information Gain."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#stopping-condition",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#stopping-condition",
    "title": "Decision Tree (DT)",
    "section": "Stopping condition",
    "text": "Stopping condition\n\nEach split produces new nodes that recursively become the starting points for new splits. - A node stops splitting when certain stopping conditions are satisfied. and such nodes are leaf nodes. - The leaf node doesn’t have children but has a label according to the dataset it corresponds to.\nThe basic stopping condition is that the dataset that the leaf node corresponds to has impureness of 0 (single training instance or all training instance have the same label), in which case the splitting stops because there is no need to reduce the impureness.\nHowever, always splitting into pureness usually induces overfitting. Thus, there are other stopping conditions that can achieve early stopping to avoid overfitting. - Dataset size is below a threshold. - Splitting criteria improvement is below a threshold. - Tree depth is above a threshold. - Number of nodes is above a threshold."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#label-assignment",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#label-assignment",
    "title": "Decision Tree (DT)",
    "section": "## Label assignment",
    "text": "## Label assignment\nFor each node, we can assign a label to the node according to the labels of the dataset it corresponds to. - Classification: majority label. - Regression: mean label.\nEvery node can have a label assigned, but only leaf nodes use labels.\n\nMisclassification cost\nAssuming the assigned label of the node t is y_{t}, the misclassification cost of a node t is - Classification:\n$$ r(t) = 1 - P(y_{t}) $$\n\nRegression:\n r(t) = \\frac{1}{N(t)} \\sum_{y \\in t} (y - y_{t}) \nwhere y \\in t means all labels in the dataset that node t corresponds to.\n\nThen weighted misclassification cost of the node t is defined as the product of misclassification cost and the probability of picking an instance that is in the node t.\n R(t) = P(t) r(t) = \\frac{\\lvert \\mathbf{D}_{t} \\rvert}{\\lvert \\mathbf{D} \\rvert} r(t) \nThen the misclassification cost of the a tree T is\n R(T) = \\sum_{t \\in \\hat{T}} R(t) \nwhere \\hat{T} is the set of the leaf nodes in tree T.\nThe weighted misclassification cost of a node t is always higher than the sum of the weighted misclassification costs of the children nodes T_{c} = \\{t_{1}, t_{2}, \\dots, t_{k}\\} that t splits to.\n R(t) \\geq \\sum_{t_{i} \\in T_{c}} R(t_{i}) \nThus, if we split one of the leaf nodes of T to get a new and larger tree T', then\n R(T) \\geq R(T') \nThis shows that the misclassification cost of a tree will always decrease or stay the same if we continue to split its leaf nodes."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#pruning",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#pruning",
    "title": "Decision Tree (DT)",
    "section": "Pruning",
    "text": "Pruning\n\nAnother way to avoid overfitting is through pruning, which is the process to make some internal nodes leaf nodes and remove their children from a sufficiently large tree T_{max} that is rooted at t_{max}.\n\nMinimal cost-complexity pruning\nPreviously we show that R(T) is not a good measure of the performance of a tree because it always favors a larger tree. Thus we introduce another metric called cost-complexity that also considers the size of the tree. - If we consider each node has a complexity of \\alpha, the cost-complexity of a node t is\n$$ R_{\\alpha}(t) = R(t) + \\alpha  $$\n    \n\nThus, the cost-complexity of a tree T is\n R_{\\alpha}(T) = \\sum_{t \\in \\hat{T}} R_{\\alpha}(t) = \\sum_{t \\in \\hat{T}} (R(t) + \\alpha) = R(T) + \\alpha \\lvert \\hat{T} \\rvert \nwhere \\hat{T} is the set of leaf nodes of T.\n\nCost-complexity can be seen as adding a regularization term that penalize the complexity of the tree to the misclassification cost. - \\alpha is the regularization parameter that balances the training accuracy and tree complexity.\n\nGiven \\alpha, the goal of the pruning of T_{max} is to get a pruned tree \\hat{T}_{max} (a subtree of T_{max} that also rooted at t_{max}) that minimizes R_{\\alpha}(\\hat{T}_{max}).\n\n\n\nWeakest-link cutting\nWeakest-link cutting is an efficient way of doing the minimal cost-complexity pruning.\nIf the tree T is pruned by deleting subtree T_{t} rooted at the node t (replaced with t), the cost-complexity difference between pruned tree \\hat{T} and unpruned tree T is\n R_{\\alpha}(\\hat{T}) - R_{\\alpha}(T) = R_{\\alpha}(t) - R_{\\alpha}(T_{t}) \n\nIf \\alpha = 0, R_{\\alpha}(t) - R_{\\alpha}(T_{t}) = R(t) - R(T_{t}) \\geq 0.\nAs \\alpha becomes larger, R_{\\alpha}(t) - R_{\\alpha}(T_{t}) is getting smaller and will eventually becomes &lt; 0, since \\alpha is increasing slower than \\alpha \\lvert \\hat{T} \\rvert.\n\nGiven a sufficiently large \\alpha, R_{\\alpha}(t) - R_{\\alpha}(T_{t}) &lt; 0, which means that the cost-complexity of the node t is better than its subtree T_{t}, and thus T_{t} should be pruned.\n\nGiven a tree T, the weakest link \\bar{t} is the internal node in T that achieves R_{\\alpha}(\\bar{t}) - R_{\\alpha}(T_{\\bar{t}}) = 0 with the smallest \\alpha value.\n\nThe \\alpha value that achieves R_{\\alpha}(t) - R_{\\alpha}(T_{t}) = 0 can be directly calculated.\n\n  \\begin{align}\n  R_{\\alpha}(t) - R_{\\alpha}(T_{t}) & = 0 \\\\\n  R(t) + \\alpha - (R(T) + \\alpha \\lvert \\hat{T} \\rvert) & = 0 \\\\\n  R(t) - R(T) + \\alpha (1 + \\lvert \\hat{T} \\rvert) & = 0 \\\\\n  \\alpha & = \\frac{R(t) - R(T_{t})}{\\lvert T_{t} \\rvert - 1} \\\\\n  \\end{align}\n  \nThe weakest link is defined as\n \\bar{t} = \\arg \\min_{t \\in T \\setminus \\hat{T}} \\frac{R(t) - R(T_{t})}{\\lvert T_{t} \\rvert - 1} \nwhere T \\setminus \\hat{T} means the set of the internal nodes of T.\nIf there are more than 1 internal node that achieves R_{\\alpha}(\\bar{t}) - R_{\\alpha}(T_{\\bar{t}}) = 0 with same minimum \\alpha value, they are all called the weakest links.\n\nWeakest-link cutting finds the optimal subtree \\hat{T} of T_{max} that minimizes R_{\\alpha}(\\hat{T}) with a predefined threshold \\alpha_{max} in a iterative way.\n\nWe start the pruning process by first removing from T_{max} the subtrees T_{t} rooted at nodes t that have already achieved R_{\\alpha}(t) - R_{\\alpha}(T_{t}) = 0. We denote the resulting tree T_{0}.\nIn each iteration i, the weakest link(s) \\bar{t} of tree T_{i - 1} is identified by calculating\n \\bar{t} = \\arg \\min_{t \\in T_{i - 1} \\setminus \\hat{T}_{i - 1}} \\frac{R(t) - R(T_{t})}{\\lvert T_{t} \\rvert - 1} \nIn the meantime, we can also calculate the \\alpha_{i} that identifies the weakest link(s).\n \\alpha_{i} = \\min_{t \\in T_{i - 1} \\setminus \\hat{T}_{i - 1}} \\frac{R(t) - R(T_{t})}{\\lvert T_{t} \\rvert - 1} \nWe replace T_{\\bar{t}} (the subtree rooted at \\bar{t}) by \\bar{t} and denote the resulting tree T_{i}.\nContinue the iteration until the minimum \\alpha required to achieve R_{\\alpha}(\\bar{t}) - R_{\\alpha}(T_{\\bar{t}}) = 0 is above a predefined threshold \\alpha_{max}."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#cart-tree-building",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#cart-tree-building",
    "title": "Decision Tree (DT)",
    "section": "## CART Tree building",
    "text": "## CART Tree building\n\nIdentify all possible splits\nCART considers binary split of a single feature for each node (each node only splits a one feature and only has 2 children). - For a categorical feature that has k distinct values, CART considers all possible ways to split the k distinct values into 2 groups. - The maximum ways of splitting is 2^{k - 1} - 1. - e.g. If the categorical feature has 4 distinct values: \\{1, 2, 3, 4\\}, then all possible splits are\n    |index|1|2|3|4|5|6|7|\n    |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n    |left child|{1}|{2}|{3}|{4}|{1, 2}|{1, 3}|{1, 4}|\n    |right child|{2, 3, 4}|{1, 3, 4}|1, 2, 4}|{1, 2, 3}|{3, 4}|{2, 4}|{2, 3}|\n\nFor a numerical feature that has k distinct values appeared in the dataset, CART considers all the intervals between 2 consecutive values as the splits.\n\nThe maximum ways of splitting is k - 1.\ne.g. If the numerical feature has 6 distinct values: \\{-5.0, 1.0, 3.0, 5.0, 7.0, 11.0\\}, then all possible splits are\n\n\n\n\n\n\n\n\n\n\n\nindex\n1\n2\n3\n4\n5\n\n\n\n\nleft child\n \\leq -2.0 \n \\leq 2.0 \n \\leq 4.0 \n \\leq 6.0 \n \\leq 9.0 \n\n\nright child\n &gt; -2.0 \n &gt; 2.0 \n &gt; 4.0 \n &gt; 6.0 \n &gt; 9.0 \n\n\n\n\n\nAt a given node, CART considers all possible splits of all features and chooses the one that has the maximum splitting criteria.\n\n\nRecursive tree building\n\nIdentify all possible splittings among all features. For each categorical feature, each discrete value is a possible splitting. For each numerical feature, we can do either a) treat it as categorical feature by discretizing it or b) sort all training value of this numerical feature in ascending order and each interval between two consecutive number is a possible split.\nCalculate the uncertainty difference (Gini Gain or Information Gain) for all possible splitting and select the splitting with max uncertainty difference to split.\nOnce a node splits into two children, compute the data points that satisfy the two branches respectively. For each branch, return to procedure 1 with the new sub dataset.\nThe splitting stops when no further splitting can be made (the dataset contains only one class)."
  },
  {
    "objectID": "Knowledge/Supervised Learning/Decision Tree (DT).html#references",
    "href": "Knowledge/Supervised Learning/Decision Tree (DT).html#references",
    "title": "Decision Tree (DT)",
    "section": "References",
    "text": "References\n\n\nhttps://victorzhou.com/blog/intro-to-random-forests/\nhttps://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote17.html\nhttp://www.odbms.org/wp-content/uploads/2014/07/DecisionTrees.pdf\nhttps://scientistcafe.com/ids/splitting-criteria.html\nhttps://online.stat.psu.edu/stat508/book/export/html/647"
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html",
    "title": "Normal Matrices",
    "section": "",
    "text": "The matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is a normal matrix if and only if\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#properties-of-normal-matrices",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#properties-of-normal-matrices",
    "title": "Normal Matrices",
    "section": "Properties of normal matrices",
    "text": "Properties of normal matrices\n(normal-matrices-property-1)=\n\nUnitary matrices are normal.\n{prf:proof} unitary matrices are normal :class: dropdown\nAccording to the property of unitary matrices, a matrix \\mathbf{U} is unitary if and only if\n\n  \\mathbf{U}^{-1} \\mathbf{U} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}.\n  \nThus, unitary matrices are normal.\n::\n\n(normal-matrices-property-2)=\n\nDiagonal matrices are normal.\n{prf:proof} diagonal matrices are normal :class: dropdown\nConsider \\mathbf{D} \\in \\mathbb{C}^{n \\times n} as a diagonal matrix with d_{1}, \\dots, d_{n} in its diagonal.\n\n  \\mathbf{D}^{H} \\mathbf{D} = \\sum_{i=1}^{n} d_{i}^{2} = \\mathbf{D} \\mathbf{D}^{H}.\n   :::\n\n(normal-matrices-property-3)=\n\nUnitary similarity preserves normality. That is, if \\mathbf{A} is a normal matrix and is unitarily similar to \\mathbf{B}\n\n  \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{B}\n  \nor\n\n  \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U} = \\mathbf{B},\n  \nthen \\mathbf{B} is also a normal matrix.\n{prf:proof} unitary similarity preserves normality :class: dropdown\nThe goal is to prove\n\n  \\mathbf{B}^{H} \\mathbf{B} = \\mathbf{B} \\mathbf{B}^{H}.\n  \nFirst we expand \\mathbf{B}^{H} \\mathbf{B} to have\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  & [\\mathbf{U} \\mathbf{U}^{H} = \\mathbf{U} \\mathbf{U}^{-1} = \\mathbf{I}]\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}.\n  \\end{aligned}\n  \nSince \\mathbf{A} is a normal matrix,\n\n  \\mathbf{A} \\mathbf{A}^{H} = \\mathbf{A}^{H} \\mathbf{A}.\n  \nContinue from the derivation above,\n\n  \\begin{aligned}\n  \\mathbf{B}^{H} \\mathbf{B}\n  & = \\mathbf{U}^{H} \\mathbf{A} \\mathbf{A}^{H} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{I} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  \\\\\n  & = \\mathbf{U} \\mathbf{A}^{H} \\mathbf{U}^{H} \\mathbf{U}^{H} \\mathbf{A} \\mathbf{U}\n  & [TODO]\n  \\\\\n  & = (\\mathbf{U}^{H} \\mathbf{A}^{H} \\mathbf{U})^{H} (\\mathbf{U}^{H} \\mathbf{A} \\mathbf{U})\n  \\\\\n  & = \\mathbf{B} \\mathbf{B}^{H}.\n  \\end{aligned}\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#unitary-diagonalization",
    "title": "Normal Matrices",
    "section": "Unitary diagonalization",
    "text": "Unitary diagonalization\nA matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is normal if and only if \\mathbf{A} is unitarily similar to a diagonal matrix\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\iff \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nwhere \\mathbf{U} is a unitary matrix and \\mathbf{\\Lambda} is a diagonal matrix.\n{prf:proof} unitary diagonalization :class: dropdown\nWe first prove that\n\n\\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H} \\Rightarrow \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda}.\n\nTODO\nThen we prove that\n\n\\mathbf{U}^{-1} \\mathbf{A} \\mathbf{U} = \\mathbf{\\Lambda} \\Rightarrow \\mathbf{A}^{H} \\mathbf{A} = \\mathbf{A} \\mathbf{A}^{H}.\n\nAccording to the property of unitary matrices, unitary similarity preserves normality.\nAlso according to property of unitary matrices, all diagonal matrices are normal.\nThus, \\mathbf{A} is normal because \\mathbf{A} is unitarily similar to a diagonal matrix, which is always normal. :::\nSince the columns of \\mathbf{U} are eigenvectors of \\mathbf{A} and are orthonormal to each other the columns of \\mathbf{U} must be a complete orthonormal set of eigenvectors for \\mathbf{A}, and the diagonal entries of \\mathbf{\\Lambda} are the associated eigenvalues."
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#hermitian-symmetric-matrices",
    "title": "Normal Matrices",
    "section": "Hermitian (symmetric) matrices",
    "text": "Hermitian (symmetric) matrices\nA square complex (real) matrix \\mathbf{A} is hermitian (symmetric) if and only if\n\n\\mathbf{A}^{H} = \\mathbf{A},\n\nwhich implies a hermitian (symmetric) matrix is a normal matrix.\nAll eigenvalues of Hermitian matrices are real.\n{prf:proof} all eigenvalues of Hermitian matrices are real :class: dropdown\nSuppose (\\lambda, \\mathbf{v}) is a eigenpair for the Hermitian matrix \\mathbf{A}.\n\n\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}.\n\nMultiplying both sides by \\mathbf{v}^{H} on the left to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\lambda \\lVert \\mathbf{v} \\rVert^{2}.\n\\\\\n\\end{aligned}\n\nAlternatively we can take the transpose conjugate of both sides, and then multiply both sides by \\mathbf{v} on the right to get,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{v}\n& = \\lambda \\mathbf{v}\n\\\\\n(\\mathbf{A} \\mathbf{v})^{H}\n& = (\\lambda \\mathbf{v})^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H}\n& = \\lambda^{*} \\mathbf{v}^{H}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\mathbf{v}^{H} \\mathbf{v}\n\\\\\n\\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\end{aligned}\n\nSince \\mathbf{A} is a hermitian matrix\n\n\\begin{aligned}\n\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n& = \\mathbf{v}^{H} \\mathbf{A}^{H} \\mathbf{v}\n\\\\\n\\lambda \\lVert \\mathbf{v} \\rVert^{2}\n& = \\lambda^{*} \\lVert \\mathbf{v} \\rVert^{2}\n\\\\\n\\lambda\n& = \\lambda^{*},\n\\end{aligned}\n\nwhich means \\lambda is a real number. :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#rayleigh-quotient",
    "title": "Normal Matrices",
    "section": "Rayleigh quotient",
    "text": "Rayleigh quotient\nGiven a Hermitian matrix \\mathbf{M} \\in \\mathbb{C}^{n \\times n}, the Rayleigh quotient is a function R_{\\mathbf{M}} (\\mathbf{x}): \\mathbb{C}^{n} \\setminus \\{ 0 \\} \\rightarrow \\mathbb{R}\n\nR_{\\mathbf{M}} (\\mathbf{x}) = \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\nthat takes a nonzero vector \\mathbf{x} and returns a real number.\nSince the Hermitian matrix \\mathbf{M} has all real eigenvalues, they can be ordered. Suppose \\lambda_{1}, \\dots, \\lambda_{n} is the eigenvalues in descending orders.\nThen, given a Hermitian matrix, its Rayleigh quotient is upper bounded and lower bounded by maximum and minimum eigenvalues of \\mathbf{M} respectively,\n\n\\lambda_{1} \\geq R_{\\mathbf{M}} (\\mathbf{x}) \\geq \\lambda_{n}.\n\nThat is,\n\n\\lambda_{1} = \\max_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n},\n\n\n\\lambda_{n} = \\min_{\\mathbf{x} \\neq 0} \\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}.\n\n{prf:proof} Rayleigh quotient is bounded by eigenvalues :class: dropdown\nSince \\mathbf{M} is a Hermitian matrix, we can expand it using unitary diagonalization:\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{x}^{H} \\mathbf{M} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& = \\frac{\n    \\mathbf{x}^{H} \\mathbf{U}^{H} \\mathbf{\\Lambda} \\mathbf{U} \\mathbf{x}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n\\\\\n& = \\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n&\n[\\mathbf{y} = \\mathbf{U} \\mathbf{x}].\n\\end{aligned}\n\nSince \\mathbf{U} is a unitary matrix, according to the property of the unitary matrix,\n\n\\mathbf{y}^{H} \\mathbf{y} = \\mathbf{x}^{H} \\mathbf{x}.\n\nThus,\n\n\\begin{aligned}\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{x}^{H} \\mathbf{x}\n}\n& =\n\\frac{\n    \\mathbf{y}^{H} \\mathbf{\\Lambda} \\mathbf{y}\n}{\n    \\mathbf{y}^{H} \\mathbf{y}\n}\n\\\\\n& =\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}.\n\\end{aligned}\n\nSince \\lambda_{1} \\geq \\lambda_{i} \\geq \\lambda_{n}, \\forall i = 1, \\dots, n,\n\n\\lambda_{1}\n=\n\\lambda_{1}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{1} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\geq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n},\n\n\n\\lambda_{n}\n=\n\\lambda_{n}\n\\frac{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n=\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{n} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}\n\\leq\n\\frac{\n    \\sum_{i=1}^{n} \\lambda_{i} y_{i}^{2}\n}{\n    \\sum_{i=1}^{n} y_{i}^{2}\n}.\n :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#properties-of-definite-matrices",
    "href": "Knowledge/Linear Algebra/15_Normal_Matrices_and_Positive_Definite_Matrices.html#properties-of-definite-matrices",
    "title": "Normal Matrices",
    "section": "Properties of definite matrices",
    "text": "Properties of definite matrices\n(definite-matrices-property-1)=\n\nPositive definite matrix always has full rank.\n{prf:proof} positive definite matrix always has full rank :class: dropdown\nWe prove by contradiction.\nSuppose a positive definite matrix \\mathbf{A} does NOT have full rank, which means that there exists at least one non-zero vector \\mathbf{x} \\in N (\\mathbf{A}) such that\n\n  \\mathbf{A} \\mathbf{x} = 0.\n  \nThen, multiplying both sides by \\mathbf{x}^{H},\n\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x} = 0.\n  \nwhich contradicts to the fact that \\mathbf{A} is positive definite.\nThus, positive definite matrix always has full rank. :::\n\n(definite-matrices-property-2)=\n\nA matrix \\mathbf{A} is positive definite (semi-definite) if and only if its eigenvalues are positive (non-negative).\n{prf:proof} a matrix \\mathbf{A} is positive definite (semi-definite) if and only if its eigenvalues are positive (non-negative) :class: dropdown\nWe first prove that a matrix \\mathbf{A} is positive definite (semi-definite) if its all eigenvalues are positive (non-negative).\nSince \\mathbf{A} is a Hermitian matrix, it can be unitarily diagonalized:\n\n  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H},\n  \nwhere the columns of \\mathbf{U} contains the orthonormal eigenvectors of \\mathbf{A} and diagonal of \\mathbf{\\Lambda} has the corresponding real eigenvalues.\nSince we are told all eigenvalues are positive or non-negative(TODO),\n\n  \\mathbf{\\Lambda} = \\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{\\Lambda}^{\\frac{1}{2}}\n  = \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{A}\n  & = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}})^{H} \\mathbf{U}^{H}\n  \\\\\n  & = \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  \\end{aligned}\n  \nMultiplying \\mathbf{A} with \\mathbf{x} to get\n\n  \\begin{aligned}\n  \\mathbf{x}^{H} \\mathbf{A} \\mathbf{x}\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{\\Lambda}^{\\frac{1}{2}} \\mathbf{U})^{H} \\mathbf{x}\n  \\\\\n  & = \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} (\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}})^{H}\n  \\\\\n  & = \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2}\n  & [\\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\in \\mathbb{C}^{n}].\n  \\\\\n  \\end{aligned}\n  \nSince \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} is non-negative for any vector \\mathbf{x}, the matrix \\mathbf{A} is positive semi-definite.\nIf all eigenvalues are positive, there is no zero in \\mathbf{\\Lambda}. Since \\mathbf{\\Lambda}^{\\frac{1}{2}} is a diagonal matrix, the matrix \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has scaled columns of \\mathbf{U}. Since \\mathbf{U} is a unitary matrix, its columns are all linearly independent and thus \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} also have linearly independent columns. Thus, \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} has full rank and\n\n  N (\\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}}) = \\{ 0 \\}.\n  \nThus, \\lVert \\mathbf{x}^{H} \\mathbf{U} \\mathbf{\\Lambda}^{\\frac{1}{2}} \\rVert^{2} &gt; 0 for any non-zero vector \\mathbf{x}. Therefore, the matrix \\mathbf{A} is positive definite.\nThen we prove that all eigenvalues of positive definite (semi-definite) matrices are positive (non-negative).\nConsider any eigenpair (\\lambda, \\mathbf{v}) of \\mathbf{A}.\nSince \\mathbf{v} is non-zero by the definition of eigenvector, we have\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\mathbf{v}^{H} \\mathbf{v}\n  \\\\\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  & = \\lambda \\lVert \\mathbf{v} \\rVert^{2}\n  \\\\\n  \\lambda\n  & = \\frac{\n      \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v}\n  }{\n      \\lVert \\mathbf{v} \\rVert^{2}\n  }\n  \\end{aligned}\n  \nSince \\mathbf{A} is positive definite (semi-definite),\n\n  \\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} &gt; 0 \\quad (\\mathbf{v}^{H} \\mathbf{A} \\mathbf{v} \\geq 0),\n  \nwhich means\n\n  \\lambda &gt; 0 \\quad (\\lambda \\geq 0).\n  \n:::\nTODO \\mathbf{A} = \\mathbf{B}^{H} \\mathbf{B}"
  },
  {
    "objectID": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html",
    "href": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html",
    "title": "Determinants",
    "section": "",
    "text": "For an matrix \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, the determinant of A is defined to be the scalar\n\\text{det} (\\mathbf{A}) = \\lvert \\mathbf{A} \\rvert = \\sum_{p \\in \\mathcal{P}} \\sigma (p) \\prod_{i=1}^{n} a_{i, p_{i}}\nwhere\nNote that each term a1p1a2p2 · · · anpn in (6.1.1) contains exactly one entry from each row and each column of A."
  },
  {
    "objectID": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#eigenspace",
    "href": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#eigenspace",
    "title": "Determinants",
    "section": "Eigenspace",
    "text": "Eigenspace\nGiven a eigenvalue \\lambda,\n\n\\begin{aligned}\n\\mathbf{A} \\mathbf{x}\n& = \\lambda \\mathbf{x}\n\\\\\n(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x}\n& = 0\n\\\\\n\\mathbf{x}\n& \\in N (\\mathbf{A} - \\lambda \\mathbf{I}),\n\\\\\n\\end{aligned}\n\nwhich shows that\n\n\\{ \\mathbf{x} \\neq 0 \\mid \\mathbf{x} \\in N (\\mathbf{A} - \\lambda \\mathbf{I}) \\}\n\nis the set of all eigenvectors associated with the \\lambda, and N (\\mathbf{A} - \\lambda \\mathbf{I}) is an eigenspace for \\mathbf{A}.\nBecause of \\mathbf{x} \\neq \\mathbf{0} and rank-nullity theorem,\n\nN (\\mathbf{A} - \\lambda \\mathbf{I}) \\neq \\{ \\mathbf{0} \\}\n\\Rightarrow \\text{rank} (\\mathbf{A} - \\lambda \\mathbf{I}) &lt; n\n\nwhich, according to the property of rank, indicates that \\mathbf{A} - \\lambda \\mathbf{I} is a singular matrix, that is,\n\n\\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}) = 0."
  },
  {
    "objectID": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "href": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#characteristic-polynomial-and-equation",
    "title": "Determinants",
    "section": "Characteristic polynomial and equation",
    "text": "Characteristic polynomial and equation\nThe characteristic polynomial of \\mathbf{A} \\in \\mathbb{C}^{n \\times n} is a function\n\np (\\lambda) = \\text{det} (\\mathbf{A} - \\lambda \\mathbf{I}).\n\n\nThe degree of p (\\lambda) is n,\nthe leading term in p (\\lambda) is (-1)^{n} \\lambda^{n}.\n\nThe characteristic equation for \\mathbf{A} is\n\np (\\lambda) = 0.\n\n\nThe eigenvalues of \\mathbf{A} are the solutions of the characteristic equation.\nThus, \\mathbf{A} has n eigenvalues, but some may be complex numbers (even if the entries of A are real numbers), and some eigenvalues may be repeated."
  },
  {
    "objectID": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#multiplicities",
    "href": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#multiplicities",
    "title": "Determinants",
    "section": "Multiplicities",
    "text": "Multiplicities\nLet \\lambda_{1}, \\dots, \\lambda_{n} \\in \\sigma (\\mathbf{A}) be the unique eigenvalues for \\mathbf{A}. Then\n\nthe algebraic multiplicity of an eigenvalue \\lambda_{i} is the number of times it appears as the root of p (\\lambda),\n\n\\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = a_{i} \\iff \\dots + (\\lambda - \\lambda_{i})^{a_{i}} + \\dots = 0.\n\nThat is, the algebraic multiplicity of \\lambda_{i} is the number of times \\lambda_{i} has repeated in all eigenvalues.\nthe geometric multiplicity of an eigenvalue \\lambda_{i} is the number of the dimension of eigenspace associated with \\lambda_{i},\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{dim} N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}).\n  \nThat is, the geometric multiplicity of \\lambda_{i} is the number of linearly independent eigenvectors associated with \\lambda_{i}.\n\n\nSpecial multiplicities\n\nWhen \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = 1, \\lambda_{i} is called a simple eigenvalue, since there can only be one unique eigenvector associated with this eigenvalue.\nEigenvalues such that \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}) = \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) are called semi-simple eigenvalues of A, as all eigenvectors associated with the eigenvalues that have the value of \\lambda_{i} are linearly independent.\n\n\n\nProperties of multiplicities\n(multiplicities-property-1)=\n\nFor every \\mathbf{A} \\in \\mathbb{C}^{n \\times n}, and for each \\lambda_{i} \\in \\sigma(\\mathbf{A}),\n\n  \\text{geo mult}_{\\mathbf{A}} (\\lambda_{i}) \\leq \\text{alg mult}_{\\mathbf{A}} (\\lambda_{i}).\n  \n:::{prf:proof} $ {} ({i}) {} ({i})$. :class: dropdown\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "href": "Knowledge/Linear Algebra/13_Determinants_and_Eigensystems.html#independent-eigenvectors",
    "title": "Determinants",
    "section": "Independent eigenvectors",
    "text": "Independent eigenvectors\nLet \\{ \\lambda_{1}, \\dots, \\lambda_{k} \\} be a set of distinct eigenvalues for \\mathbf{A}.\n\nIf \\{ (\\lambda_{1}, \\mathbf{x}_{1}), \\dots, (\\lambda_{k}, \\mathbf{x}_{k}) \\} is a set of eigenpairs for \\mathbf{A}, then \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is a linearly independent set.\n:::{prf:proof} :class: dropdown\nWe prove by contradiction.\nSuppose \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly dependent, but has been reordered so that the first r eigenvectors \\{ \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k} \\} is linearly independent.\nThus, the r + 1th eigenvector is\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nMultiply the both sides by the \\mathbf{A} - \\lambda_{r + 1} \\mathbf{I} to get\n\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1} = (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}.\n  \nSince (\\lambda_{r + 1}, x_{r + 1}) is an eigenpair,\n\n  \\begin{aligned}\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{r + 1}\n  & = 0\n  \\\\\n  (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} - \\lambda_{r + 1} \\mathbf{I}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\mathbf{A} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i} \\mathbf{x}_{i} - \\lambda_{r + 1} \\mathbf{x}_{i})\n  & = 0\n  & [\\mathbf{A} \\mathbf{x}_{i} = \\lambda_{i} \\mathbf{x}_{i}]\n  \\\\\n  \\sum_{i=1}^{r} \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) \\mathbf{x}_{i}\n  & = 0\n  \\\\\n  \\end{aligned}\n  \nSince \\{ \\mathbf{1}, \\dots, \\mathbf{x}_{r} \\} are linearly independent,\n\n  \\alpha_{i} (\\lambda_{i}  - \\lambda_{r + 1}) = 0, \\forall i = 1, \\dots, r\n  \nbut since we assume \\lambda_{i}, \\dots, \\lambda_{r + 1}, \\dots, \\lambda_{n} are different,\n\n  \\lambda_{i} \\neq \\lambda_{r + 1} \\Rightarrow \\alpha_{i} = 0, \\forall i = 1, \\dots, r,\n  \nwhich means\n\n  \\mathbf{x}_{r + 1} = \\sum_{i=1}^{r} \\alpha_{i} \\mathbf{x}_{i} = 0.\n  \nHowever, eigenvectors are all non-zero vectors and thus an contradiction occurs. :::\nIf \\mathcal{B}_{i} is a basis for N (\\mathbf{A} - \\lambda_{i} \\mathbf{I}), then \\mathcal{B} = \\mathcal{B}_{1} \\cup \\dots, \\cup \\mathcal{B}_{k} is a linearly independent set.\n:::{prf:proof} :class: dropdown\nTODO :::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html",
    "title": "Fields",
    "section": "",
    "text": "A field is a set \\mathbb{F}, equipped with two operations addition + and multiplication \\cdot, obeying the rules (axioms) listed below."
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#axioms-of-fields",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#axioms-of-fields",
    "title": "Fields",
    "section": "Axioms of fields",
    "text": "Axioms of fields\nFor all x, y, and z in the field \\mathbb{F} (\\forall x, y, z \\in \\mathbb{F}), we have:\n\nClosure under addition and multiplication:\n\n  x + y \\in \\mathbb{F},\n  \n\n  x \\cdot y \\in \\mathbb{F}.\n  \nCommutativity of addition and multiplication:\n\n  x + y = y + x,\n  \n\n  x \\cdot y = y \\cdot x.\n  \nAssociativity of addition and multiplication:\n\n  (x + y) + z = x + (y + z),\n  \n\n  (x \\cdot y) \\cdot z = x \\cdot (y \\cdot z).\n  \nDistributive property of multiplication:\n\n  (x + y) \\cdot z = x \\cdot z + y \\cdot z.\n  \nThere is an element in \\mathbb{F} called “zero” 0 \\in \\mathbb{F} such that\n\n  x + 0 = x,\n  \nand there is another element in \\mathbb{F} called “one” 1 \\in \\mathbb{F}, 1 \\neq 0, such that\n\n  x \\cdot 1 = x.\n  \nFor each x \\in \\mathbb{F}, there is an element in \\mathbb{F} called addictive inverse x_{I} of x such that\n\n  x + x_{I} = 0,\n  \nand if x \\neq 0, there is an element in \\mathbb{F} called multiplicative inverse x^{-1} of x such that\n\n  x \\cdot x^{-1} = 1."
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#properties-of-fields",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#properties-of-fields",
    "title": "Fields",
    "section": "Properties of fields",
    "text": "Properties of fields\n\nZero and one are unique: there is only one “zero” and one “one” in any field \\mathbb{F}.\n:::{prf:proof} “zero” and “one” are unique :class: dropdown\nWe prove “zero” is unique by contradiction.\nSuppose there are two different “zero”s 0_{0} and 0_{1}.\nDue to the definition of “zero”,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{1}\n  & [0_{0} \\text{ is \"zero\"}]\n  \\\\\n  0_{0} + 0_{1}\n  & = 0_{0}\n  & [0_{1} \\text{ is \"zero\"}]\n  \\\\\n  \\end{aligned}\n  \nDue to the Commutativity axiom,\n\n  \\begin{aligned}\n  0_{1} + 0_{0}\n  & = 0_{0} + 0_{1}\n  \\\\\n  0_{1}\n  & = 0_{0}\n  \\\\\n  \\end{aligned}\n  \nwhich contradicts to the fact that 0_{0} and 0_{1} (1_{0} and 1_{1}) are different. Thus, there is only a unique “zero” in \\mathbb{F}.\nThe proof that the “one” in any \\mathbb{F} is unique is the same as above by replacing every addition + with multiplication \\cdot and 0_{0}, 0_{1} with 1_{0}, 1_{1}. :::\nAddictive and multiplicative inverse of every element are unique: there is only one addictive inverse and multiplicative inverse of every element (other than “zero” for multiplicative inverse) in any field \\mathbb{F}.\n:::{prf:proof} addictive and multiplicative inverse of every element are unique :class: dropdown\nWe prove the addictive inverse is unique by contradiction.\nSuppose there are two different addictive inverses of x: x_{1} and x_{2}.\nBy following the definition of addictive inverse,\n\n  (x + x_{1}) + x_{2} = 0 + x_{2} = x_{2}.\n  \nAlso,\n\n  \\begin{aligned}\n  (x + x_{1}) + x_{2}\n  & = x_{2}\n  \\\\\n  x + (x_{1} + x_{2})\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x + (x_{2} + x_{1})\n  & = x_{2}\n  & [\\text{commutativity}]\n  \\\\\n  (x + x_{2}) + x_{1}\n  & = x_{2}\n  & [\\text{associativity}]\n  \\\\\n  x_{1}\n  & = x_{2}\n  \\end{aligned}\n  \nwhich contradicts to the fact that x_{1} and x_{2} are different. Thus, the addictive inverse of every element in any field is unique.\nThe proof that the multiplicative inverse of every element in any field is unique is the same as above by replacing every addition + with multiplication \\cdot. :::\nFor every x in \\mathbb{F}, x \\cdot 0 = 0\n:::{prf:proof} x \\cdot 0 = 0 :class: dropdown\nConsider\n\n  \\begin{aligned}\n  x \\cdot 0 + x \\cdot 0\n  & = x \\cdot (0 + 0)\n  & [\\text{distributive property}]\n  \\\\\n  & = x \\cdot 0\n  & [\\text{definition of } 0]\n  \\end{aligned}\n  \nBy the definition of addictive inverse, there must exist an addictive inverse y of x \\cdot 0 in \\mathbb{F} such that\n\n  \\begin{aligned}\n  x \\cdot 0 + y\n  & = 0\n  \\\\\n  (x \\cdot 0 + x \\cdot 0) + y\n  & = 0\n  \\\\\n  x \\cdot 0 + (x \\cdot 0 + y)\n  & = 0\n  & [\\text{associativity}]\n  \\\\\n  x \\cdot 0 + 0\n  & = 0\n  & [\\text{addictive inverse}]\n  \\\\\n  x \\cdot 0\n  & = 0\n  & [\\text{definition of 0}]\n  \\end{aligned}\n   :::\nx_{I} = 1_{I} \\cdot x\n:::{prf:proof} x_{I} = 1_{I} \\cdot x :class: dropdown\n\n  \\begin{aligned}\n  (x + x_{I}) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  (x_{I} + x) + 1_{I} \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1 \\cdot x + 1_{I} \\cdot x)\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + (1  + 1_{I}) \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0 \\cdot x\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I} + 0\n  & = 1_{I} \\cdot x\n  \\\\\n  x_{I}\n  & = 1_{I} \\cdot x\n  \\end{aligned}"
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#axioms-of-vector-spaces",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#axioms-of-vector-spaces",
    "title": "Fields",
    "section": "Axioms of vector spaces",
    "text": "Axioms of vector spaces\nFor all u, v, and w in the vector space \\mathcal{V} (\\forall u, v, w \\in \\mathcal{V}) and for all \\alpha and \\beta in the field \\mathbb{F}, we have\n\nClosure under vector addition and scalar multiplication\n\n\nu + v \\in \\mathcal{V},\n\n\n\\alpha \\cdot v \\in \\mathcal{V}.\n\n\nCommutativity of vector addition:\n\n  u + v = v + u.\n  \nNote that there is no requirement of the commutativity of scalar multiplication in the definition of the vector space.\nAssociativity of vector addition and scalar multiplication:\n\n  (u + v) + \\mathbf{w} = u + (v + \\mathbf{w}),\n  \n\n  (\\alpha \\cdot \\beta) \\cdot v = \\alpha \\cdot (\\beta \\cdot v).\n  \nNote that in the left hand side of the second equation, the first dot is field multiplication while the second one is the scalar multiplication, but the in the right hand side both dots are scalar multiplication.\nDistributive property of scalar multiplication:\n\n  \\alpha \\cdot (u + v) = \\alpha \\cdot u + \\alpha \\cdot v,\n  \n\n  (\\alpha + \\beta) \\cdot v = \\alpha \\cdot v + \\beta \\cdot v.\n  \nThere is an element in \\mathcal{V} called “zero” vector 0 \\in \\mathcal{V} such that\n\n  v + 0 = v,\n  \nand the definition of “one” in the field 1 \\in \\mathbb{F} is applied in the vector space\n\n  1 \\cdot v = v.\n  \nNote that there is no requirement for the existence of “one” vector in the definition of the vector space.\nFor each v \\in \\mathcal{V}, there is an element in \\mathcal{V} called addictive inverse vector v_{I} of v such that\n\n  v + v_{I} = 0.\n  \nNote that there is no requirement for the scalar multiplicative inverse in the definition of the vector space."
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#linear-combination",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#linear-combination",
    "title": "Fields",
    "section": "Linear combination",
    "text": "Linear combination\nLet \\mathcal{V} be a vector space over the field \\mathbb{F}. Given a set of vectors v_{1}, \\dots, v_{n} \\in \\mathcal{V} and a set of field elements \\alpha_{1}, \\dots, \\alpha_{n} \\in \\mathbb{F}, the vector u is a linear combination of v_{1}, \\dots, v_{n} with \\alpha_{1}, \\dots, \\alpha_{n} as coefficients if\n\nu = \\sum_{i=1}^{n} \\alpha_{i} v_{i}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#properties-of-subspace",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#properties-of-subspace",
    "title": "Fields",
    "section": "Properties of subspace",
    "text": "Properties of subspace\nLet \\mathcal{U} and \\mathcal{V} be the subspaces of a vector space over \\mathbb{F}.\n\n0 \\in \\mathcal{W}.\n:::{prf:proof} “zero” vector must be in any subspace. :class: dropdown\nSince the subspace \\mathcal{W} cannot be empty, there is at least an element v \\in \\mathcal{W}.\nSince 1 \\in \\mathbb{F} and 1_{I} \\in \\mathbb{F},\n\n  1_{I} \\cdot v = v_{I} \\in \\mathcal{W},\n  \naccording to the axiom of the closure under scalar multiplication.\nAccording to the axiom of the closure under vector addition,\n\n  v + v_{I} = 0 \\in \\mathcal{W}.\n  \nThus, “zero” vector must be in \\mathcal{W}. :::\n\\mathcal{W} is also a vector space.\n:::{prf:proof} any subspace is a vector space. :class: dropdown\nBy virtue of the closure axioms, all axioms of the vector space are obeyed by \\mathcal{W}.\nSince all elements in \\mathcal{W} are also in the vector space \\mathcal{V}, the axioms of\n\nclosure\ncommutativity\nassociativity\ndistributive property\n1 \\cdot v = v\n\nin \\mathcal{W} follow directly from those in \\mathcal{V}\nThe existence of “zero” vector and addictive inverse are proved in the proof above. :::\nThe subspace\n\n  \\mathcal{W} + \\mathcal{U} = \\left\\{\n      w + u \\mid w \\in \\mathcal{W}, u \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n:::{prf:proof} the sum of two subspaces is a subspace. :class: dropdown\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} + \\mathcal{U}. Thus, \\mathcal{W} + \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} + \\mathcal{U}, then there exists elements a_{1} \\in \\mathcal{W}, a_{2} \\in \\mathcal{U} such that a = a_{1} + a_{2} and b_{1} \\in \\mathcal{W}, b_{2} \\in \\mathcal{U} such that b = b_{1} + b_{2}.\nBecause of closure under addition,\n\n  a_{1} + b_{1} \\in \\mathcal{W},\n  \n\n  a_{2} + b_{2} \\in \\mathcal{U}.\n  \nThus, according to the definition of the set addition\n\n  a + b = (a_{1} + b_{1}) + (a_{2} + b_{2}) \\in \\mathcal{W} + \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} + \\mathcal{U}, then there exists elements x_{1} \\in \\mathcal{W}, x_{2} \\in \\mathcal{U} such that x = x_{1} + x_{2}.\nBecause of closure under scalar multiplication,\n\n  \\alpha \\cdot x_{1} \\in \\mathcal{W} \\quad \\forall \\alpha \\in \\mathbb{F},\n  \n\n  \\alpha \\cdot x_{2} \\in \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, according to the definition of the set addition\n\n  \\alpha \\cdot x = \\alpha \\cdot x_{1} + \\alpha \\cdot x_{2} \\in \\mathcal{W} + \\mathcal{U} \\quad \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, \\mathcal{W} + \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n:::\nThe subspace\n\n  \\mathcal{W} \\cap \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W}, v \\in \\mathcal{U}\n  \\right\\}\n  \nis also a subspace.\n:::{prf:proof} The intersection of two subspaces is a subspace. :class: dropdown\nSince 0 \\in \\mathcal{W} and 0 \\in \\mathcal{U}, then 0 \\in \\mathcal{W} \\cap \\mathcal{U}. Thus, \\mathcal{W} \\cap \\mathcal{U} is non-empty.\nSuppose a, b \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces\n\n  a + b \\in \\mathcal{W},\n  \\\\\n  a + b \\in \\mathcal{U}.\n  \nThus, by definition,\n\n  a + b \\in \\mathcal{W} \\cap \\mathcal{U}.\n  \nAgain, suppose x \\in \\mathcal{W} \\cap \\mathcal{U}, then because of the closure axiom of the subspaces,\n\n  \\alpha \\cdot x \\in \\mathcal{W}, \\forall \\alpha \\in \\mathbb{F},\n  \\\\\n  \\alpha \\cdot x \\in \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\n  \nThus, by definition, \\alpha \\cdot x \\in \\mathcal{W} \\cap \\mathcal{U}, \\forall \\alpha \\in \\mathbb{F}.\nThus, \\mathcal{W} \\cap \\mathcal{U} is a non-empty set that is closed for both vector addition and scalar multiplication and thus is a subspace.\n:::\nThe subspace\n\n  \\mathcal{W} \\cup \\mathcal{U} = \\left\\{\n      v \\mid v \\in \\mathcal{W} \\mathop{\\text{or}} v \\in \\mathcal{U}\n  \\right\\}\n  \nis a subspace if and only if \\mathcal{W} \\subseteq \\mathcal{U} or \\mathcal{U} \\subseteq \\mathcal{W}\n:::{prf:proof} The union of two subspaces is a subspace iff one subspace is contained in the other. :class: dropdown\nFirst we prove if \\mathcal{U} \\subseteq \\mathcal{W}, then \\mathcal{U} \\cup \\mathcal{W} is a subspace.\nSince \\mathcal{U} \\subseteq \\mathcal{W},\n\n  \\mathcal{U} \\cup \\mathcal{W} = \\mathcal{W}\n  \nand thus is a subspace.\nThe same argument can be made for \\mathcal{W} \\subseteq \\mathcal{U}.\nThen we prove if \\mathcal{U} \\cup \\mathcal{W} is a subspace, then one subspace is contained in the other by contradiction.\nSuppose \\mathcal{U} \\cup \\mathcal{W} is a subspace, but \\mathcal{U} \\nsubseteq \\mathcal{W} and \\mathcal{W} \\nsubseteq \\mathcal{U}. This means there exists at least two vectors u and w such that\n\n  u \\in \\mathcal{U}, u \\notin \\mathcal{W}\n  \n\n  w \\in \\mathcal{W}, w \\notin \\mathcal{U}.\n  \nSince u, w \\in \\mathcal{U} \\cup \\mathcal{W} and closure under addition property, the vector\n\n  v = u + w\n  \nis also in \\mathcal{U} \\cup \\mathcal{W}, which means it must also be in \\mathcal{U} or \\mathcal{W}.\nIf v \\in \\mathcal{U}, because there must exist an addictive inverse of u \\in \\mathcal{U}, then\n\n  \\begin{aligned}\n  v\n  & = u + w\n  \\\\\n  v + u_{I}\n  & = u + u_{I} + w\n  \\\\\n  w\n  & = v + u_{I}\n  \\\\\n  \\end{aligned}\n  \nwhich shows that w \\in \\mathcal{U} and contradicts to our assumption.\nThe same contradiction can also be found when v \\in \\mathcal{W}.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "href": "Knowledge/Linear Algebra/01_Fields_and_Spaces.html#example-subspaces-of-2-dimensional-real-value-column-vectors",
    "title": "Fields",
    "section": "Example: subspaces of 2-dimensional real-value column vectors",
    "text": "Example: subspaces of 2-dimensional real-value column vectors\nThe 2-dimensional real-value column vector space \\mathbb{R}^{2} has 3 types of subspaces\n\nThe subspace of the zero vector only,\n\n\\mathcal{W} = \\left\\{ 0 \\right\\}.\n\nThe subspace of the vector space itself,\n\n\\mathcal{W} = \\mathbb{R}^{2}.\n\nAny “line” that goes through zero vector"
  },
  {
    "objectID": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html",
    "href": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html",
    "title": "Basis",
    "section": "",
    "text": "A set of vectors v_{1}, \\dots, v_{k} is a basis of a subspace \\mathcal{W} of a vector space \\mathcal{V} over a field \\mathbb{F}, if"
  },
  {
    "objectID": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#properties-of-basis",
    "href": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#properties-of-basis",
    "title": "Basis",
    "section": "Properties of basis",
    "text": "Properties of basis\n\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a minimal spanning set for \\mathcal{W}. That is, there is no vector in \\mathcal{B} that can be removed such that \\mathcal{B} is still a spanning set.\n:::{prf:proof} :class: dropdown\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a minimal spanning set, which means that the vectors in \\mathcal{B} are linear independent and \\mathcal{B} contains at least one vector b such that \\hat{\\mathcal{B}} = \\mathcal{B} \\setminus \\{ b \\} is still a spanning set.\nTherefore, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n - 1} such that b is a linear combination of the vectors in \\hat{\\mathcal{B}}\n\n  b = \\sum_{b_{i} \\in \\hat{\\mathcal{B}}} \\alpha_{i} b_{i}.\n  \nHowever, this means that the vectors in \\mathcal{B} are linear dependent, which violates our assumption.\n:::\nIf \\mathcal{B} = \\{ b_{1}, \\dots, b_{n} \\} is a basis of the subspace \\mathcal{W}, \\mathcal{B} is a maximal linearly independent subset of \\mathcal{W}. That is, there is no vector in \\mathcal{W} that can be added to \\mathcal{B} such that \\mathcal{B} is still a linearly independent set.\n:::{prf:proof} :class: dropdown\nWe will prove by contradiction. Assume \\mathcal{B} is a basis of the subspace \\mathcal{W} but is not a maximal linearly independent set, which means that \\mathcal{B} is a spanning set of \\mathcal{W} and there exists a vector b \\in \\mathcal{W} that can be added to \\mathcal{B} such that \\hat{\\mathcal{B}} = \\mathcal{B} \\cup \\{ b \\} is still a linearly independent set.\nHowever, since \\mathcal{B} is a spanning set of \\mathcal{W}, there exists a set of coefficients \\alpha_{1}, \\dots, \\alpha_{n} such that b is a linear combination of the vectors in \\mathcal{B}\n\n  b = \\sum_{b_{i} \\in \\mathcal{B}} \\alpha_{i} b_{i}.\n  \nwhich means that the vectors in \\hat{\\mathcal{B}} are linear dependent, which violates our assumption.\n:::\n\n(existence-of-basis)=\n\nExistence of basis\nA subspace may NOT have a basis e.g. \\mathcal{W} = \\{ 0 \\} has no linearly independent vector, but a subspace must have a basis if it has a finite spanning set.\nThat is, every finite spanning set of a subspace contains a basis.\n:::{prf:proof} existence of basis :class: dropdown\nSuppose the subspace \\mathcal{W} has a finite spanning set of\n\n  \\mathcal{A}_{0} = \\{ v_{i}, \\dots, v_{n} \\}.\n  \nIf the spanning set is linearly independent, the set \\{ v_{i}, \\dots, v_{n} \\} is a basis of \\mathcal{W}.\nIf the spanning set is not linearly independent, then there exists j (1 \\leq j \\leq n) such that\n\n  v_{j} = \\sum_{i=1, i \\neq j}^{n} \\alpha_{i} v_{i},\n  \nand the set\n\n  \\mathcal{A}_{1} = \\{ v_{1}, \\dots, v_{n} \\} \\setminus \\{ v_{j} \\}\n  \nis still a spanning set.\nWe can continue removing such v_{j} if the resulting set \\mathcal{A}_{i} is not linearly independent.\nSince the resulting set \\mathcal{A}_{i} is always a spanning set of \\mathcal{W}, we will eventually get to the step i where the \\mathcal{A}_{i} is linearly dependent and \\mathcal{A}_{i} will be the basis for the subspace \\mathcal{W}. :::\n\n(cardinality-of-basis)=\n\nCardinality of basis\nThe numbers of elements of all bases of a given subspace are the same.\n:::{prf:proof} cardinality of basis :class: dropdown\nSuppose \\mathcal{A} is a basis of a subspace \\mathcal{S} and \\mathcal{B} is another basis of \\mathcal{S}. Thus, \\mathcal{A} and \\mathcal{B} are both linearly independent and spanning sets.\nSince \\mathcal{A} is a spanning set and \\mathcal{B} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\geq \\lvert \\mathcal{B} \\rvert.\n  \nSince \\mathcal{B} is a spanning set and \\mathcal{A} is linearly independent,\n\n  \\lvert \\mathcal{A} \\rvert \\leq \\lvert \\mathcal{B} \\rvert.\n  \nThus,\n\n  \\lvert \\mathcal{A} \\rvert = \\lvert \\mathcal{B} \\rvert.\n  \n:::\n\n(extension-of-a-basis)=\n\nExtension of a basis\nIf the subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\{ b_{1}, \\dots, b_{k} \\} is a basis of \\mathcal{U}, then there exists an extension of \\{ b_{1}, \\dots, b_{k} \\}\n\n  \\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\}\n  \nthat is a basis for \\mathcal{V}\n:::{prf:proof} extension of a basis :class:dropdown\n\\{ b_{1}, \\dots, b_{k}, b_{k + 1}, \\dots, b_{n} \\} is a basis for \\mathcal{V} if \\mathcal{U} \\subseteq \\mathcal{V} and \\{ b_{1}, \\dots, b_{k} \\} is a basis of \\mathcal{U}.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#properties-of-dimension",
    "href": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#properties-of-dimension",
    "title": "Basis",
    "section": "Properties of dimension",
    "text": "Properties of dimension\n(dimension-property-1)=\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V}, then\n\n  \\text{dim} (\\mathcal{U}) \\leq \\text{dim} (\\mathcal{V}).\n  \n:::{prf:proof} $ () () $ if $ $ :class: dropdown\nSuppose there is a basis\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{n} \\}\n  \nfor \\mathcal{U}.\nSince \\mathcal{U} \\subseteq \\mathcal{V}, there are 2 cases.\nIn the case where \\mathcal{U} = \\mathcal{V}, \\mathcal{A} is also a basis of \\mathcal{V} and thus,\n\n  \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}).\n  \nIn the case where \\mathcal{U} \\subset \\mathcal{V}, there is at least 1 vector x \\in \\mathcal{V} such that\n\n  x \\notin \\text{span} (\\mathcal{A}).\n  \n:::\n\n(dimension-property-2)=\n\nIf a subspace \\mathcal{U} is a subset of a finite dimensional subspace \\mathcal{V} and \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), then\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n:::{prf:proof} \\mathcal{U} = \\mathcal{V} if \\mathcal{U} \\subseteq \\mathcal{V} and \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}). :class: dropdown\nSince \\mathcal{U} \\subset \\mathcal{V} and according to the property of extension of the basis, there exists a basis b_{\\mathcal{U}} for \\mathcal{U} and b_{\\mathcal{V}} for \\mathcal{V} such that\n\n  \\mathcal{B}_{\\mathcal{U}} \\subset \\mathcal{B}_{\\mathcal{V}}.\n  \nHowever, since \\text{dim} (\\mathcal{U}) = \\text{dim} (\\mathcal{V}), it must follows that\n\n  \\mathcal{B}_{\\mathcal{U}} = \\mathcal{B}_{\\mathcal{V}}.\n  \nSince \\mathcal{U} and \\mathcal{V} shares the same basis,\n\n  \\mathcal{U} = \\mathcal{V}.\n  \n:::\n\n(dimension-property-3)=\n\nGiven \\mathcal{U} and \\mathcal{V} are subspaces, then\n\n  \\text{dim} (\\mathcal{U} + \\mathcal{V}) = \\text{dim} (\\mathcal{U}) + \\text{dim} (\\mathcal{V}) - \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}).\n  \n:::{prf:proof} \\text{dim} (\\mathcal{U} + \\mathcal{V}) = \\text{dim} (\\mathcal{U}) + \\text{dim} (\\mathcal{V}) - \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}). :class: dropdown\nSuppose \\text{dim} (\\mathcal{U} \\cap \\mathcal{V}) = k, \\text{dim} (\\mathcal{U}) = m, and \\text{dim} (\\mathcal{V}) = n.\nAssume there is a basis\n\n  \\mathcal{B} = \\{ b_{1}, \\dots, b_{k} \\}\n  \nfor \\mathcal{U} \\cap \\mathcal{V}.\nSince \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{U} and \\mathcal{U} \\cap \\mathcal{V} \\subseteq \\mathcal{V}, there exists a basis \\mathcal{A} for \\mathcal{U} and \\mathcal{C} for \\mathcal{V}\n\n  \\mathcal{A} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m} \\}\n  \n\n  \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, c_{k + 1}, \\dots, c_{n} \\}\n  \nas extensions of \\mathcal{B}, according to extension of basis property.\nConsider the set\n\n  \\mathcal{A} \\cup \\mathcal{C} = \\{ b_{1}, \\dots, b_{k}, a_{k + 1}, \\dots, a_{m}, c_{k + 1}, \\dots, c_{n} \\}\n  \nwe will prove that it is a basis of \\mathcal{U} + \\mathcal{V}.\nFirst, consider all x \\in \\mathcal{U} + \\mathcal{V}, which can be represented using the basis of \\mathcal{U} and the basis of \\mathcal{V}\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{m} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} (\\alpha_{i} + \\beta_{i}) b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i} + \\beta_{i} c_{i}.\n  \\end{aligned}\n  \nThus, any vector x \\in \\mathcal{U} + \\mathcal{V} is a linear combination of vectors in \\mathcal{A} \\cup \\mathcal{C} and we have\n\n  \\mathcal{U} + \\mathcal{V} \\subseteq \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nConversely, all x \\in \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) can be written as the linear combinations of vectors in \\mathcal{A} and \\mathcal{C}, and thus\n\n  \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right) \\subseteq \\mathcal{U} + \\mathcal{V}.\n  \nThus,\n\n  \\mathcal{U} + \\mathcal{V} = \\text{span} \\left( \\mathcal{A} \\cup \\mathcal{C} \\right).\n  \nNext, we will prove the vectors in \\mathcal{A} \\cup \\mathcal{C} are linearly independent by contradiction.\nConsider x \\in \\mathcal{U} \\cap \\mathcal{V}, which can be represented by the basis of \\mathcal{U} \\cap \\mathcal{V}, \\mathcal{U} and, \\mathcal{V} at the same time\n\n  \\begin{aligned}\n  x\n  & = \\sum_{i=1}^{k} \\alpha_{i} b_{i} + \\sum_{i=k+1}^{m} \\alpha_{i} a_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\beta_{i} b_{i} + \\sum_{i=k+1}^{n} \\beta_{i} c_{i}\n  \\\\\n  & = \\sum_{i=1}^{k} \\epsilon_{i} b_{i}.\n  \\end{aligned}\n  \nSuppose x = 0, then because\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "href": "Knowledge/Linear Algebra/04_Basis_and_Dimension.html#types-of-the-subspaces",
    "title": "Basis",
    "section": "Types of the subspaces",
    "text": "Types of the subspaces\nWe can classify the types of the subspaces based on their dimensions. For example, in \\mathbb{R}^{n} there are n types of subspaces\n\n0 dimension subspace: \\{ 0 \\}.\n1 dimension subspaces.\n…\nn dimension subspace: \\mathbb{R}^{n} itself."
  },
  {
    "objectID": "Knowledge/Linear Algebra/11_Pseudoinverse.html",
    "href": "Knowledge/Linear Algebra/11_Pseudoinverse.html",
    "title": "Pseudoinverse",
    "section": "",
    "text": "A generalized inverse for any matrix can be defined using URV factorization or SVD.\nGiven a URV factorization of matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{T}\nthe pseudoinverse of \\mathbf{A} is defined as\n\\mathbf{A}^{\\dagger} = \\mathbf{V}\n\\begin{bmatrix}\n\\mathbf{C}^{-1} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{U}^{T}.\nThe pseudoinverse of matrix \\mathbf{A} can also be stated as a matrix \\mathbf{A}^{\\dagger} that satisfies the following:\n\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger},\nwhere \\mathbf{A} \\mathbf{A}^{\\dagger} and \\mathbf{A}^{\\dagger} \\mathbf{A} are symmetric matrix:\n(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger},\n(\\mathbf{A}^{\\dagger} \\mathbf{A})^{T} = \\mathbf{A}^{\\dagger} \\mathbf{A}."
  },
  {
    "objectID": "Knowledge/Linear Algebra/11_Pseudoinverse.html#properties-of-pseudoinverse",
    "href": "Knowledge/Linear Algebra/11_Pseudoinverse.html#properties-of-pseudoinverse",
    "title": "Pseudoinverse",
    "section": "Properties of pseudoinverse",
    "text": "Properties of pseudoinverse\n\nThe pseudoinverse \\mathbf{A}^{\\dagger} of \\mathbf{A} is unique.\n:::{prf:proof} \\mathbf{A}^{\\dagger} is unique :class: dropdown\nWe prove by contradiction. Suppose that there are 2 pseudoinverse \\mathbf{B}, \\mathbf{C} for \\mathbf{A}.\n\n  \\begin{aligned}\n  \\mathbf{A} \\mathbf{B}\n  & = (\\mathbf{A} \\mathbf{C} \\mathbf{A}) \\mathbf{B}\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{C}^{T} \\mathbf{A}^{T}) (\\mathbf{B}^{T} \\mathbf{A}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T})\n  \\\\\n  & = \\mathbf{C}^{T} (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T}\n  \\\\\n  & = \\mathbf{C}^{T} \\mathbf{A}^{T}\n  \\\\\n  & = \\mathbf{A} \\mathbf{C}\n  \\\\\n  \\end{aligned}\n  \n\n  \\begin{aligned}\n  \\mathbf{B} \\mathbf{A}\n  & = \\mathbf{B} (\\mathbf{A} \\mathbf{C} \\mathbf{A})\n  & [\\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T}) (\\mathbf{A}^{T} \\mathbf{C}^{T})\n  & [(\\mathbf{A} \\mathbf{A}^{\\dagger})^{T} = \\mathbf{A} \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = (\\mathbf{A}^{T} \\mathbf{B}^{T} \\mathbf{A}^{T}) \\mathbf{C}^{T}\n  \\\\\n  & = (\\mathbf{A} \\mathbf{B} \\mathbf{A})^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{A}^{T} \\mathbf{C}^{T}\n  \\\\\n  & = \\mathbf{C} \\mathbf{A}\n  \\\\\n  \\end{aligned}\n  \nThus,\n\n  \\begin{aligned}\n  \\mathbf{B}\n  & = \\mathbf{B} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{B}\n  & [\\mathbf{C} \\mathbf{A} = \\mathbf{B} \\mathbf{A}]\n  \\\\\n  & = \\mathbf{C} \\mathbf{A} \\mathbf{C}\n  & [\\mathbf{A} \\mathbf{B} = \\mathbf{A} \\mathbf{C}]\n  \\\\\n  & = \\mathbf{C}\n  \\end{aligned}\n  \n:::\nGiven a full rank matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}.\nIf m &gt; n, the left inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T}.\n  \nIf m &lt; n, the right inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A} and is written as\n\n  \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} .\n  \nIf m = n, the inverse of \\mathbf{A} is the pseudoinverse of \\mathbf{A}.\n:::{prf:proof} the inverse of a full rank matrix is its pseudoinverse :class: dropdown\nWe first prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} (when m &gt; n) and (\\mathbf{A} \\mathbf{A}^{T})^{-1} (when m &lt; n) exist.\nSince \\mathbf{A} is a full rank matrix,\n\n  \\text{rank} (\\mathbf{A}) = \\min (m, n).\n  \nAccording to the property of rank,\n\n  \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\text{rank} (\\mathbf{A} \\mathbf{A}^{T}) = \\text{rank} (\\mathbf{A}) = \\text{min} (m, n).\n  \nThus, \\mathbf{A}^{T} \\mathbf{A} (m &gt; n) and \\mathbf{A} \\mathbf{A}^{T} (m &lt; n) are non-singular.\n\nif m &gt; n, \\mathbf{A}^{T} \\mathbf{A} \\in \\mathbb{R}^{n \\times n} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = n,\nif m &lt; n, \\mathbf{A} \\mathbf{A}^{T} \\in \\mathbb{R}^{m \\times m} is full rank because \\text{rank} (\\mathbf{A}^{T} \\mathbf{A}) = \\min (m, n) = m.\n\nwe prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} when (m &gt; n) is the pseudoinverse of \\mathbf{A}.\n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{A} \\mathbf{I}_{n \\times n} = \\mathbf{A}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} \\mathbf{A}^{\\dagger} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = \\mathbf{I}_{n \\times n} \\mathbf{A}^{\\dagger} = \\mathbf{A}^{\\dagger}\n  \n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n} = \\mathbf{A}^{T} \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} = \\mathbf{A}^{T} (\\mathbf{A}^{\\dagger})^{\\mathbf{T}}= (\\mathbf{A}^{\\dagger} \\mathbf{A})^{T}\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} = \\mathbf{A} (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} = (\\mathbf{A}^{\\dagger})^{T} \\mathbf{A}^{T} = (\\mathbf{A} \\mathbf{A}^{\\dagger})^{T}\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} when (m &lt; n) can be proved similarly.\nThen we prove that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} is the left inverse of \\mathbf{A} when m &gt; n:\n\n  \\mathbf{A}^{\\dagger} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} \\mathbf{A} = \\mathbf{I}_{n \\times n}.\n  \nThe case that \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} is the right inverse of \\mathbf{A} when m &lt; n can be proved similarly.\nThus we have proved that (\\mathbf{A}^{T} \\mathbf{A})^{-1} \\mathbf{A}^{T} (m &gt; n) and \\mathbf{A}^{T} (\\mathbf{A} \\mathbf{A}^{T})^{-1} (m &lt; n) are the left and right inverse of \\mathbf{A}.\nWhen m = n, we have both\n\n  \\mathbf{A} \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{A},\n  \n\n  \\mathbf{A} \\mathbf{A}^{\\dagger} \\mathbf{A} = \\mathbf{A},\n  \nbut since \\mathbf{A}^{-1} and \\mathbf{A}^{\\dagger} are both unique, it must be that\n\n  \\mathbf{A}^{-1} = \\mathbf{A}^{\\dagger}.\n  \n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/10_URV_Fractorization_and_SVD.html",
    "href": "Knowledge/Linear Algebra/10_URV_Fractorization_and_SVD.html",
    "title": "URV Factorization",
    "section": "",
    "text": "For each A \\in \\mathbb{R}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U}_{m \\times m} and \\mathbf{V}_{n \\times n} and a nonsingular matrix \\mathbf{C}_{r \\times r} such that\n\n\\mathbf{A} = \\mathbf{U} \\mathbf{R} \\mathbf{V}^{T} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{C}_{r \\times r} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}_{m \\times n}\n\\mathbf{V}^{T}.\n\n\nThe first r columns in \\mathbf{U} are an orthonormal basis for R (\\mathbf{A}).\nThe last m - r columns of \\mathbf{U} are an orthonormal basis for N (\\mathbf{A}^{T}).\nThe first r columns in \\mathbf{V} are an orthonormal basis for R (\\mathbf{A}^{T}).\nThe last m - r columns of \\mathbf{V} are an orthonormal basis for N (\\mathbf{A}).\n\n:::{prf:proof} URV factorization :class: dropdown\nSuppose vector spaces \\mathbb{R}^{m} and \\mathbb{R}^{n} have orthonormal bases \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{m} \\} and \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n} \\}, respectively.\nGiven any matrix \\mathbf{A} \\in \\mathbb{R}^{m \\times n}, the orthogonal decomposition theorem shows that the following subspaces are complementary in \\mathbb{R}^{m} and \\mathbb{R}^{n}.\n\nR (\\mathbf{A}) \\oplus N (\\mathbf{A}^{T}) = \\mathbb{R}^{m},\n\n\nR (\\mathbf{A}^{T}) \\oplus N (\\mathbf{A}) = \\mathbb{R}^{n}.\n\nAccording to the property of the complementary subspaces, we can separate the basis for \\mathbb{R}^{m} into the basis for R (\\mathbf{A}) and N (\\mathbf{A}^{T}), and the basis for \\mathbb{R}^{n} into the basis for N (\\mathbf{A}) and R (\\mathbf{A}^{T}).\nSince \\mathrm{rank} (\\mathbf{A}) = r, suppose the bases are separated in the following way:\n\n\\mathcal{B}_{R (\\mathbf{A})} = \\{ \\mathbf{u}_{1}, \\dots, \\mathbf{u}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A}^{T})} = \\{ \\mathbf{u}_{r + 1}, \\dots , \\mathbf{u}_{m} \\},\n\\mathcal{B}_{R (\\mathbf{A}^{T})} = \\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{r} \\},\n\\mathcal{B}_{N (\\mathbf{A})} = \\{ \\mathbf{v}_{r + 1}, \\dots , \\mathbf{v}_{n} \\}.\n\nNow we treat the bases for \\mathbb{R}^{m} and \\mathbb{R}^{n} as the columns of the matrices \\mathbf{U} and \\mathbf{V}:\n\n\\mathbf{U} =\n\\begin{bmatrix}\n\\mathbf{u}_{1} & \\dots & \\mathbf{u}_{m}\n\\end{bmatrix},\n\n\n\\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{v}_{1} & \\dots & \\mathbf{v}_{n}\n\\end{bmatrix},\n\nand define\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}.\n\nNote that\n\n\\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0\n\\end{bmatrix}\n\n\n\\mathbf{U}^{T} \\mathbf{A} = (\\mathbf{A}^{T} \\mathbf{U})^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{n}\n\\end{bmatrix}^{T} =\n\\begin{bmatrix}\n\\mathbf{A}^{T} \\mathbf{u}_{1} & \\dots & \\mathbf{A}^{T} \\mathbf{u}_{r} & 0 & \\dots & 0\n\\end{bmatrix}^{T}\n\nsince \\mathbf{v}_{r + 1}, \\dots, \\mathbf{v}_{n} \\in N (\\mathbf{A}), and \\mathbf{u}_{r + 1}, \\dots, \\mathbf{u}_{m} \\in N (\\mathbf{A}^{T}).\nThus, \\mathbf{R} has mostly 0 except the top left corner:\n\n\\mathbf{R} = \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} =\n\\begin{bmatrix}\n\\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{1}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{1} & \\dots & \\mathbf{u}_{r}^{T} \\mathbf{A} \\mathbf{v}_{r} & 0 & \\dots & 0 \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & \\dots & 0 & 0 & \\dots & 0 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{C} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}.\n\nThen we can see \\mathbf{A} can always be decomposed in terms of \\mathbf{U}, \\mathbf{R}, \\mathbf{V} because of the property of orthogonal matrix\n\n\\begin{aligned}\n\\mathbf{U} \\mathbf{R} \\mathbf{V}^{T}\n& = \\mathbf{U} \\mathbf{U}^{T} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{T}\n\\\\\n& = \\mathbf{U} \\mathbf{U}^{-1} \\mathbf{A} \\mathbf{V} \\mathbf{V}^{-1}\n\\\\\n& = \\mathbf{A}.\n\\end{aligned}\n\nTo see why \\mathbf{C} is always non-singular, we first note that\n\n\\text{rank} (\\mathbf{A}) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r\n\nbecause the multiplication by a full-rank square matrix preserves rank.\nThus,\n\n\\text{rank} (\\mathbf{C}) = \\text{rank} \\left(\n    \\begin{bmatrix}\n    \\mathbf{C} & 0 \\\\\n    0 & 0 \\\\\n    \\end{bmatrix}\n\\right) = \\text{rank} (\\mathbf{U}^{T} \\mathbf{A} \\mathbf{V}) = r.\n\n:::\n\nSingular Value Decomposition (SVD)\nSingular value decomposition is a special case of the URV factorization where the \\mathbf{C} matrix is a diagonal matrix with increasing values in the diagonal.\nFor each \\mathbf{A} \\in \\mathbb{C}^{m \\times n} of rank r, there are orthogonal matrices \\mathbf{U} \\in \\mathbb{R}^{m \\times m} and \\mathbf{V} \\in \\mathbb{R}^{n \\times n}, and a diagonal matrix \\mathbf{D} \\in \\mathbb{C}^{r \\times r} = \\text{diag} (\\sigma_{1}, \\dots, \\sigma_{r}) such that\n\n\\mathbf{A} = \\mathbf{U}\n\\begin{bmatrix}\n\\mathbf{D} & 0 \\\\\n0 & 0 \\\\\n\\end{bmatrix}\n\\mathbf{V}^{H}\n\nwith\n\n\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots, \\geq \\sigma_{r},\n\nwhere\n\nthe columns in \\mathbf{U} and \\mathbf{V} are singular vectors and\nthe diagonal values of \\mathbf{D} (\\sigma_{i}) are singular values."
  },
  {
    "objectID": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html",
    "href": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html",
    "title": "Orthogonal Complement",
    "section": "",
    "text": "For a subset \\mathcal{M} of an inner-product space \\mathcal{V}, the orthogonal complement \\mathcal{M}^{\\perp} of \\mathcal{M} is defined to be the set of all vectors in \\mathcal{V} that are orthogonal to every vector in \\mathcal{M}\n\\mathcal{M}^{\\perp} = \\left\\{\n    x \\in \\mathcal{V} \\mid \\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M}\n\\right\\}.\nThe set \\mathcal{M}^{\\perp} is a subspace even if \\mathcal{M} is not.\n:::{prf:proof} \\mathcal{M}^{\\perp} is a subspace :class: dropdown\nThe set \\mathcal{M}^{\\perp} is closed under addition and multiplication.\nSuppose x, y \\in \\mathcal{M}^{\\perp}, which means\n\\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M},\n\\langle y, m \\rangle = 0, \\forall m \\in \\mathcal{M}.\nClosed under addition:\n\\langle x + y, m \\rangle = \\langle x, m \\rangle + \\langle y, m \\rangle = 0, \\forall m \\in \\mathcal{M}.\nClosed under multiplication:\n\\langle \\alpha x, m \\rangle = \\alpha \\langle x, m \\rangle = 0, \\forall m \\in \\mathcal{M}, \\forall \\alpha \\in \\mathbb{F}.\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "href": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#orthogonal-complementary-subspaces",
    "title": "Orthogonal Complement",
    "section": "Orthogonal Complementary Subspaces",
    "text": "Orthogonal Complementary Subspaces\nWhen \\mathcal{M} is a subspace of a finite-dimensional inner-product space \\mathcal{V},\n\n\\mathcal{V} = \\mathcal{M} \\oplus \\mathcal{M}^{\\perp}.\n\n:::{prf:proof} \\mathcal{V} = \\mathcal{M} \\oplus \\mathcal{M}^{\\perp} :class: dropdown\nTODO\n:::"
  },
  {
    "objectID": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#properties-of-orthogonal-complementary-subspaces",
    "href": "Knowledge/Linear Algebra/09_Orthogonal_Complement_and_Decomposition.html#properties-of-orthogonal-complementary-subspaces",
    "title": "Orthogonal Complement",
    "section": "Properties of orthogonal complementary subspaces",
    "text": "Properties of orthogonal complementary subspaces\nSuppose \\mathcal{M} is a subspace of an n-dimensional inner-product space \\mathcal{V}.\n(orthogonal-complementary-subspaces-property-1)=\n\n\\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp}) = n\n\n:::{prf:proof} \\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp}) = n :class: dropdown\nSince \\mathcal{M}^{T} and \\mathcal{M} are complementary subspaces, the property of complementary subspace shows that\n\n\\mathcal{B}_{\\mathcal{M}} \\cup \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\mathcal{B}_{\\mathcal{V}}\n\nand\n\n\\mathcal{B}_{\\mathcal{M}} \\cap \\mathcal{B}_{\\mathcal{M}^{\\perp}} = \\emptyset.\n\nThus,\n\n\\begin{aligned}\n\\lvert \\mathcal{B}_{\\mathcal{M}} \\rvert + \\lvert \\mathcal{B}_{\\mathcal{M}^{\\perp}} \\rvert\n& = \\lvert \\mathcal{B}_{\\mathcal{V}} \\rvert\n\\\\\n\\text{dim} (\\mathcal{M}) + \\text{dim} (\\mathcal{M}^{\\perp})\n& = n\n\\end{aligned}\n\n:::\n(orthogonal-complementary-subspaces-property-2)=\n\n\\mathcal{M}^{\\perp^{\\perp}} = \\mathcal{M}.\n\n:::{prf:proof} \\mathcal{M}^{\\perp^{\\perp}} = \\mathcal{M} :class: dropdown\nSince \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{V},\n\nx \\in \\mathcal{M}^{\\perp^{\\perp}} \\Rightarrow x \\in \\mathcal{V}.\n\nSince \\mathcal{M} and \\mathcal{M}^{\\perp} are complementary subspaces, every x \\in \\mathcal{V} can be uniquely represented by m \\in \\mathcal{M} and n \\in \\mathcal{M}^{\\perp}\n\nx = m + n.\n\nSince \\mathcal{M}^{\\perp^{\\perp}} \\perp \\mathcal{M}^{\\perp}, by definition\n\n\\begin{aligned}\n0\n& = \\langle x, n \\rangle\n\\\\\n& = \\langle m + n, n \\rangle\n\\\\\n& = \\langle m, n \\rangle + \\langle n, n \\rangle\n\\\\\n& = \\langle n, n \\rangle\n& [\\mathcal{M} \\perp \\mathcal{M}^{\\perp} \\Rightarrow \\langle m, n \\rangle = 0].\n\\end{aligned}\n\nBy the definition of the inner product,\n\n\\langle n, n \\rangle = 0 \\Rightarrow n = 0.\n\nThus, for each x \\in \\mathcal{M}^{\\perp^{\\perp}}\n\nx = m \\in \\mathcal{M} \\Rightarrow \\mathcal{M}^{\\perp^{\\perp}} \\subseteq \\mathcal{M}.\n\nBy the property\n\n\\text{dim} (\\mathcal{M}) = n - \\text{dim} (\\mathcal{M}^{\\perp}) = \\text{dim} (\\mathcal{M}^{\\perp^{\\perp}}).\n\nThen by the property,\n\n\\mathcal{M} = \\mathcal{M}^{\\perp^{\\perp}}.\n\n:::"
  },
  {
    "objectID": "Knowledge/Information Theory/Entropy.html",
    "href": "Knowledge/Information Theory/Entropy.html",
    "title": "Joeyonng",
    "section": "",
    "text": "The entropy H(X) of a discrete random variable X is defined by\n\nH (X) = - \\sum_{x} \\mathbb{P}_{X} (x) \\log \\mathbb{P}_{X} (x),\n\nwhich can be written as the expectation of the random variable \\log \\mathbb{P}_{X} [x]\n\nH (X) = - \\mathbb{E}_{X} \\left[\n    \\log \\mathbb{P}_{X} (x)\n\\right].\n\n\n\n\nThe joint entropy H(X, Y) of a pair of discrete random variables X, Y with a joint distribution \\mathbb{P}_{X, Y} (x, y) is defined as\n\n\\begin{aligned}\nH (X, Y)\n& = - \\mathbb{E}_{X, Y} \\left[\n    \\mathbb{P}_{X, Y} (x, y)\n\\right]\n\\\\\n& = - \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X, Y} (x, y)\n\\end{aligned}\n\n\n\n\nGiven a joint probability \\mathbb{P}_{X, Y} (x, y), the conditional entropy H (Y \\mid X) is defined as\n\n\\begin{aligned}\nH(Y \\mid X)\n& = - \\mathbb{E}_{X, Y} \\left[\n    \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\right]\n\\\\\n& = - \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = - \\sum_{x} \\mathbb{P}_{X} (x) \\sum_{y} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\log \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\sum_{x} \\mathbb{P}_{X} (x) H (Y \\mid x)\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Information Theory/Entropy.html#entropy",
    "href": "Knowledge/Information Theory/Entropy.html#entropy",
    "title": "Joeyonng",
    "section": "",
    "text": "The entropy H(X) of a discrete random variable X is defined by\n\nH (X) = - \\sum_{x} \\mathbb{P}_{X} (x) \\log \\mathbb{P}_{X} (x),\n\nwhich can be written as the expectation of the random variable \\log \\mathbb{P}_{X} [x]\n\nH (X) = - \\mathbb{E}_{X} \\left[\n    \\log \\mathbb{P}_{X} (x)\n\\right].\n\n\n\n\nThe joint entropy H(X, Y) of a pair of discrete random variables X, Y with a joint distribution \\mathbb{P}_{X, Y} (x, y) is defined as\n\n\\begin{aligned}\nH (X, Y)\n& = - \\mathbb{E}_{X, Y} \\left[\n    \\mathbb{P}_{X, Y} (x, y)\n\\right]\n\\\\\n& = - \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X, Y} (x, y)\n\\end{aligned}\n\n\n\n\nGiven a joint probability \\mathbb{P}_{X, Y} (x, y), the conditional entropy H (Y \\mid X) is defined as\n\n\\begin{aligned}\nH(Y \\mid X)\n& = - \\mathbb{E}_{X, Y} \\left[\n    \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\right]\n\\\\\n& = - \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = - \\sum_{x} \\mathbb{P}_{X} (x) \\sum_{y} \\mathbb{P}_{Y \\mid X} (y \\mid x) \\log \\mathbb{P}_{Y \\mid X} (y \\mid x)\n\\\\\n& = \\sum_{x} \\mathbb{P}_{X} (x) H (Y \\mid x)\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Information Theory/Entropy.html#relative-entropy",
    "href": "Knowledge/Information Theory/Entropy.html#relative-entropy",
    "title": "Joeyonng",
    "section": "Relative entropy",
    "text": "Relative entropy\n\nRelative entropy (KL Divergence)\nThe relative entropy or Kullback–Leibler (KL) divergence between two distributions p (x), q (x) for the same random variable X is defined as\n\n\\begin{aligned}\nD (p \\Vert q)\n& = \\mathbb{E}_{X} \\left[\n    \\log \\frac{ p (X) }{ q(X) }\n\\right]\n\\\\\n& = \\sum_{x} p (x) \\log \\frac{ p (x) }{ q (x) }\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Information Theory/Entropy.html#mutual-information",
    "href": "Knowledge/Information Theory/Entropy.html#mutual-information",
    "title": "Joeyonng",
    "section": "Mutual Information",
    "text": "Mutual Information\n\nMutual information\nThe mutual information I (X; Y) is the relative entropy between the joint distribution of two random variables X, Y and the product distribution\n\n\\begin{aligned}\nI (X; Y)\n& = \\mathbb{E}_{X, Y} \\left[\n    \\frac{\n        \\mathbb{P}_{X, Y} (X, Y)\n    }{\n        \\mathbb{P}_{X} (X) \\mathbb{P}_{Y} (Y)\n    }\n\\right]\n\\\\\n& = \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\frac{\n    \\mathbb{P}_{X, Y} (x, y)\n}{\n    \\mathbb{P}_{X} (x) \\mathbb{P}_{Y} (y)\n}\n\\\\\n& = D (\\mathbb{P}_{X, Y} \\Vert \\mathbb{P}_{X} \\mathbb{P}_{Y})\n\\end{aligned}\n\nAlso, the mutual information can be rewritten using entropies\n\n\\begin{aligned}\nI (X; Y)\n& = \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\frac{\n    \\mathbb{P}_{X, Y} (x, y)\n}{\n    \\mathbb{P}_{X} (x) \\mathbb{P}_{Y} (y)\n}\n\\\\\n& = \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\frac{\n    \\mathbb{P}_{X \\mid Y} (x \\mid y) \\mathbb{P}_{Y} (y)\n}{\n    \\mathbb{P}_{X} (x) \\mathbb{P}_{Y} (y)\n}\n\\\\\n& =\n- \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X} (x)\n+ \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X \\mid Y} (x \\mid y)\n\\\\\n& =\n- \\sum_{x} \\mathbb{P}_{X} (x) \\log \\mathbb{P}_{X} (x)\n+ \\sum_{x, y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X \\mid Y} (x \\mid y)\n\\\\\n& =\nH (X) - H (X \\mid Y)\n\\end{aligned}\n\nThen according to the chain rule of entropy H (X, Y) = H (Y) + H (X \\mid Y),\n\nI (X; Y) = H (X) - H (X \\mid Y) = H (X) + H (Y) - H (X, Y).\n\n\n\nConditional mutual information\n\n\\begin{aligned}\nI (X; Y \\mid Z)\n& = \\sum_{x, y, z} \\mathbb{P}_{X, Y, Z} \\log \\frac{\n    \\mathbb{P}_{X, Y \\mid Z}\n}{\n    \\mathbb{P}_{X \\mid Z} \\mathbb{P}_{Y \\mid Z}\n}\n\\\\\n& = - \\sum_{x, y, z} \\mathbb{P}_{X, Y, Z} \\log \\left[\n    \\mathbb{P}_{X \\mid Z} \\mathbb{P}_{Y \\mid Z}\n\\right]\n+ \\sum_{x, y, z} \\mathbb{P}_{X, Y, Z} \\log \\mathbb{P}_{X \\mid Y, Z}\n\\\\\n& = - \\sum_{x, z} \\mathbb{P}_{X, Z} \\log \\mathbb{P}_{X \\mid Z}\n+ \\sum_{x, y, z} \\mathbb{P}_{X, Y, Z} \\log \\mathbb{P}_{X \\mid Y, Z}\n\\\\\n& = H (X \\mid Z) - H (X \\mid Y, Z)\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Information Theory/Entropy.html#chain-rule",
    "href": "Knowledge/Information Theory/Entropy.html#chain-rule",
    "title": "Joeyonng",
    "section": "Chain rule",
    "text": "Chain rule\n\nChain rule for entropy\nThe joint entropy between 2 variables can be decomposed into an entropy and a conditional entropy\n\n\\begin{aligned}\nH (X, Y)\n& = - \\sum_{x} \\sum_{y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X, Y} (x, y)\n\\\\\n& = - \\sum_{x} \\sum_{y} \\mathbb{P}_{X, Y} (x, y) \\log \\left[\n    \\mathbb{P}_{X \\mid Y} (x \\mid y) \\mathbb{P}_{Y} (y)\n\\right]\n\\\\\n& = - \\sum_{x} \\sum_{y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X \\mid Y} (x \\mid y)\n- \\sum_{x} \\sum_{y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{Y} (y)\n\\\\\n& = - \\sum_{x} \\sum_{y} \\mathbb{P}_{X, Y} (x, y) \\log \\mathbb{P}_{X \\mid Y} (x \\mid y)\n- \\sum_{y} \\mathbb{P}_{Y} (y) \\log \\mathbb{P}_{Y} (y)\n\\\\\n& = H(X \\mid Y) + H (Y).\n\\end{aligned}\n\nThe chain rule can also be applied to conditional entropy.\n\n\\begin{aligned}\nH (X, Y \\mid Z)\n& = - \\sum_{x} \\sum_{y} \\sum_{z} \\mathbb{P}_{X, Y} (x, y, z) \\log \\mathbb{P}_{X, Y, \\mid Z} (x, y \\mid z)\n\\\\\n& = - \\sum_{x} \\sum_{y} \\sum_{z} \\mathbb{P}_{X, Y, Z} (x, y, z) \\log \\left[\n    \\mathbb{P}_{X \\mid Y, Z} (x \\mid y, z) \\mathbb{P}_{Y \\mid Z} (y \\mid z)\n\\right]\n\\\\\n& = - \\sum_{x} \\sum_{y} \\sum_{z} \\mathbb{P}_{X, Y, Z} (x, y, z) \\log \\mathbb{P}_{X \\mid Y, Z} (x \\mid y, z)\n- \\sum_{x} \\sum_{y} \\sum_{z} \\mathbb{P}_{X, Y, Z} (x, y, z) \\log \\mathbb{P}_{Y \\mid Z} (y \\mid z)\n\\\\\n& = - \\sum_{x} \\sum_{y} \\sum_{z} \\mathbb{P}_{X, Y, Z} (x, y, z) \\log \\mathbb{P}_{X \\mid Y, Z} (x \\mid y, z)\n- \\sum_{y} \\sum_{z} \\mathbb{P}_{Y, Z} (y, z) \\log \\mathbb{P}_{Y \\mid Z} (y \\mid z)\n\\\\\n& = H(X \\mid Y, Z) + H (Y \\mid Z).\n\\end{aligned}\n\nBy treating X_{2}, \\dots, X_{n} as one joint variable, the chain rule for joint entropy with multiple variables X_{1}, \\dots, X_{n} can be derived\n\n\\begin{aligned}\nH (X_{1}, \\dots, X_{n})\n& = H (X_{1}) + H (X_{2}, \\dots, X_{n} \\mid X_{1})\n\\\\\n& = H (X_{1}) + H (X_{2} \\mid X_{1}) + H (X_{3}, \\dots, X_{n} \\mid X_{2}, X_{1})\n\\\\\n& = H (X_{1}) + H (X_{2} \\mid X_{1}) +, \\dots, + H (X_{n} \\mid X_{n - 1}, \\dots, X_{1})\n\\\\\n& = \\sum_{i = 1}^{n} H (X_{i} \\mid X_{i - 1}, \\dots, X_{1})\n\\end{aligned}\n\n\n\nChain rule for mutual information\n\n\\begin{aligned}\nI (X_{1}, \\dots, X_{n}; Y)\n& = H (X_{1}, \\dots, X_{n}) - H (X_{1}, \\dots, X_{n} \\mid Y)\n\\\\\n& = \\sum_{i = 1}^{n} H (X_{i} \\mid X_{i - 1}, \\dots, X_{1})\n- \\sum_{i = 1}^{n} H (X_{i} \\mid Y, X_{i - 1}, \\dots, X_{1})\n\\\\\n& = \\sum_{i = 1}^{n} \\left[\n    H (X_{i} \\mid X_{i - 1}, \\dots, X_{1})\n    - H (X_{i} \\mid Y, X_{i - 1}, \\dots, X_{1})\n\\right]\n\\\\\n& = \\sum_{i = 1}^{n} I (X_{i}; Y \\mid X_{i - 1}, \\dots, X_{1})\n\\end{aligned}"
  },
  {
    "objectID": "Knowledge/Calculus/Chain_rule.html",
    "href": "Knowledge/Calculus/Chain_rule.html",
    "title": "Multi-variable chain rule",
    "section": "",
    "text": "The multi-variable chain rule is used to calculate the derivative of the output of a composite function with respect to the input where the inside function of the composite function is a vector-valued function.\n\nBase case: scalar-valued univariate function c (x) = f (\\mathbf{g} (x))\nGiven a scalar-valued univariate function c (x) = f (\\mathbf{g} (x)) that is a composite function of a scalar-valued multivariate function f: \\mathbb{R}^{m} \\mapsto \\mathbb{R} and a vector-valued univaraite function \\mathbf{g}: \\mathbb{R} \\mapsto \\mathbb{R}^{m} that takes a real-value input, the chain rule for calculating the derivative of c (x) at point x is\n\n\\frac{d c}{d x} (x) = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial g_{i} (x)} (\\mathbf{g} (x)) \\frac{d g_{i}}{d x} (x)\n\nwhere\n\n\\frac{\\partial f}{\\partial g_{i} (x)} (\\mathbf{g} (x)) is the partial derivative of the function f with respect of the dimension i at the point \\mathbf{g} (x),\n\\frac{d g_{i}}{d x} (x) is the derivative of function g_{i} at the point x.\n\nTherefore, the derivative of c (x) is a real number that is calculated using the chain rule as the dot product of the gradient of f (\\mathbf{g} (x)) at point \\mathbf{g} (x) and the vector of the derivatives of g_{i} (x) at point x\n\n\\begin{aligned}\n\\frac{d c}{d x} (x)\n& = \\sum_{i=1}^{n} \\frac{\\partial f}{\\partial g_{i} (x)} (\\mathbf{g} (x)) \\frac{d g_{i}}{d x} (x)\n\\\\\n& = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial g_{1} (x)} (\\mathbf{g} (x)) \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial g_{m} (x)} (\\mathbf{g} (x)) \\\\\n\\end{bmatrix}^{T}\n\\begin{bmatrix}\n\\frac{d g_{1}}{d x} (x) \\\\\n\\vdots \\\\\n\\frac{d g_{m}}{d x} (x) \\\\\n\\end{bmatrix}\n\\\\\n& = \\nabla^{T} f (\\mathbf{g} (x)) \\frac{d \\mathbf{g}}{d x} (x)\n\\end{aligned}.\n\n\n\nExtension 1: scalar-valued multivariate function c (x) = f (\\mathbf{g} (\\mathbf{x}))\nWhen the input is a vector, the composite function c (\\mathbf{x}) = f (\\mathbf{g} (\\mathbf{x})) is a scalar-valued multivariate function, whose gradient is calculated by applying the multi-variable chain rule to every element of the input vector\n\n\\begin{aligned}\n\\nabla^{T} c (\\mathbf{x})\n& = \\begin{bmatrix}\n\\frac{\\partial c}{\\partial x_{1}} (\\mathbf{x}) & \\dots & \\frac{\\partial c}{\\partial x_{n}} (\\mathbf{x}) \\\\\n\\end{bmatrix}\n\\\\\n& = \\begin{bmatrix}\n\\frac{d c_{\\mathbf{x}}}{d x_{1}} (x_{1}) & \\dots & \\frac{d c_{\\mathbf{x}}}{d x_{n}} (x_{n}) \\\\\n\\end{bmatrix}\n\\\\\n& = \\nabla^{T} f (\\mathbf{g} (\\mathbf{x})) \\begin{bmatrix}\n\\frac{d \\mathbf{g}}{d x_{1}} (x_{1}) & \\dots & \\frac{d \\mathbf{g}}{d x_{n}} (x_{n})\n\\end{bmatrix}\n\\\\\n& = \\nabla^{T} f (\\mathbf{g} (\\mathbf{x})) \\begin{bmatrix}\n\\nabla^{T} g_{1} (\\mathbf{x}) \\\\\n\\vdots \\\\\n\\nabla^{T} g_{m} (\\mathbf{x}) \\\\\n\\end{bmatrix}\n\\\\\n& = \\nabla^{T} f (\\mathbf{g} (\\mathbf{x})) \\mathbf{J} \\mathbf{g} (\\mathbf{x}).\n\\\\\n\\end{aligned}\n\n\n\nExtension 2: vector-valued univariate function \\mathbf{c} (x) = \\mathbf{f} (\\mathbf{g} (x))\nWhen the output is a vector, the composite function \\mathbf{c} (x) = \\mathbf{f} (\\mathbf{g} (x)) is a vector-valued univariate function, whose derivative is a vector\n\n\\begin{aligned}\n\\frac{d \\mathbf{c}}{d x} (x)\n& = \\begin{bmatrix}\n\\frac{d c_{1}}{d x} (x) \\\\\n\\vdots \\\\\n\\frac{d c_{n}}{d x} (x) \\\\\n\\end{bmatrix}\n\\\\\n& = \\begin{bmatrix}\n\\nabla^{T} f_{1} (\\mathbf{g} (x)) \\\\\n\\vdots \\\\\n\\nabla^{T} f_{n} (\\mathbf{g} (x)) \\\\\n\\end{bmatrix}\n\\frac{d \\mathbf{g}}{d x} (x)\n\\\\\n& = \\mathbf{J} \\mathbf{f} (\\mathbf{g} (x)) \\frac{d \\mathbf{g}}{d x} (x).\n\\\\\n\\end{aligned}\n\n\n\nExtension 3: vector-valued multivariate function \\mathbf{c} (\\mathbf{x}) = \\mathbf{f} (\\mathbf{g} (\\mathbf{x}))\nWhen both input and output are vectors, the composite function \\mathbf{c} (\\mathbf{x}) = \\mathbf{f} (\\mathbf{g} (\\mathbf{x})) is a vector-valued multivariate function, whose derivative is a Jacobian matrix\n\n\\begin{aligned}\n\\mathbf{J} \\mathbf{f} (\\mathbf{x}) & = \\begin{bmatrix}\n\\frac{\\partial f_{1}}{x_{1}} (\\mathbf{x}) & \\dots & \\frac{\\partial f_{1}}{x_{n}} (\\mathbf{x}) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_{m}}{x_{1}} (\\mathbf{x}) & \\dots & \\frac{\\partial f_{m}}{x_{n}} (\\mathbf{x}) \\\\\n\\end{bmatrix}\n\\\\\n& = \\begin{bmatrix}\n- & \\nabla^{T} f_{1} (\\mathbf{g} (\\mathbf{x})) & - \\\\\n& \\vdots & \\\\\n- & \\nabla^{T} f_{n} (\\mathbf{g} (\\mathbf{x})) & - \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n| & & | \\\\\n\\frac{d \\mathbf{g}}{d x_{1}} (x_{1}) & \\dots & \\frac{d \\mathbf{g}}{d x_{n}} (x_{n}) \\\\\n| & & | \\\\\n\\end{bmatrix}\n\\\\\n& = \\begin{bmatrix}\n- & \\nabla^{T} f_{1} (\\mathbf{g} (\\mathbf{x})) & - \\\\\n& \\vdots & \\\\\n- & \\nabla^{T} f_{n} (\\mathbf{g} (\\mathbf{x})) & - \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n- & \\nabla^{T} g_{1} (\\mathbf{x}) & - \\\\\n& \\vdots & \\\\\n- & \\nabla^{T} g_{m} (\\mathbf{x}) & - \\\\\n\\end{bmatrix}\n\\\\\n& = \\mathbf{J} \\mathbf{f} (\\mathbf{g} (\\mathbf{x})) \\mathbf{J} \\mathbf{g} (\\mathbf{x}).\n\\\\\n\\end{aligned}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Litao Qiao 乔立涛",
    "section": "",
    "text": "Hello! I’m Litao Qiao, also known online as Joeyonng. I’m currently a PhD student in ECE department of University of California, San Diego (UCSD) since 2020. UCSD is also where I earned my bachelor’s in 2018.\nMy primary research interest is Explainable machine learning and my expertise extends to general machine learning theory, combinatorial logic, and deep learning.\nThis website serves as a platform for sharing my learned knowledge and research notes. Additionally, I enjoy developing web applications and you can find some of my side projects in my Github.\n\n  \n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n  \n    \n     Resume"
  },
  {
    "objectID": "about.html#selected-papers",
    "href": "about.html#selected-papers",
    "title": "Litao Qiao 乔立涛",
    "section": "Selected Papers",
    "text": "Selected Papers\nQiao, Litao, Weijia Wang, Sanjoy Dasgupta and Bill Lin. “Rethinking Logic Minimization for Tabular Machine Learning.” IEEE Transactions on Artificial Intelligence 4 (2023): 1129-1140.\nQiao, Litao, Weijia Wang and Bill Lin. “Alternative Formulations of Decision Rule Learning from Neural Networks.” Mach. Learn. Knowl. Extr. 5 (2023): 937-956.\nWang, Weijia, Litao Qiao and Bill Lin. “Disjunctive Threshold Networks for Tabular Data Classification.” IEEE Open Journal of the Computer Society 4 (2023): 185-194.\nQiao, Litao, Weijia Wang and Bill Lin. “Learning Accurate and Interpretable Decision Rule Sets from Neural Networks.” AAAI Conference on Artificial Intelligence (2021)."
  }
]