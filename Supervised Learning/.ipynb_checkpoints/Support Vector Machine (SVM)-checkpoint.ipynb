{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63a62a0-3fd6-4f94-8d57-e74d733330ad",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "***\n",
    "\n",
    "1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters.\n",
    "1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane.\n",
    "1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. \n",
    "1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd61bf-dfd6-459a-a839-71e1899a122f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preliminary\n",
    "***\n",
    "(lagrangian)=\n",
    "#### The Lagrangian dual problem (Duality)\n",
    "\n",
    "1. Given a minimization primal problem:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\min_{x} \\quad & f(x) \\\\\n",
    "    \\text{s.t. } \\quad & h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n",
    "    \\quad & l_{j}(x) = 0, \\quad j = 1, \\dots, m \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "1. The Lagrangian of the primal problem is defined as:\n",
    "    $$ L(x, u, v) = f(x) + \\sum_{i}^{n} u_{i}h_{i}(x) + \\sum_{j}^{m} v_{j}l_{j}(x) $$\n",
    "    where $u_{i}$ and $v_{j}$ are new variables called Lagrangian multipliers.\n",
    "1. The Lagrange dual function is:\n",
    "    $$ g(u, v) = \\min_{x} L(x, u, v) $$\n",
    "1. The Lagrange dual problem is:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\max_{u, v} \\quad & g(u, v) \\\\\n",
    "    \\text{s.t. } \\quad & u \\geq 0 \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "1. The properties of dual problem:\n",
    "    1. The dual problem is always convex even if the primal problem is not convex.\n",
    "    1. For any primal problem and its dual problem, the weak duality always holds (the optimal value of the primal problem is always greater or equal to the optimal value of the dual problem).\n",
    "\n",
    "#### Karush-Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "1. Given the Lagrange dual problem stated above, the KKT conditions are:\n",
    "    1. Stationarity condition: \n",
    "        $$ 0 \\in \\partial \\left( f(x) + \\sum_{i=1}^{n} u_{i} h_{i}(x) + \\sum_{j=1}^{m} v_{j}l_{j}(x) \\right) $$\n",
    "    1. Complementary Slackness:\n",
    "        $$ u_{i}h_{i}(x) = 0, \\quad i = 1, \\dots, n $$\n",
    "    1. Primal feasibility:\n",
    "        $$ h_{i}(x) \\leq 0, \\quad i = 1, \\dots, n $$\n",
    "        $$ l_{j}(x) = 0,  \\quad j = 1, \\dots, m $$\n",
    "    1. Dual feasibility:\n",
    "        $$ u_{i} \\geq 0, \\quad i = 1, \\dots, n $$\n",
    "1. If a strong duality (the primal optimal objective and the dual optimal objective are equal) holds, the $x^{*}$ and $u^{*}, v^{*}$ are primal and dual solutions if and only if $x^{*}$ and $u^{*}, v^{*}$ satisfy the KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb8204-b9cc-4732-b9f0-471579e6495f",
   "metadata": {},
   "source": [
    "## SVM formulation\n",
    "***\n",
    "\n",
    "#### SVM without slacks (hard margin SVM)\n",
    "\n",
    "Given a dataset with $n$ instances $x_{i} \\in R^{d}$ and $n$ labels $y_{i} \\in \\{-1, 1\\}$, a hard margin SVM model is a linear function (hyperplane) that is defined by a set of weights $w \\in R^{d}$ and a bias $b \\in R$, which has the largest distances to the support vectors. You can get the hyperplane by solving following optimization problem:\n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "\\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} \\\\\n",
    "\\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1, \\quad i = 1, \\dots n \\\\\n",
    "\\end{alignat}\n",
    "$$\n",
    "Solving the above optimization problem will give us two parallel hyperplanes ($w x + b = 1$ and $w x + b = -1$) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between.\n",
    "\n",
    "1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as \n",
    "    $$ \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert w \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert w \\rVert} = \\frac{2}{\\lVert w \\rVert} $$\n",
    "1. The constraints specify that the instances must be on the correct side of the two hyperplanes:\n",
    "    $$ w x_{i} + b \\geq 1 \\quad \\mathrm{if} y_{i} = 1 $$\n",
    "    $$ w x_{i} + b \\leq -1 \\quad \\mathrm{if} y_{i} = -1 $$\n",
    "    and $y_{i}(w x_{i} + b) \\geq 1$ summarizes the above two conditions.\n",
    "            \n",
    "#### SVM with slacks (soft margin SVM)\n",
    "\n",
    "In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n",
    "$$\n",
    "\\begin{alignat}{2}\n",
    "\\min \\quad & \\frac{1}{2} \\lVert w \\rVert^{2} + C \\sum_{i}^{n} \\xi_{i} \\\\\n",
    "\\text{s.t. } \\quad & y_{i}(w x_{i} + b) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots n \\\\\n",
    "\\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n",
    "\\end{alignat}\n",
    "$$\n",
    "where $\\xi_{i}$ is the slack variable for the instance $x_{i}$ and $C$ is a hyperparameter that penalizes the misclassification of $x_{i}$. \n",
    "\n",
    "1. If $\\xi_{i}$ is nonzero for $x_{i}$, it means that $x_{i}$ is on the misclassified side of $w x_{i} + b = 1$ (or $w x_{i} + b = -1$) and the distance is $\\xi_{i}$. \n",
    "1. If $C = 0$, $\\xi_{i}$ can be arbitrary large for each $x_{i}$. If $C \\to \\inf$, it is the same as hard margin SVM because any misclassification can induce infinite loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cfc66-1ee5-49e2-869a-88e907bc511b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solving SVM\n",
    "***\n",
    "\n",
    "### Solving hard margin SVM\n",
    "1. Rewrite the primal program for easier Lagrangian computation below:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\min \\quad & \\frac{1}{2} ww \\\\\n",
    "    \\text{s.t. } \\quad & -(y_{i}(w x_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots n \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "1. We can derive the Lagrangian primal function from the primal program:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    L(w, b, \\alpha) & = f(w, b) + \\sum_{i}^{n} \\alpha h_{i}(w, b) \\\\\n",
    "    & = \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    where $\\alpha$ is a new variable called Lagrangian multiplier.\n",
    "1. Then we can write and solve Lagrangian dual function:\n",
    "    $$ \n",
    "    \\begin{alignat}{2}\n",
    "    g(\\alpha) & = \\min_{w, b} L(w, b, \\alpha) \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Taking the derivation of $L(w, b, \\alpha)$ over $w$:\n",
    "    $$ \n",
    "    \\begin{alignat}{2}\n",
    "    \\frac{\\partial L}{\\partial w} & = 0 \\\\\n",
    "    w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} & = 0 \\\\\n",
    "    w & = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Taking the derivation of $L(w, b, \\alpha)$ over $b$:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\frac{\\partial L}{\\partial b} & = 0 \\\\\n",
    "    \\sum_{i}^{n} \\alpha_{i}y_{i} & = 0 \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Plug in $w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i}$ back to $g(\\alpha)$:\n",
    "    $$ \n",
    "    \\begin{alignat}{2}\n",
    "    g(\\alpha) \n",
    "    & = \\min_{w, b} \\frac{1}{2} ww - \\sum_{i}^{n} \\alpha_{i}(y_{i}(w x_{i} + b) - 1) \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) \n",
    "        - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 \\right) \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \n",
    "        - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) + \\sum_{i}^{n}\\alpha_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \n",
    "        - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} \n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \n",
    "        - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) \n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \n",
    "        - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\    \n",
    "    & = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\  \n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Since we know that $\\alpha_{i}y_{i} = 0$, then $b\\sum_{i}^{n} \\alpha_{i}y_{i} = 0$, and thus the final Lagrange dual function is:\n",
    "    $$ g(\\alpha) = \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) $$\n",
    "1. The Lagrange dual problem is written as:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n",
    "    \\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n",
    "    \\quad & \\alpha_{i}y_{i} = 0 \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Notice that $\\alpha_{i}y_{i} = 0$ is added as part of the constraint.\n",
    "1. Since strong duality holds for hard margin SVM and also soft margin SVM, solving dual problem has the same solution as the primal problem. The benefits of solving its dual problem are:\n",
    "    1. The Lagrange dual problem only involves $\\alpha_{i}$, but primal problem has $w$ and $b$, which are much more parameters.\n",
    "    1. The Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn't.\n",
    "\n",
    "### Solving soft margin SVM\n",
    "1. Similar as hard margin SVM, we can write Lagrangian dual function as:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    g(\\alpha, \\beta) & = \\min_{w, b} \\frac{1}{2} ww \n",
    "        - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    where a new Lagrange multiplier is introduced for the constraint $\\xi_{i} \\geq 0$.\n",
    "1. Similar as hard margin SVM, we can solve Lagrangian dual function by taking the derivatives over the $w$, $b$, and $\\xi_i$:\n",
    "    $$ \n",
    "    \\begin{alignat}{2}\n",
    "    \\frac{\\partial L}{\\partial w} = 0 & \\Rightarrow w - \\sum_{i}{n} \\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\\\\n",
    "    \\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i}^{n} \\alpha_{i}y_{i} = 0 \\\\\n",
    "    \\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    and plug the $w = \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i}$ and $C = \\alpha_{i} + \\beta_{i}$ back in $g(\\alpha, \\beta)$. \n",
    "    $$ \n",
    "    \\begin{alignat}{2}\n",
    "    g(\\alpha, \\beta) \n",
    "    & = \\min_{w, b} \\frac{1}{2} ww + C\\sum_{i}^{n}\\xi_{i} - \\sum_{i}^{n} \\alpha_{i}\\left( y_{i}(w x_{i} + b) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n}\\beta_{i}\\xi_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        - \\sum_{i}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j}^{n} \\alpha_{j}y_{j}x_{j} \\right) x_{i} + b \\right) - 1 + \\xi_{i} \\right) - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        - \\sum_{i}^{n} \\alpha_{i}y_{i}\\left( \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} + b \\right) \n",
    "        + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        - \\sum_{i}^{n} \\alpha_{i}y_{i} \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) x_{i} \n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        - \\left( \\sum_{i}^{n} \\alpha_{i}y_{i}x_{i} \\right) \\left( \\sum_{j}^{n} \\alpha_{j} y_{j} x_{j} \\right) \n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "    & = \\min_{w, b} \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        - \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j})\n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\    \n",
    "    & = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n}(\\alpha_{i} + \\beta_{i})\\xi_{i}\n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n",
    "    & = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + \\sum_{i}^{n} \\alpha_{i}\\xi_{i} + \\sum_{i}^{n} \\beta_{i}\\xi_{i}\n",
    "        + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} - \\sum_{i}^{n} \\alpha_{i} \\xi_{i} - \\sum_{i}^{n} \\beta_{i} \\xi_{i} \\\\  \n",
    "    & = \\min_{w, b} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) + b\\sum_{i}^{n} \\alpha_{i}y_{i} + \\sum_{i}^{n}\\alpha_{i} \\\\ \n",
    "    & = \\min_{w, b} \\sum_{i}^{n}\\alpha_{i}  - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\ \n",
    "    \\end{alignat}\n",
    "    $$ \n",
    "    which has exactly the same form as Lagrangian dual function of hard margin SVM. \n",
    "1. The Lagrange dual problem is written as:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n",
    "    \\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n",
    "    \\quad & \\beta_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n",
    "    \\quad & \\alpha_{i}y_{i} = 0 \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    Since we know $C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}$, the constraint $\\beta_{i} \\geq 0$ can be removed by merging into $\\alpha_{i} \\geq 0$:\n",
    "    $$\n",
    "    \\begin{alignat}{2}\n",
    "    \\max \\quad & \\sum_{i}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i}^{n}\\sum_{j}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(x_{i}x_{j}) \\\\\n",
    "    \\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots n \\\\\n",
    "    \\quad & \\alpha_{i}y_{i} = 0 \\\\\n",
    "    \\end{alignat}\n",
    "    $$\n",
    "    The only difference with Lagrange dual problem of hard margin SVM is the addition of $C \\geq \\alpha_{i}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641a433-39ef-43e2-83b8-248c38db0176",
   "metadata": {},
   "source": [
    "## Kernel trick\n",
    "***\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa02c16-dee0-4893-a074-3dcc26799e48",
   "metadata": {},
   "source": [
    "## Reference\n",
    "***\n",
    "\n",
    "1. https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/\n",
    "2. https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
