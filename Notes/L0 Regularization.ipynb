{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4404486d",
   "metadata": {},
   "source": [
    "---\n",
    "title: L0 Regularization\n",
    "date: 06/02/2022\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2507c",
   "metadata": {},
   "source": [
    "This page contains my reading notes on \n",
    "\n",
    "- [**Learning Sparse Neural Networks through $L_{0}$ Regularization**](https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/572f5d18a3943dce4e14f937ef66977a01891096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22f433b-66a3-41b0-91a0-c04796ab7167",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return min(1, max(0, x))\n",
    "\n",
    "def logit_dist():\n",
    "    u = np.random.random()\n",
    "    logit = np.log(u) - np.log(1 - u)\n",
    "    \n",
    "    return logit\n",
    "\n",
    "def binary_concrete(loc, temp):\n",
    "    logit = logit_dist()\n",
    "    bc = sigmoid((logit + loc) / temp) \n",
    "    \n",
    "    return bc\n",
    "\n",
    "def stretch_binary_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n",
    "    bc = binary_concrete(loc, temp)\n",
    "    stretch_bc = bc * (zeta - gamma) + gamma\n",
    "    \n",
    "    return stretch_bc\n",
    "\n",
    "def hard_concrete(loc, temp, gamma=-0.1, zeta=1.1):\n",
    "    stretch_bc = stretch_binary_concrete(loc, temp, gamma, zeta)\n",
    "    hc = hard_sigmoid(stretch_bc) \n",
    "\n",
    "    return hc\n",
    "    \n",
    "def plot_probability(list_samples, bins=100, **kwargs):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for samples in list_samples:\n",
    "        weights = np.ones_like(samples) / len(samples)\n",
    "        plt.hist(samples, weights=weights, bins=bins, alpha=0.5, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c332469-48d2-4c80-9f09-1c5cc67ebe60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Problem formulation\n",
    "\n",
    "Given a vector $x$ of length $n$ (matrix can also be seen as a vector by stacking up the rows/cols), the common vector norms are:\n",
    "\n",
    "- $L_{0}$ norm: \n",
    "\n",
    "    $$ \\sum_{i=1}^{n} \\mathbb{1}[x_{i} \\neq 0] $$\n",
    "    \n",
    "- $L_{1}$ norm: \n",
    "\n",
    "    $$ \\sum_{i=1}^{n} \\lvert x_{i} \\rvert $$\n",
    "    \n",
    "    which is also called ridge regularization in neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8c62a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $L_{2}$ norm:\n",
    "\n",
    "    $$ \\sum_{i=1}^{n} x_{i}^2 $$\n",
    "    \n",
    "    which is also called lasso regularization in neural network.\n",
    "    \n",
    "- $L_{\\infty}$ norm: \n",
    "\n",
    "    $$ \\max_{i=1}^{n} \\lvert x_i \\rvert $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fb1ad-71fc-4ce8-b507-625427caeeee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The normal way to prune the edges of the neural network is to use $L_{1}$ or $L_{2}$ regularization to drive weights to near 0 (not exactly 0), and then directly set all weights that are less than threshold to 0.\n",
    "\n",
    "- $L_{0}$ is not used because the operation of counting the number of 0s is not differentiable.\n",
    "\n",
    "- However, $L_{0}$ regularization is still desired because it won't affect the magnitude of the weights in the pruning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfa507-c735-4f1f-ba76-520bd32e845a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## General recipe of $L_{0}$ regularization\n",
    "\n",
    "The loss function used to train a neural network with $L_{0}$ regularization is:\n",
    "\n",
    "$$ \\mathcal{L}(f(x, \\theta), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[\\theta_{i} \\neq 0] $$\n",
    "    \n",
    "where \n",
    "\n",
    "- $\\mathcal{L}$ is a standard loss function (cross-entropy loss, softmax)\n",
    "- $\\theta$ are the parameters in the network\n",
    "- $x, y$ are training instances \n",
    "- $\\lambda$ is a hyper-parameter that balance loss and the regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e03af-0d37-4ce4-8ed8-ddb7480c3a81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we attach a **trainable** binary random variable $z_{i}$ to each element of the model parameter $\\theta_{i}$, then the weights used in the feed-forward operation of the neural network can be replaced by $\\theta \\odot z$. The loss function then becomes:\n",
    "\n",
    "$$ \\mathcal{L}(f(x, \\theta \\odot z), y) + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\mathbb{1}[z_{i} \\neq 0]$$\n",
    "    \n",
    "where \n",
    "\n",
    "- $z \\in \\{0, 1\\}^{\\lvert \\theta \\rvert}$ is **randomly sampled in each forward propagation** according to some distribution.\n",
    "- $\\odot$ corresponds to the elementwise product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb09a2-9a2c-4eb6-9d99-a4778db519d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we assume each $z_{i}$ as a binary random variable with a **Bernoulli distribution** that has a parameter $\\pi_{i}$, i.e. $z_{i} = \\mathrm{Bern}(\\pi_{i})$, then the loss function becomes:\n",
    "\n",
    "$$ \\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} $$\n",
    "\n",
    "where $\\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} [\\cdot]$ gives an expectation value of a function that has a Bernoulli distribution $z$ as the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11052d3a-653b-4fd1-a509-abc814614a46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The reformulation of the above loss function can be established because\n",
    "\n",
    "1. Since the minimum of a function is upper bounded by the expectation of the function, minimizing $\\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big]$ is the same as minimizing the upper bound of $\\mathcal{L}(f(x, \\theta \\odot z), y)$.\n",
    "    \n",
    "1. According to the definition of the Bernoulli distribution, $\\pi$ gives the probability of $z$ being 1 (non zero). Thus, **minimizing $\\pi$ is to increase the probability of $z$ being 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0d6a0-2b4c-46d0-ae15-2e77c6bb0162",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the equation above, \n",
    "\n",
    "- $\\pi_{i}$ is a parameter that we want to be learned using gradient descent. \n",
    "\n",
    "- Thus, the second term $\\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i}$ can be directly minimized to regularize $\\pi$ because gradient of the second term w.r.t to $pi_{i}$ can be easily calculated.\n",
    "\n",
    "- However, the first term $\\mathbb{E}_{z=\\mathrm{Bern}(\\pi)} \\big[ \\mathcal{L}(f(x, \\theta \\odot z), y) \\big]$ is still problematic because $z$ as a categorical random variable cannot be differentiated with respect to $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996de73b-7dae-45fb-8b2f-c16a803ea4c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Hard concrete distribution\n",
    "\n",
    "### Binary Concrete distribution\n",
    "\n",
    "The binary concrete distribution can be seen as a **continuous** approximation of the Bernoulli distribution. It has 2 parameters:\n",
    "\n",
    "- location $\\alpha$: similar to the probability parameter of the Bernoulli distribution. \n",
    "\n",
    "- temperature $\\beta$: it controls how similar the binary concrete distribution is with the Bernoulli distribution.\n",
    "\n",
    "Using the the reparametrization trick, the binary concrete distribution $s$ can be represented as:\n",
    "\n",
    "$$ s = \\operatorname{sigmoid} \\left( \\frac{\\alpha + l}{\\beta} \\right) $$\n",
    "\n",
    "where $l$ is a sample from the logistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1101fd3-89db-47ea-a11d-2301b65c38f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c609ff46804483cbbc078626eeecb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100000, description='size', max=300000, min=-100000), Output()), _dom_clâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_logit(size=100000, **kwargs):\n",
    "    logit_samples = [logit_dist() for _ in range(size)]\n",
    "    plot_probability([logit_samples])\n",
    "    \n",
    "interact(plot_logit);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c394c3-506d-4e16-aa41-6776260ee524",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73042ad6690425c87a36d14a2d92437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100000, description='size', max=300000, min=-100000), IntSlider(value=0,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_bc(size=100000, **kwargs):\n",
    "    ber_samples = np.random.binomial(1, 0.5, size=size)\n",
    "    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n",
    "    plot_probability([ber_samples, bc_samples])\n",
    "    \n",
    "interact(plot_bc, loc=(-3, 3), temp=(0.001, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba029272-75c4-42ec-9fa1-dbff86dc5c04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### From binary concrete to hard concrete \n",
    "\n",
    "We cannot use $s$ (binary concrete distribution) to directly replace $z$ (Bernoulli distribution)\n",
    "- The range of $s$ is $(0, 1)$ and **never touches 0 or 1**.\n",
    "- However, we want $z$ to be either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f18cc-3070-4858-86eb-a0ded7f984b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A simple trick to solve this problem is \n",
    "\n",
    "1. First \"stretch\" the binary concrete distribution from interval $(0, 1)$ to interval $(\\gamma, \\zeta)$ with $\\gamma < 0$ and $\\zeta > 1$\n",
    "    \n",
    "    $$ \\bar{s} = s(\\zeta - \\gamma) + \\gamma $$\n",
    "    \n",
    "1. Then clip the stretch binary concrete distribution into the range $[0, 1]$\n",
    "\n",
    "    $$ \\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1) $$ \n",
    "    \n",
    "    where \n",
    "    \n",
    "    - $\\mathrm{clip}(x, \\mathrm{min}, \\mathrm{max})$ means to clip $x$ between the range $[\\mathrm{min}, \\mathrm{max}]$. \n",
    "    \n",
    "    - **$\\bar{z}$ is a random variable that follows the hard concrete distribution and it can be used to replace $z$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fade06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb32e219e9ed4ec3bf2b3c3cb4efa4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100000, description='size', max=300000, min=-100000), IntSlider(value=0,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_sbc(size=100000, **kwargs):\n",
    "    bc_samples = [binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n",
    "    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n",
    "    plot_probability([bc_samples, sbc_samples])\n",
    "    \n",
    "interact(plot_sbc, loc=(-3, 3), temp=(0.001, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f7b367",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aaf3b071be48fcb4546c3430a449d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100000, description='size', max=300000, min=-100000), IntSlider(value=0,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hc(size=100000, **kwargs):\n",
    "    hc_samples = [hard_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n",
    "    sbc_samples = [stretch_binary_concrete(kwargs['loc'], kwargs['temp']) for _ in range(size)]\n",
    "    plot_probability([hc_samples, sbc_samples])\n",
    "    \n",
    "interact(plot_hc, loc=(-3, 3), temp=(0.001, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4a932-f6a5-4604-acd9-500bb6f1b9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Also, we cannot use $\\alpha$ (location parameter of binary concrete distribution) to replace $\\pi$ (probability parameter of Bernoulli distribution) in the regularization term of the loss function. \n",
    "- Remember that the regularization term above measures the sum of the probabilities of the $z$ (Bernoulli distribution) being non-zero.\n",
    "\n",
    "    $$ \\sum_{i=1}^{\\lvert \\theta \\rvert} \\pi_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4072",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Measuring the probability of $\\bar{z}$ of being non-zero is the same as **measuring the probability of $\\bar{s}$ being positive**.\n",
    "\n",
    "- Because all negative values of $\\bar{s}$ is clipped to be 0.\n",
    "    \n",
    "- Since we know that the total probability of a random variable being all values is 1, the probability of $\\bar{s}$ being positive is written as: \n",
    "\n",
    "    $$ \\textrm{q}(\\bar{s} > 0 | \\phi) = 1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi) $$\n",
    "\n",
    "    where \n",
    "\n",
    "    - $\\textrm{Q}(\\cdot)$ is the cumulative density function (CDF) \n",
    "    - $\\mathrm{Q}(s \\leq 0 | \\phi)$ gives the probability of $s$ being negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4b6b2-791d-4252-86ab-3ee44e0a8fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Thus, the loss function above can be rewritten using the hard concrete distribution by:\n",
    "\n",
    "$$ \\mathbb{E}_{\\bar{s}} \\big[ \\mathcal{L}(f(x, \\theta \\odot \\bar{z}), y) \\big] + \\lambda \\sum_{i=1}^{\\lvert \\theta \\rvert} (1 - \\mathrm{Q}(\\bar{s} \\leq 0 | \\phi)) $$\n",
    "\n",
    "**The loss function is \"fully\" differentiable with respect to $\\alpha$**\n",
    "\n",
    "- $\\bar{z} = \\mathrm{clip}(\\bar{s}, 0, 1)$ and $\\bar{s}$ is a differentiable function with respect to $\\alpha$.\n",
    "\n",
    "- $\\mathrm{Q}(\\bar{s} \\leq 0 | \\phi)$ can be written as a differentiable function with respect to $\\alpha$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
