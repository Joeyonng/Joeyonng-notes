{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63a62a0-3fd6-4f94-8d57-e74d733330ad",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Support Vector Machine (SVM)\n",
    "---\n",
    "\n",
    "1. The objective of support vector machine is to find a hyperplane in a N dimensional space that separates two classes. Thus similar to linear regression, SVM also contains a weight vector and a bias as parameters.\n",
    "1. To find the correct parameters, we first need to assume the training instances are linearly separable. Then an convex optimization problem is solved to find the weights and bias such that the hyperplane has the maximum distances from the support vectors. The support vectors are the training instances that are closest to the hyperplane.\n",
    "1. If the training set contains noise points that make them linearly non-separable, we can add slack variable for each training instance to the constraints of the optimization problem so that it permits some training instances to be on the other side of the hyperplane. Basically large slack variables allow more misclassified training instances and the sum of them is added to the target function to be minimized. \n",
    "1. A hyperparameter C can be used to determine how important the slack variables are. Setting C to be 0 means that we want the SVM to perfectly separate two classes in the training set while a suitable value means that we allow some errors in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a245c-a393-4d38-a81d-c08bb22af6b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "### Convex Optimization\n",
    "\n",
    "#### Lagrangian\n",
    "\n",
    "Given a (possibly non-convex) minimization primal problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{x} \\quad & f(x) \\\\\n",
    "\\text{s.t. } \\quad & g_{i}(x) \\leq 0, \\quad i = 1, \\dots, n \\\\\n",
    "\\quad & h_{i}(x) = 0, \\quad j = 1, \\dots, m \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $x$ here is used to represent all the input variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cfc92-ecf2-4442-a266-ac6cbd727cf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **Lagrangian** of the primal problem above is defined as:\n",
    "\n",
    "$$ \\mathcal{L}(x, \\lambda, \\nu) = f(x) + \\sum_{i=1}^{n} \\lambda_{i} g_{i}(x) + \\sum_{j=1}^{m} \\nu_{j} h_{i}(x) $$\n",
    "\n",
    "where $\\{ \\lambda_{1}, \\dots, \\lambda_{n} \\}$ and $\\{ \\nu_{1}, \\dots, \\nu_{m} \\}$ are two sets of new variables called Lagrangian multipliers.\n",
    "\n",
    "The Lagrangian can be used to convert the primal problem with constraints to the following **unconstrained** problem\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\min_{x} \\mathcal{P}(x) = \\min_{x} \\quad \\max_{\\lambda, \\nu} \\quad & \\mathcal{L}(x, \\lambda, \\nu) \\\\\n",
    "\\text{s.t. } \\quad & \\lambda_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the values of $\\{ \\lambda_{1}, \\dots, \\lambda_{n} \\}$ and $\\{ \\nu_{1}, \\dots, \\nu_{m} \\}$ can be freely chosen to maximize the Lagrangian, $\\mathcal{P}(x)$ is the same as $f(x)$ when $x$ satisfies the constraints in the original primal problem. Otherwise, $\\mathcal{P}(x)$ becomes infinity.\n",
    "\n",
    "$$ \n",
    "\\mathcal{P}(x) = \n",
    "\\begin{cases}\n",
    "\\begin{align}\n",
    "& f(x) && \\text{if } g_{i}(x) \\leq 0, i = 1, \\dots, n \\text{ and } h_{i}(x) = 0, j = 1, \\dots, m \\\\\n",
    "& \\infty && \\text{otherwise} \\\\\n",
    "\\end{align}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02267e3-6b18-4daf-a098-f75f958a0e56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Duality\n",
    "\n",
    "We can create a new optimization problem by reverting the order of $\\min$ and $\\max$ in the Lagrangian unconstrained optimization problem. \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\lambda, \\nu} \\mathcal{D}(\\lambda, \\nu) = \\max_{\\lambda, \\nu} \\quad & \\min_{x} \\quad \\mathcal{L}(x, \\lambda, \\nu) \\\\\n",
    "\\text{s.t. } \\quad & \\lambda_{i} \\geq 0, i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $ \\mathcal{D}(\\lambda, \\nu) = \\min_{x} \\mathcal{L}(x, \\lambda, \\nu) $ is the **Lagrange dual function**.\n",
    "\n",
    "- $\\max_{\\lambda, \\nu} \\mathcal{D}(\\lambda, \\nu)$ is the **Lagrange dual problem**.\n",
    "\n",
    "\n",
    "The properties of the dual problem:\n",
    "\n",
    "1. The dual problem is always convex even if the primal problem is not convex.\n",
    "1. For any primal problem and its dual problem, the weak duality always holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654874d-1ea7-4188-a180-3dce667ff97a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Weak duality\n",
    "\n",
    "**Weak duality** states that the optimal value of the primal problem is greater or equal to the optimal value of its dual problem.\n",
    "\n",
    "Weak duality holds for any primal problem and its dual problem, even if the primal problem is not convex. \n",
    "\n",
    "#### Strong duality\n",
    "\n",
    "**Strong duality** states that the optimal value of the primal problem is the same as the optimal value of its dual problem.\n",
    "\n",
    "For convex primal problems, the strong duality holds if **Slater's conditions** holds. Slater's conditions test whether there exists an $x$ that meet all the constraints of the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd61bf-dfd6-459a-a839-71e1899a122f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Karush-Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "Given the Lagrange dual problem stated above, the KKT conditions are:\n",
    "\n",
    "1. Stationarity condition: \n",
    "\n",
    "    $$ \\frac{\\partial}{\\partial x}  \\mathcal{L}(x, \\lambda, \\nu) = 0 $$\n",
    "    \n",
    "1. Complementary slackness condition:\n",
    "\n",
    "    $$ \\lambda_{i} g_{i}(x) = 0, \\quad i = 1, \\dots, n $$\n",
    "    \n",
    "1. Primal feasibility condition:\n",
    "\n",
    "    $$ g_{i}(x) \\leq 0, \\quad i = 1, \\dots, n $$\n",
    "    \n",
    "    $$ h_{i}(x) = 0, \\quad j = 1, \\dots, m $$\n",
    "    \n",
    "1. Dual feasibility condition:\n",
    "\n",
    "    $$ \\lambda_{i} \\geq 0, \\quad i = 1, \\dots, n $$\n",
    "        \n",
    "If a strong duality holds, the $x^{*}$ and $\\lambda^{*}, \\nu^{*}$ are primal and dual solutions if and only if $x^{*}$ and $\\lambda^{*}, \\nu^{*}$ satisfy the KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59537096-fdd6-4ae3-ad50-ae7905cfafe6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Hyperplane and margin\n",
    "---\n",
    "\n",
    "### Hyperplane\n",
    "\n",
    "In the following context, a hyperplane in a $d$-dimensional space is represented by \n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $$\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^{d}$ and $b \\in \\mathbb{R}$ are constants defining the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d1016",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Given a point $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$,\n",
    "\n",
    "    - $\\mathbf{x}_{i}$ is **on** the hyperplane if $ \\mathbf{w} \\cdot \\mathbf{x}_{i} + b = 0 $.\n",
    "    \n",
    "    - $\\mathbf{x}_{i}$ is **above** the hyperplane if $ \\mathbf{w} \\cdot \\mathbf{x}_{i} + b > 0 $.\n",
    "    \n",
    "    - $\\mathbf{x}_{i}$ is **below** the hyperplane if $ \\mathbf{w} \\cdot \\mathbf{x}_{i} + b < 0 $.\n",
    "\n",
    "1. The hyperplane doesn't change if both $\\mathbf{w}$ and $b$ are multiplied by the same scaling factor.\n",
    "\n",
    "    $$ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\Leftrightarrow k \\mathbf{w} \\cdot \\mathbf{x} + k b = 0 $$\n",
    "    \n",
    "    where $k$ is an arbitrary non-zero scaling factor.\n",
    "\n",
    "1. The distance of a point $\\mathbf{x}_{i}$ to the hyperplane defined by $\\mathbf{w}$ and $b$ is the perpendicular distance of the point to the hyperplane:\n",
    "\n",
    "    $$ d(\\mathbf{x}_{i}) = \\frac{\\lvert \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\rvert}{\\lVert \\mathbf{w} \\rVert} $$\n",
    "    \n",
    "    where $\\lVert \\mathbf{w} \\rVert$ is the $L_{2}$ norm of $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e04a2-37c8-49f8-9c41-b2ef8ce916ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Margin\n",
    "\n",
    "We define **geometric margin** of an instance $\\mathbf{x}_{i}$ and label $y_{i}$ with respect to a hyperplane defined by $\\mathbf{w}$ and $b$ to be\n",
    "\n",
    "$$ \\gamma_{i} = \\frac{y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b )}{\\lVert \\mathbf{w} \\rVert} $$\n",
    "\n",
    "- Due to the definition of label to be $\\{ -1, 1 \\}$ instead of $\\{ 0, 1 \\}$, the sign of the geometric margin indicates whether the instance is classified correctly by the hyperplane. The geometric margin of $\\mathbf{x}_{i}$ is positive only if the $\\mathbf{x}_{i}$ is on the correct side of the hyperplane. \n",
    "\n",
    "- Note the similarity between the definition of geometric margin and the definition of the distance of the point to the hyperplane. The magnitude of the geometric margin is the distance between the instance and the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db915fed-77b9-4cb6-bf6e-237a0fcbb9d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Hard margin SVM (SVM without slacks)\n",
    "---\n",
    "\n",
    "### Formulation\n",
    "\n",
    "Given a dataset with $n$ instances $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ and $n$ labels $y_{i} \\in \\{-1, 1\\}$, we assume that there exists at least a hyperplane that can perfectly separates all training instances. That is, \n",
    "\n",
    "- All instances with label 1 are above the hyperplane.\n",
    "\n",
    "- All instances with label -1 are below the hyperplane.\n",
    "\n",
    "In case there are more than one hyperplanes that can perfectly separates the training instances, a hard margin SVM model will choose the hyperplane that has the **largest geometric margin to the training instances that are closest (minimum geometric margin) to the hyperplane**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90dac9-e020-4fd8-a90b-718c966d56a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}, b} \\quad & \\hat{\\gamma} \\\\\n",
    "\\text{s.t. } \\quad & \\gamma_{i} \\geq \\hat{\\gamma}, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\gamma}$ is the minimum geometric margin that the hyperplane has with respect to all the training instances. \n",
    "\n",
    "We can get the following convex optimization problem by simplifying the optimization problem above:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^{2} \\\\\n",
    "\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121ff2e-0fd9-42e7-bd33-72faf7aa179d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: derivation of the SVM primal problem\n",
    ":class: dropdown\n",
    "\n",
    "Expand the definition of the geometric margin in the original optimization problem:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}, b} \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert} \\\\\n",
    "\\text{s.t. } \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b )}{\\lVert \\mathbf{w} \\rVert} \\geq \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert}, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{x}}$ represents the instance that achieves the minimum geometric margin to the hyperplane. \n",
    "\n",
    "Since $\\lVert \\mathbf{w} \\rVert$ is non-negative, we can multiply $\\lVert \\mathbf{w} \\rVert$ on both sides of the constraint to get\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}, b} \\quad & \\frac{y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )}{\\lVert \\mathbf{w} \\rVert} \\\\\n",
    "\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b ), \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the value of $\\geq y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b )$ can be arbitrarily scaled up or down by multiplying with a scaling factor, we can introduce a implicit constraint that $y_{i} ( \\mathbf{w} \\cdot \\hat{\\mathbf{x}} + b ) = 1$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}} \\quad & \\frac{1}{\\lVert \\mathbf{w} \\rVert} \\\\\n",
    "\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To help simplifying in the solving process, maximizing $\\frac{1}{\\lVert \\mathbf{w} \\rVert}$ is the same as minimizing $\\frac{\\lVert \\mathbf{w} \\rVert^{2}}{2}$:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\max_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert \\\\\n",
    "\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037996a-3e4c-463f-98de-7686edebd0fc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Solving the above optimization problem will give us two parallel hyperplanes ($\\mathbf{w} x + b = 1$ and $\\mathbf{w} x + b = -1$) that strictly separate the positive and negative training instances and at the same time have the maximum gap in between.\n",
    "\n",
    "1. The objective maximizes the squared distance between the parallel hyperplanes by minimizing the multiplicative inverse of the squared distance between the parallel hyperplanes, which is defined as \n",
    "\n",
    "    $$ \\frac{\\lvert b_{2} - b_{1} \\rvert}{\\lVert \\mathbf{w} \\rVert} = \\frac{\\lvert (b + 1) - (b - 1) \\rvert}{\\lVert \\mathbf{w} \\rVert} = \\frac{2}{\\lVert \\mathbf{w} \\rVert} $$\n",
    "    \n",
    "1. The constraints specify that the instances must be on the correct side of the two hyperplanes:\n",
    "    \n",
    "    $$\n",
    "    y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) \\geq 1 \\Leftrightarrow\n",
    "    \\begin{cases} \n",
    "    \\begin{align}\n",
    "    & \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\geq 1 && \\text{if } y_{i} = 1 \\\\\n",
    "    & \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\leq -1 && \\text{if } y_{i} = -1 \\\\\n",
    "    \\end{align}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffe1ec-411c-4cf0-8f4b-401a81b25ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving hard margin SVM\n",
    "Rewrite the primal problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} \\\\\n",
    "\\text{s.t. } \\quad & -(y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i} + b) - 1) \\leq 0, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive the Lagrangian from the primal problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}, b, \\alpha) & = f(\\mathbf{w}, b) + \\sum_{i=1}^{n} \\alpha_{i} g_{i}(\\mathbf{w}, b) \\\\\n",
    "& = \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ffd15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we can write the dual function:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathcal{D}(\\alpha) & = \\min_{\\mathbf{w}, b} L(\\mathbf{w}, b, \\alpha) \\\\\n",
    "& = \\min_{\\mathbf{w}, b} \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c164adc-cbaf-443d-81c1-9cd580a742c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "1. Solving the dual function by taking the derivative of $L(\\mathbf{w}, b, \\alpha)$ over $\\mathbf{w}$ and $b$:\n",
    "\n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    & \\frac{\\partial L}{\\partial \\mathbf{w}} = 0 \\Rightarrow \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_{i}y_{i} \\mathbf{x}_{i} = 0 \\Rightarrow \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i}\\mathbf{x}_{i} \\\\\n",
    "    & \\frac{\\partial L}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i}y_{i} = 0 \n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "2. Plug in $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i}\\mathbf{x}_{i}$ back and simplify $\\mathcal{D}(\\alpha)$:\n",
    "    \n",
    "    $$ \n",
    "    \\begin{align}\n",
    "    \\mathcal{D}(\\alpha) \n",
    "    & = b \\sum_{i=1}^{n} \\alpha_{i} y_{i} + \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} (\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}) \\\\\n",
    "    & = \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    \\quad & [\\text{since } \\sum_{i=1}^{n} \\alpha_{i}y_{i} = 0] \\\\\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff63b6-d92e-45b1-a162-5a129007ebab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: simplify $\\mathcal{D}(\\alpha)$ for hard margin SVM\n",
    ":class: dropdown\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathcal{D}(\\alpha) \n",
    "& = \\min_{\\mathbf{w}, b} \n",
    "    \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} ( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 ) \\\\\n",
    "& = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) - 1 \\right) \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} \n",
    "    + b\\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    - \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n",
    "& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\\n",
    "& = b \\sum_{i=1}^{n} \\alpha_{i} y_{i} + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a2f4a-b744-4a85-9f11-a070e22ad769",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Lagrange dual problem is written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\\n",
    "\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the primal problem is a convex function and there definitely exists at least one solution to the primal problem, Slater's condition proves that strong duality holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805b8de-9317-483a-95f3-41104d13089b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since solving the dual problem is the same as solving the primal problem, the benefits of solving the dual problem are:\n",
    "\n",
    "1. The Lagrange dual problem only involves $\\alpha_{i}$ and most of them are 0, but primal problem has $\\mathbf{w}$ and $b$, which are much more parameters.\n",
    "\n",
    "1. The Lagrange dual problem allows application of kernel trick in the computation process, but the primal problem doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c0548-74bc-4bb0-9aa9-994eb636c2e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Soft margin SVM (SVM with slacks)\n",
    "---\n",
    "\n",
    "### Formulation\n",
    "\n",
    "In case there is no way that the instances can be linearly separated, we can use slack variables in the formulation to tolerate a small number of non-separable training instances. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}} \\quad & \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^{2} + C \\sum_{i=1}^{n} \\xi_{i} \\\\\n",
    "\\text{s.t. } \\quad & y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) \\geq 1 - \\xi_{i}, \\quad i = 1, \\dots, n \\\\\n",
    "\\quad & \\xi_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\xi_{i}$ is the slack variable for the instance $\\mathbf{x}_{i}$ and $C$ is a hyperparameter that penalizes the misclassification of $\\mathbf{x}_{i}$. \n",
    "\n",
    "1. If $\\xi_{i}$ is nonzero for $\\mathbf{x}_{i}$, it means that $\\mathbf{x}_{i}$ is on the misclassified side of $\\mathbf{w} \\cdot \\mathbf{x}_{i} + b = 1$ (or $\\mathbf{w} \\cdot \\mathbf{x}_{i} + b = -1$) and the distance is $\\xi_{i}$. \n",
    "1. If $C = 0$, $\\xi_{i}$ can be arbitrary large for each $\\mathbf{x}_{i}$. If $C \\to \\inf$, it is the same as hard margin SVM because any misclassification can induce infinite loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154441b-7b78-416d-a183-68c7258b9193",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving soft margin SVM\n",
    "Similar as hard margin SVM, we can write Lagrangian dual function as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{D}(\\alpha, \\beta) & = \\min_{\\mathbf{w}, b} \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 + \\xi_{i} \\right) - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a503de-182a-49ce-97dd-cb41b37e2ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, we can solve Lagrangian dual function by taking the derivatives over the $\\mathbf{w}$, $b$, and $\\xi_i$:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = 0 & \\Rightarrow \\mathbf{w} - \\sum_{i=1}{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} = 0 \\Rightarrow \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\\\\n",
    "\\frac{\\partial L}{\\partial b} = 0 & \\Rightarrow \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_{i}} = 0 & \\Rightarrow C - \\alpha_{i} - \\beta_{i} = 0 \\Rightarrow C = \\alpha_{i} + \\beta_{i} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Plug the $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i}$ and $C = \\alpha_{i} + \\beta_{i}$ back and simplify $\\mathcal{D}(\\alpha, \\beta)$. \n",
    "\n",
    "$$ \\mathcal{D}(\\alpha, \\beta) = \\sum_{i=1}^{n} \\alpha_{i}  - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) $$\n",
    "\n",
    "which has exactly the same form as Lagrangian dual function of hard margin SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54206dbd-54c6-479a-9517-79e20dd528e6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: simplify $\\mathcal{D}(\\alpha)$ for soft margin SVM\n",
    ":class: dropdown\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathcal{D}(\\alpha, \\beta) \n",
    "& = \\min_{\\mathbf{w}, b} \n",
    "    \\frac{1}{2} \\mathbf{\\mathbf{w}} \\mathbf{\\mathbf{w}} \n",
    "    + C \\sum_{i=1}^{n} \\xi_{i} - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} ( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b ) - 1 + \\xi_{i} \\right) \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "& = \\frac{1}{2} \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\left( y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) - 1 + \\xi_{i} \\right) \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} + b \\right) \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i}) \\xi_{i}\n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \\cdot \\mathbf{x}_{i} \n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i}y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n",
    "    - \\left( \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\left( \\sum_{j=1}^{n} \\alpha_{j} y_{j} \\mathbf{x}_{j} \\right) \n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n",
    "    - \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} )\n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i}y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\    \n",
    "& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i}\\mathbf{x}_{j} )\n",
    "    + \\sum_{i=1}^{n} ( \\alpha_{i} + \\beta_{i} ) \\xi_{i}\n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\  \n",
    "& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    + \\sum_{i=1}^{n} \\beta_{i} \\xi_{i}\n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \n",
    "    - \\sum_{i=1}^{n} \\alpha_{i} \\xi_{i} \n",
    "    - \\sum_{i=1}^{n} \\beta_{i} \\xi_{i} \\\\  \n",
    "& = - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \n",
    "    + b \\sum_{i=1}^{n} \\alpha_{i} y_{i} \n",
    "    + \\sum_{i=1}^{n} \\alpha_{i} \\\\ \n",
    "& = \\sum_{i=1}^{n}\\alpha_{i} \n",
    "    - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} ( \\mathbf{x}_{i} \\cdot \\mathbf{x}_{j} ) \\\\ \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584381a9-0867-4823-9e7e-f7708b47f614",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Lagrange dual problem is written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) \\\\\n",
    "\\text{s.t. } \\quad & \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "& \\beta_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since we know $C = \\alpha_{i} + \\beta_{i} \\Rightarrow \\alpha_{i} = C - \\beta_{i}$, the constraint $\\beta_{i} \\geq 0$ can be removed by merging into $\\alpha_{i} \\geq 0$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i}\\alpha_{j}y_{i}y_{j}(\\mathbf{x}_{i}\\mathbf{x}_{j}) \\\\\n",
    "\\text{s.t. } \\quad & C \\geq \\alpha_{i} \\geq 0, \\quad i = 1, \\dots, n \\\\\n",
    "& \\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The only difference with Lagrange dual problem of hard margin SVM is the addition of $C \\geq \\alpha_{i}, \\quad i = 1, \\dots, n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f14da8-da49-4a41-b4e3-a4f8814165c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference of SVM\n",
    "---\n",
    "\n",
    "Solving the SVM dual problem will generate an optimal Lagrange multiplier $\\alpha_{i}^{*}$ for each training instance $\\mathbf{x}_{i}$. All $\\alpha_{i}^{*}$'s, $\\mathbf{x}_{i}$'s and $y_{i}$'s can be used to calculate the optimal $\\mathbf{w}^{*}$ and $b^{*}$ that define the hyperplane as the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a18a5-27e5-4338-bfe6-c60c05911f01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Support vectors\n",
    "\n",
    "**Support vectors** are the training instances whose optimal Lagrange multipliers $\\alpha_{i}^{*}$'s are positive.\n",
    "\n",
    "For hard margin SVM:\n",
    "\n",
    "- All support vectors are the training instances that are the closest to the decision hyperplane $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 0$.\n",
    "\n",
    "    - All positive support vectors are on the hyperplane $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 1$.\n",
    "    \n",
    "    - All negative support vectors are on the hyperplane $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = -1$.\n",
    "    \n",
    "- While there are some training instances on the hyperplanes $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1$ have $\\alpha_{i}^{*} = 0$, **the training instances that are NOT on the hyperplanes must have  $\\alpha_{i}^{*} = 0$**.\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) > 1 \\Rightarrow \\alpha_{i}^{*} = 0 $$\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow \\alpha_{i}^{*} \\geq 0 $$\n",
    "    \n",
    "    Note that there won't be any training instances that have $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) < 1$ because of the constraints specified in the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44169c-f877-42f3-99c9-64fe99284e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    ":::{admonition} Proof: the values of $\\alpha_{i}^{*}$'s for the support vectors in the hard margin SVM\n",
    "\n",
    "Strong duality implies that the optimal solutions $\\alpha^{*}$, $\\mathbf{w}^{*}$ and $b^{*}$ meet the KKT conditions. The complementary slackness condition states that \n",
    "\n",
    "$$ \\alpha_{i} g_{i}(x) = 0 \\Rightarrow - \\alpha_{i} ( y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 ) = 0, \\quad i = 1, \\dots, n $$\n",
    "\n",
    "- $\\alpha_{i}^{*}$ must be 0 if $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) > 1$\n",
    "\n",
    "- $\\alpha_{i}^{*}$ can be non-zero if $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ff95e-8aef-46f1-90ed-6245a3287b46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For soft margin SVM,\n",
    "\n",
    "- All support vectors satisfy the constraint:\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 - \\xi_{i}, \\quad i \\in \\mathcal{S} $$\n",
    "    \n",
    "    where $\\mathcal{S} = \\{ i \\mid \\alpha_{i}^{*} > 0 \\}$.\n",
    "\n",
    "    - All positive support vectors are **either on or below** the hyperplane $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = 1$.\n",
    "    \n",
    "    - All negative support vectors are **either on or above** the hyperplane $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = -1$.\n",
    "    \n",
    "-  Again the training instances on the hyperplanes $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1$ can have $0 \\leq \\alpha_{i}^{*} \\leq C$, but the training instances that are NOT on the hyperplanes must have  $\\alpha_{i}^{*} = 0$ or  $\\alpha_{i}^{*} = C$.\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow 0 \\leq \\alpha_{i}^{*} \\leq 0 $$\n",
    "    \n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) > 1 \\Rightarrow \\alpha_{i}^{*} = 0 $$\n",
    "    \n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) < 1 \\Rightarrow \\alpha_{i}^{*} = C $$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aabf9-3362-486d-b2e6-a489e31edec4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    ":::{admonition} Proof: the values of $\\alpha_{i}^{*}$'s for the support vectors in the soft margin SVM\n",
    "\n",
    "Strong duality implies that the optimal solutions $\\alpha^{*}$, $\\mathbf{w}^{*}$ and $b^{*}$ meet the KKT conditions. The complementary slackness condition states that \n",
    "\n",
    "$$ - \\alpha_{i}^{*} ( y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 + \\xi_{i} ) = 0, \\quad i = 1, \\dots, n $$\n",
    "\n",
    "$$ - \\beta_{i} \\xi_{i} = 0, \\quad i = 1, \\dots, n $$\n",
    "\n",
    "- If $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1$, $\\alpha_{i}^{*}$ is a non-negative number between $0$ and $C$.\n",
    "    \n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) = 1 \\Rightarrow 0 \\leq \\alpha_{i}^{*} \\leq 0 $$\n",
    "    \n",
    "- If $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) > 1$, $\\alpha_{i}^{*}$ must be $0$,\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) > 1 \\Rightarrow \\alpha_{i}^{*} = 0 $$\n",
    "    \n",
    "    which follows by the equation 1 and the constraint $\\xi_{i} \\geq 0$ in the primal problem.\n",
    "\n",
    "- If $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) < 1$, $\\alpha_{i}^{*}$ must be $C$,\n",
    "\n",
    "    $$ y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) < 1 \\Rightarrow \\alpha_{i}^{*} = C $$,\n",
    "    \n",
    "    which is explained as follows:\n",
    "    \n",
    "    1. Since the constraint  $y_{i} ( \\mathbf{w}^{*} \\cdot \\mathbf{x}_{i} + b^{*} ) - 1 + \\xi_{i} \\geq 0$ in the primal problem, $\\xi_{i} > 0$. \n",
    "    1. Since the $\\beta_{i} \\xi_{i} = 0$ from the equation 2, $\\beta_{i} = 0$.\n",
    "    1. Since we know that $\\alpha_{i}^{*} + \\beta_{i} = C$, $\\alpha_{i}^{*} = C$.\n",
    "    \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93fe74-0b8a-4bb6-b36a-df3b40c780df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Calculate $\\mathbf{w}^{*}$ and $b^{*}$ using $\\alpha_{i}^{*}$'s\n",
    "\n",
    "In solving both hard and soft margin SVM, we have\n",
    "\n",
    "$$ \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i} $$\n",
    "\n",
    "Since only $\\alpha_{i}^{*}$'s for the support vectors are non-zero, we can derive $\\mathbf{w}^{*}$ by:\n",
    "\n",
    "$$ \\mathbf{w}^{*} = \\sum_{i \\in \\mathcal{S}} \\alpha_{i}^{*} y_{i} \\mathbf{x}_{i} $$\n",
    "\n",
    "where $\\mathcal{S} = \\{ i \\mid \\alpha_{i}^{*} > 0 \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b584d82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To calculate $b^{*}$, we can select any training instance $\\hat{\\mathbf{x}}$ with label $\\hat{y}$ that is on the hyperplanes $\\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} = \\pm 1$. Thus, we have \n",
    "\n",
    "$$ \n",
    "\\begin{align} \n",
    "\\hat{y} ( \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} ) & = 1 \\\\\n",
    "\\hat{y}^{2} ( \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} ) & = \\hat{y} & [\\text{multiply both sides by } \\hat{y}] \\\\\n",
    "\\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}} + b^{*} & = \\hat{y} & [\\hat{y}^{2} = 1] \\\\\n",
    "b^{*} & = \\hat{y} - \\mathbf{w}^{*} \\cdot \\hat{\\mathbf{x}}  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9c9f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming $b^{*}$ has been calculated, we can express the inference of a SVM model as follows: \n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\operatorname{svm}(\\mathbf{x}) & = \\mathbf{w}^{*} \\cdot \\mathbf{x} + b^{*} \\\\\n",
    "& = \\left( \\sum_{i \\in \\mathcal{S}} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\right) \\cdot \\mathbf{x} + b^{*} \\\\\n",
    "& = \\sum_{i \\in \\mathcal{S}} \\alpha_{i} y_{i} \\mathbf{x}_{i} \\cdot \\mathbf{x} + b^{*} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641a433-39ef-43e2-83b8-248c38db0176",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Kernel trick\n",
    "---\n",
    "\n",
    "> TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa02c16-dee0-4893-a074-3dcc26799e48",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Reference\n",
    "---\n",
    "\n",
    "1. https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture7.pdf\n",
    "1. https://www.stat.cmu.edu/~ryantibs/convexopt-F16/scribes/kkt-scribed.pdf\n",
    "1. https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/\n",
    "1. https://cse.iitkgp.ac.in/~dsamanta/courses/da/resources/slides/10SupportVectorMachine.pdf\n",
    "1. https://see.stanford.edu/materials/aimlcs229/cs229-notes3.\n",
    "1. https://www-ai.cs.tu-dortmund.de/LEHRE/SEMINARE/SS09/AKTARBEITENDESDM/LITERATUR/PlattSMO.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
